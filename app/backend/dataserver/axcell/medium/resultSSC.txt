[0, [["Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks.", "background_label"], ["HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients.", "background_label"], ["In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size.", "objective_label"], ["We modify Martens' HF for these settings and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting.", "method_label"], ["Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classification and deep autoencoder experiments.", "result_label"]]]
[0, [["The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time.", "background_label"], ["We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time.", "method_label"], ["The method relies on local gradient variations across samples.", "method_label"], ["In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems.", "method_label"], ["Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.", "result_label"]]]
[0, [["This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting.", "background_label"], ["Past research suggest diminishing returns when increasing the size of neural networks.", "background_label"], ["Our experiments on ImageNet LSVRC-2010 show that this may be due to the fact there are highly diminishing returns for capacity in terms of training error, leading to underfitting.", "background_label"], ["This suggests that the optimization method - first order gradient descent - fails at this regime.", "method_label"], ["Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required.", "result_label"]]]
[0, [["In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high.", "objective_label"], ["In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace.", "method_label"], ["As with the Hessian Free (HF) method of [7], the Hessian matrix is never explicitly constructed, and is computed using a subset of data.", "method_label"], ["In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix.", "method_label"], ["We investigate the effectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF.", "method_label"], ["Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy.", "method_label"], ["It is also simpler and more general than HF, as it does not require a positive semi-definite approximation of the Hessian matrix to work well nor the setting of a damping parameter.", "result_label"], ["The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.", "result_label"]]]
[0, [["Classifier ensemble generally should combine diverse component classifiers.", "background_label"], ["However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy.", "background_label"], ["Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature.", "background_label"], ["In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity.", "objective_label"], ["Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function.", "method_label"], ["Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers' outputs which are kernelled and weighted), and the kernel weights are automatically learned.", "method_label"], ["We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively.", "method_label"], ["Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming.", "result_label"]]]
[0, [["We review machine learning methods employing positive definite kernels.", "background_label"], ["These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel.", "background_label"], ["Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions.", "method_label"], ["The latter include nonlinear functions as well as functions defined on nonvectorial data.", "method_label"], ["We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.", "method_label"]]]
[0, [["Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability.", "background_label"], ["While various methods have been proposed to speed up their convergence, the model selection phase is often ignored.", "background_label"], ["In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach.", "background_label"], ["In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation.", "method_label"], ["The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way.", "method_label"], ["Optimal rates of convergence are proved under standard smoothness assumptions on the target function, using the range space of the fractional integral operator associated with the kernel.", "result_label"]]]
[0, [["Under mild assumptions on the kernel, we obtain the best known error rates in a regularized learning scenario taking place in the corresponding reproducing kernel Hilbert space (RKHS).", "background_label"], ["The main novelty in the analysis is a proof that one can use a regularization term that grows significantly slower than the standard quadratic growth in the RKHS norm.", "result_label"]]]
[0, [["The support vector machine (SVM) algorithm is well known to the computer learning community for its very good practical results.", "background_label"], ["The goal of the present paper is to study this algorithm from a statistical perspective, using tools of concentration theory and empirical processes.", "objective_label"], ["Our main result builds on the observation made by other authors that the SVM can be viewed as a statistical regularization procedure.", "method_label"], ["From this point of view, it can also be interpreted as a model selection principle using a penalized criterion.", "method_label"], ["It is then possible to adapt general methods related to model selection in this framework to study two important points: (1) what is the minimum penalty and how does it compare to the penalty actually used in the SVM algorithm; (2) is it possible to obtain ``oracle inequalities'' in that setting, for the specific loss function used in the SVM algorithm?", "method_label"], ["We show that the answer to the latter question is positive and provides relevant insight to the former.", "result_label"], ["Our result shows that it is possible to obtain fast rates of convergence for SVMs.", "result_label"]]]
[0, [["Crowdsourcing has been part of the IR toolbox as a cheap and fast mechanism to obtain labels for system development and evaluation.", "background_label"], ["Successful deployment of crowdsourcing at scale involves adjusting many variables, a very important one being the number of workers needed per human intelligence task (HIT).", "background_label"], ["We consider the crowdsourcing task of learning the answer to simple multiple-choice HITs, which are representative of many relevance experiments.", "background_label"], ["In order to provide statistically significant results, one often needs to ask multiple workers to answer the same HIT.", "method_label"], ["A stopping rule is an algorithm that, given a HIT, decides for any given set of worker answers if the system should stop and output an answer or iterate and ask one more worker.", "method_label"], ["Knowing the historic performance of a worker in the form of a quality score can be beneficial in such a scenario.", "method_label"], ["In this paper we investigate how to devise better stopping rules given such quality scores.", "objective_label"], ["We also suggest adaptive exploration as a promising approach for scalable and automatic creation of ground truth.", "objective_label"], ["We conduct a data analysis on an industrial crowdsourcing platform, and use the observations from this analysis to design new stopping rules that use the workers' quality scores in a non-trivial manner.", "method_label"], ["We then perform a simulation based on a real-world workload, showing that our algorithm performs better than the more naive approaches.", "result_label"]]]
[0, [["Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning.", "background_label"], ["Current crowdsourcing platforms have some limitations in the area of quality control.", "background_label"], ["Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.", "background_label"], ["We propose a simple model for adaptive quality control in crowdsourced multiple-choice tasks which we call the \\emph{bandit survey problem}.", "method_label"], ["This model is related to, but technically different from the well-known multi-armed bandit problem.", "method_label"], ["We present several algorithms for this problem, and support them with analysis and simulations.", "method_label"], ["Our approach is based in our experience conducting relevance evaluation for a large commercial search engine.", "result_label"]]]
[0, [["Personalization is important for search engines to improve user experience.", "background_label"], ["Most of the existing work do pure feature engineering and extract a lot of session-style features and then train a ranking model.", "background_label"], ["Here we proposed a novel way to model both long term and short term user behavior using Multi-armed bandit algorithm.", "objective_label"], ["Our algorithm can generalize session information across users well, and as an Explore-Exploit style algorithm, it can generalize to new urls and new users well.", "method_label"], ["Experiments show that our algorithm can improve performance over the default ranking and outperforms several popular Multi-armed bandit algorithms.", "result_label"]]]
[0, [["Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems.", "background_label"], ["It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods.", "background_label"], ["However, many questions regarding its theoretical performance remained open.", "background_label"], ["In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary.", "method_label"], ["This is among the most important and widely studied versions of the contextual bandits problem.", "method_label"], ["We provide the first theoretical guarantees for the contextual version of Thompson Sampling.", "method_label"], ["We prove a high probability regret bound of $\\tilde{O}(d^{3/2}\\sqrt{T})$ (or $\\tilde{O}(d\\sqrt{T \\log(N)})$), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of $\\sqrt{d}$ (or $\\sqrt{\\log(N)}$) of the information-theoretic lower bound for this problem.", "result_label"]]]
[0, [["We develop a new method for frequentist multiple testing with Bayesian prior information.", "method_label"], ["Our procedure finds a new set of optimal p-value weights called the Bayes weights.", "method_label"], ["Prior information is relevant to many multiple testing problems.", "background_label"], ["Existing methods assume fixed, known effect sizes available from previous studies.", "method_label"], ["However, the case of uncertain information is usually the norm.", "method_label"], ["For a Gaussian prior on effect sizes, we show that finding the optimal weights is a non-convex problem.", "method_label"], ["Despite the non-convexity, we give an efficient algorithm that solves this problem nearly exactly.", "method_label"], ["We show that our method can discover new loci in genome-wide association studies.", "result_label"], ["On several data sets it compares favorably to other methods.", "result_label"], ["Open source code is available.", "result_label"]]]
[0, [["The power of multiple testing procedures can be increased by using weighted p-values (Genovese, Roeder and Wasserman 2005).", "background_label"], ["We derive the optimal weights and we show that the power is remarkably robust to misspecification of these weights.", "method_label"], ["We consider two methods for choosing weights in practice.", "method_label"], ["The first, external weighting, is based on prior information.", "method_label"], ["The second, estimated weighting, uses the data to choose weights.", "method_label"]]]
[0, [["We present a technique for adding global context to deep convolutional networks for semantic segmentation.", "background_label"], ["The approach is simple, using the average feature for a layer to augment the features at each location.", "method_label"], ["In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g.", "background_label"], ["from FCN).", "method_label"], ["When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines.", "method_label"], ["Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach.", "method_label"], ["Code is available at https://github.com/weiliu89/caffe/tree/fcn .", "other_label"]]]
[0, [["We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014).", "background_label"], ["The main hallmark of this architecture is the improved utilization of the computing resources inside the network.", "background_label"], ["This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant.", "method_label"], ["To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing.", "method_label"], ["One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", "result_label"]]]
[0, [["Convolutional networks are powerful visual models that yield hierarchies of features.", "background_label"], ["We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation.", "background_label"], ["Our key insight is to build \"fully convolutional\"networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning.", "objective_label"], ["We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models.", "method_label"], ["We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task.", "method_label"], ["We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations.", "method_label"], ["Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.", "result_label"]]]
[0, [["Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding.", "background_label"], ["Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks.", "background_label"], ["One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects.", "background_label"], ["To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling.", "method_label"], ["To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.", "method_label"], ["This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs.", "method_label"], ["Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation.", "method_label"], ["We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.", "result_label"]]]
[0, [["Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation.", "background_label"], ["Particularly for semantic segmentation, a two-stage procedure is often employed.", "background_label"], ["Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model.", "background_label"], ["In this work we unify this two-stage process into a single joint training algorithm.", "method_label"], ["We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.", "result_label"]]]
[0, [["We present new algorithms for Personalized PageRank estimation and Personalized PageRank search.", "background_label"], ["First, for the problem of estimating Personalized PageRank (PPR) from a source distribution to a target node, we present a new bidirectional estimator with simple yet strong guarantees on correctness and performance, and 3x to 8x speedup over existing estimators in experiments on a diverse set of networks.", "method_label"], ["Moreover, it has a clean algebraic structure which enables it to be used as a primitive for the Personalized PageRank Search problem: Given a network like Facebook, a query like \"people named John\", and a searching user, return the top nodes in the network ranked by PPR from the perspective of the searching user.", "method_label"], ["Previous solutions either score all nodes or score candidate nodes one at a time, which is prohibitively slow for large candidate sets.", "method_label"], ["We develop a new algorithm based on our bidirectional PPR estimator which identifies the most relevant results by sampling candidates based on their PPR; this is the first solution to PPR search that can find the best results without iterating through the set of all candidate results.", "method_label"], ["Finally, by combining PPR sampling with sequential PPR estimation and Monte Carlo, we develop practical algorithms for PPR search, and we show via experiments that our algorithms are efficient on networks with billions of edges.", "result_label"]]]
[0, [["We propose a new algorithm, FAST-PPR, for estimating personalized PageRank: given start node $s$ and target node $t$ in a directed graph, and given a threshold $\\delta$, FAST-PPR estimates the Personalized PageRank $\\pi_s(t)$ from $s$ to $t$, guaranteeing a small relative error as long $\\pi_s(t)>\\delta$.", "background_label"], ["Existing algorithms for this problem have a running-time of $\\Omega(1/\\delta)$; in comparison, FAST-PPR has a provable average running-time guarantee of ${O}(\\sqrt{d/\\delta})$ (where $d$ is the average in-degree of the graph).", "background_label"], ["This is a significant improvement, since $\\delta$ is often $O(1/n)$ (where $n$ is the number of nodes) for applications.", "background_label"], ["We also complement the algorithm with an $\\Omega(1/\\sqrt{\\delta})$ lower bound for PageRank estimation, showing that the dependence on $\\delta$ cannot be improved.", "method_label"], ["We perform a detailed empirical study on numerous massive graphs, showing that FAST-PPR dramatically outperforms existing algorithms.", "result_label"], ["For example, on the 2010 Twitter graph with 1.5 billion edges, for target nodes sampled by popularity, FAST-PPR has a $20$ factor speedup over the state of the art.", "result_label"], ["Furthermore, an enhanced version of FAST-PPR has a $160$ factor speedup on the Twitter graph, and is at least $20$ times faster on all our candidate graphs.", "result_label"]]]
[0, [["We develop a new bidirectional algorithm for estimating Markov chain multi-step transition probabilities: given a Markov chain, we want to estimate the probability of hitting a given target state in $\\ell$ steps after starting from a given source distribution.", "background_label"], ["Given the target state $t$, we use a (reverse) local power iteration to construct an `expanded target distribution', which has the same mean as the quantity we want to estimate, but a smaller variance -- this can then be sampled efficiently by a Monte Carlo algorithm.", "method_label"], ["Our method extends to any Markov chain on a discrete (finite or countable) state-space, and can be extended to compute functions of multi-step transition probabilities such as PageRank, graph diffusions, hitting/return times, etc.", "method_label"], ["Our main result is that in `sparse' Markov Chains -- wherein the number of transitions between states is comparable to the number of states -- the running time of our algorithm for a uniform-random target node is order-wise smaller than Monte Carlo and power iteration based algorithms; in particular, our method can estimate a probability $p$ using only $O(1/\\sqrt{p})$ running time.", "result_label"]]]
[0, [["The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent.", "background_label"], ["Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains.", "objective_label"], ["This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers.", "method_label"], ["We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments.", "method_label"], ["Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.", "result_label"]]]
[0, [["Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision.", "background_label"], ["In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models.", "method_label"], ["Moreover, in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model.", "method_label"], ["We evaluate our method on the TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures.", "method_label"], ["Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.", "result_label"]]]
[0, [["While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear.", "background_label"], ["The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks.", "background_label"], ["In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student.", "method_label"], ["Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer.", "method_label"], ["This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity.", "method_label"], ["For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "result_label"]]]
[0, [["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions.", "background_label"], ["Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets.", "background_label"], ["Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique.", "method_label"], ["We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model.", "method_label"], ["We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse.", "method_label"], ["Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.", "result_label"]]]
[0, [["This paper proposes a generic formulation that significantly expedites the training and deployment of image classification models, particularly under the scenarios of many image categories and high feature dimensions.", "objective_label"], ["As a defining property, our method represents both the images and learned classifiers using binary hash codes, which are simultaneously learned from the training data.", "method_label"], ["Classifying an image thereby reduces to computing the Hamming distance between the binary codes of the image and classifiers and selecting the class with minimal Hamming distance.", "method_label"], ["Conventionally, compact hash codes are primarily used for accelerating image search.", "method_label"], ["Our work is first of its kind to represent classifiers using binary codes.", "method_label"], ["Specifically, we formulate multi-class image classification as an optimization problem over binary variables.", "method_label"], ["The optimization alternatively proceeds over the binary classifiers and image hash codes.", "background_label"], ["Profiting from the special property of binary codes, we show that the sub-problems can be efficiently solved through either a binary quadratic program (BQP) or linear program.", "background_label"], ["In particular, for attacking the BQP problem, we propose a novel bit-flipping procedure which enjoys high efficacy and local optimality guarantee.", "method_label"], ["Our formulation supports a large family of empirical loss functions and is here instantiated by exponential / hinge losses.", "method_label"], ["Comprehensive evaluations are conducted on several representative image benchmarks.", "result_label"], ["The experiments consistently observe reduced complexities of model training and deployment, without sacrifice of accuracies.", "result_label"]]]
[0, [["This paper has been withdrawn by the authour.", "other_label"]]]
[0, [["Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems.", "background_label"], ["Recently a number of approaches to learning data-dependent hash functions have been developed.", "background_label"], ["In this work, we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information.", "objective_label"], ["Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework.", "method_label"], ["The learning procedure is implemented using column generation and hence is named CGHash.", "method_label"], ["At each iteration of the column generation procedure, the best hash function is selected.", "method_label"], ["Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified.", "method_label"], ["Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.", "result_label"]]]
[0, [["Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks.", "background_label"], ["We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead.", "method_label"], ["RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.", "result_label"]]]
[0, [["Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success.", "background_label"], ["However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem.", "background_label"], ["Here we introduce a new architecture designed to overcome this.", "objective_label"], ["Our so-called highway networks allow unimpeded information flow across many layers on information highways.", "objective_label"], ["They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow.", "method_label"], ["Even with hundreds of layers, highway networks can be trained directly through simple gradient descent.", "method_label"], ["This enables the study of extremely deep and efficient architectures.", "result_label"]]]
[0, [["Recurrent neural network is a powerful model that learns temporal patterns in sequential data.", "background_label"], ["For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem.", "background_label"], ["In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent.", "method_label"], ["This is achieved by using a slight structural modification of the simple recurrent neural network architecture.", "method_label"], ["We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory.", "method_label"], ["We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997).", "result_label"]]]
[0, [["We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs).", "background_label"], ["Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images.", "method_label"], ["The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts.", "method_label"], ["Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely.", "method_label"], ["Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence.", "method_label"], ["Learning of the model is efficient and requires no retraining/reweighting of the source classifiers.", "method_label"], ["We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification.", "result_label"], ["To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject).", "method_label"], ["We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples.", "result_label"], ["It also outperforms the state-of-the-art approaches for supervised domain adaptation.", "result_label"]]]
[0, [["We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions.", "method_label"], ["Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains.", "method_label"], ["We then propose two new feature mapping functions to augment the transformed data with their original features and zeros.", "method_label"], ["The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA.", "method_label"], ["Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions.", "method_label"], ["Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method.", "method_label"], ["Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.", "result_label"]]]
[0, [["Clustering explores meaningful patterns in the non-labeled data sets.", "background_label"], ["Cluster Ensemble Selection (CES) is a new approach, which can combine individual clustering results for increasing the performance of the final results.", "background_label"], ["Although CES can achieve better final results in comparison with individual clustering algorithms and cluster ensemble methods, its performance can be dramatically affected by its consensus diversity metric and thresholding procedure.", "background_label"], ["There are two problems in CES: 1) most of the diversity metrics is based on heuristic Shannon's entropy and 2) estimating threshold values are really hard in practice.", "background_label"], ["The main goal of this paper is proposing a robust approach for solving the above mentioned problems.", "objective_label"], ["Accordingly, this paper develops a novel framework for clustering problems, which is called Weighted Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community detection arena and graph based clustering.", "method_label"], ["Under this framework, a new version of spectral clustering, which is called Two Kernels Spectral Clustering, is used for generating graphs based individual clustering results.", "method_label"], ["Further, by using modularity, which is a famous metric in the community detection, on the transformed graph representation of individual clustering results, our approach provides an effective diversity estimation for individual clustering results.", "method_label"], ["Moreover, this paper introduces a new approach for combining the evaluated individual clustering results without the procedure of thresholding.", "result_label"], ["Experimental study on varied data sets demonstrates that the prosed approach achieves superior performance to state-of-the-art methods.", "result_label"]]]
[0, [["The Wisdom of Crowds is a phenomenon described in social science that suggests four criteria applicable to groups of people.", "background_label"], ["It is claimed that, if these criteria are satisfied, then the aggregate decisions made by a group will often be better than those of its individual members.", "background_label"], ["Inspired by this concept, we present a novel feedback framework for the cluster ensemble problem, which we call Wisdom of Crowds Cluster Ensemble (WOCCE).", "objective_label"], ["Although many conventional cluster ensemble methods focusing on diversity have recently been proposed, WOCCE analyzes the conditions necessary for a crowd to exhibit this collective wisdom.", "method_label"], ["These include decentralization criteria for generating primary results, independence criteria for the base algorithms, and diversity criteria for the ensemble members.", "method_label"], ["We suggest appropriate procedures for evaluating these measures, and propose a new measure to assess the diversity.", "method_label"], ["We evaluate the performance of WOCCE against some other traditional base algorithms as well as state-of-the-art ensemble methods.", "result_label"], ["The results demonstrate the efficiency of WOCCE's aggregate decision-making compared to other algorithms.", "result_label"]]]
[0, [["Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers.", "background_label"], ["Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task.", "background_label"], ["Also, it is expensive to spend manpower on labeling the data for better performance.", "background_label"], ["To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.", "objective_label"], ["Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one.", "method_label"], ["Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.", "result_label"]]]
[0, [["Teaching machines to read natural language documents remains an elusive challenge.", "background_label"], ["Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation.", "background_label"], ["In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data.", "objective_label"], ["This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "method_label"]]]
[0, [["Measuring the relationship between any pair of variables is a rich and active area of research that is central to scientific practice.", "background_label"], ["In contrast, characterizing the common information among any group of variables is typically a theoretical exercise with few practical methods for high-dimensional data.", "background_label"], ["A promising solution would be a multivariate generalization of the famous Wyner common information, but this approach relies on solving an apparently intractable optimization problem.", "background_label"], ["We leverage the recently introduced information sieve decomposition to formulate an incremental version of the common information problem that admits a simple fixed point solution, fast convergence, and complexity that is linear in the number of variables.", "method_label"], ["This scalable approach allows us to demonstrate the usefulness of common information in high-dimensional learning problems.", "method_label"], ["The sieve outperforms standard methods on dimensionality reduction tasks, solves a blind source separation problem that cannot be solved with ICA, and accurately recovers structure in brain imaging data.", "result_label"]]]
[0, [["We introduce a new framework for unsupervised learning of representations based on a novel hierarchical decomposition of information.", "background_label"], ["Intuitively, data is passed through a series of progressively fine-grained sieves.", "background_label"], ["Each layer of the sieve recovers a single latent factor that is maximally informative about multivariate dependence in the data.", "method_label"], ["The data is transformed after each pass so that the remaining unexplained information trickles down to the next layer.", "method_label"], ["Ultimately, we are left with a set of latent factors explaining all the dependence in the original data and remainder information consisting of independent noise.", "method_label"], ["We present a practical implementation of this framework for discrete variables and apply it to a variety of fundamental tasks in unsupervised learning including independent component analysis, lossy and lossless compression, and predicting missing values in data.", "result_label"]]]
[0, [["We define the relevant information in a signal $x\\in X$ as being the information that this signal provides about another signal $y\\in \\Y$.", "background_label"], ["Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken.", "background_label"], ["Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\\X$ play a role in the prediction.", "background_label"], ["We formalize this problem as that of finding a short code for $\\X$ that preserves the maximum information about $\\Y$.", "objective_label"], ["That is, we squeeze the information that $\\X$ provides about $\\Y$ through a `bottleneck' formed by a limited set of codewords $\\tX$.", "method_label"], ["This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\\x)$ emerges from the joint statistics of $\\X$ and $\\Y$.", "method_label"], ["This approach yields an exact set of self consistent equations for the coding rules $X \\to \\tX$ and $\\tX \\to \\Y$.", "method_label"], ["Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm.", "method_label"], ["Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.", "result_label"]]]
[0, [["This paper introduces the notion of exact common information, which is the minimum description length of the common randomness needed for the exact distributed generation of two correlated random variables $(X,Y)$.", "background_label"], ["We introduce the quantity $G(X;Y)=\\min_{X\\to W \\to Y} H(W)$ as a natural bound on the exact common information and study its properties and computation.", "method_label"], ["We then introduce the exact common information rate, which is the minimum description rate of the common randomness for the exact generation of a 2-DMS $(X,Y)$.", "method_label"], ["We give a multiletter characterization for it as the limit $\\bar{G}(X;Y)=\\lim_{n\\to \\infty}(1/n)G(X^n;Y^n)$.", "other_label"], ["While in general $\\bar{G}(X;Y)$ is greater than or equal to the Wyner common information, we show that they are equal for the Symmetric Binary Erasure Source.", "result_label"], ["We do not know, however, if the exact common information rate has a single letter characterization in general.", "result_label"]]]
[0, [["The introduction of the partial information decomposition generated a flurry of proposals for defining an intersection information that quantifies how much of \"the same information\"two or more random variables specify about a target random variable.", "background_label"], ["As of yet, none is wholly satisfactory.", "background_label"], ["A palatable measure of intersection information would provide a principled way to quantify slippery concepts, such as synergy.", "background_label"], ["Here, we introduce an intersection information measure based on the G\\'acs-K\\\"orner common random variable that is the first to satisfy the coveted target monotonicity property.", "method_label"], ["Our measure is imperfect, too, and we suggest directions for improvement.", "result_label"]]]
[0, [["The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data.", "background_label"], ["Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors.", "background_label"], ["This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks.", "objective_label"], ["This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.", "result_label"]]]
[0, [["Person re-identification across disjoint camera views has been widely applied in video surveillance yet it is still a challenging problem.", "background_label"], ["One of the major challenges lies in the lack of spatial and temporal cues, which makes it difficult to deal with large variations of lighting conditions, viewing angles, body poses and occlusions.", "background_label"], ["Recently, several deep learning based person re-identification approaches have been proposed and achieved remarkable performance.", "background_label"], ["However, most of those approaches extract discriminative features from the whole frame at one glimpse without differentiating various parts of the persons to identify.", "background_label"], ["It is essentially important to examine multiple highly discriminative local regions of the person images in details through multiple glimpses for dealing with the large appearance variance.", "background_label"], ["In this paper, we propose a new soft attention based model, i.e., the end to-end Comparative Attention Network (CAN), specifically tailored for the task of person re-identification.", "objective_label"], ["The end-to-end CAN learns to selectively focus on parts of pairs of person images after taking a few glimpses of them and adaptively comparing their appearance.", "method_label"], ["The CAN model is able to learn which parts of images are relevant for discerning persons and automatically integrates information from different parts to determine whether a pair of images belongs to the same person.", "method_label"], ["In other words, our proposed CAN model simulates the human perception process to verify whether two images are from the same person.", "method_label"], ["Extensive experiments on three benchmark person re-identification datasets, including CUHK01, CHUHK03 and Market-1501, clearly demonstrate that our proposed end-to-end CAN for person re-identification outperforms well established baselines significantly and offer new state-of-the-art performance.", "result_label"]]]
[0, [["We propose a soft attention based model for the task of action recognition in videos.", "objective_label"], ["We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally.", "background_label"], ["Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses.", "method_label"], ["The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them.", "method_label"], ["We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.", "result_label"]]]
[0, [["First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB).", "background_label"], ["While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 65%-76% accuracy on benchmark sets.", "background_label"], ["Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB.", "method_label"], ["We train a recurrent neural network to solve each problem.", "method_label"], ["On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results --- even neural networks based on much more complex architectures.", "method_label"], ["The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting.", "result_label"], ["In fact, we present a preliminary analysis of the performance of our model on real queries from Comcast's X1 entertainment platform with millions of users every day.", "result_label"]]]
[0, [["We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.", "background_label"], ["This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge.", "method_label"], ["Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data.", "method_label"], ["This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "result_label"]]]
[0, [["Many of the recent trajectory optimization algorithms alternate between linear approximation of the system dynamics around the mean trajectory and conservative policy update.", "background_label"], ["One way of constraining the policy change is by bounding the Kullback-Leibler (KL) divergence between successive policies.", "background_label"], ["These approaches already demonstrated great experimental success in challenging problems such as end-to-end control of physical systems.", "background_label"], ["However, the linear approximation of the system dynamics can introduce a bias in the policy update and prevent convergence to the optimal policy.", "background_label"], ["In this article, we propose a new model-free trajectory-based policy optimization algorithm with guaranteed monotonic improvement.", "objective_label"], ["The algorithm backpropagates a local, quadratic and time-dependent \\qfunc~learned from trajectory data instead of a model of the system dynamics.", "method_label"], ["Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics.", "method_label"], ["We experimentally demonstrate on highly non-linear control tasks the improvement in performance of our algorithm in comparison to approaches linearizing the system dynamics.", "result_label"], ["In order to show the monotonic improvement of our algorithm, we additionally conduct a theoretical analysis of our policy update scheme to derive a lower bound of the change in policy return between successive iterations.", "result_label"]]]
[0, [["We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement.", "background_label"], ["By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO).", "method_label"], ["This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks.", "method_label"], ["Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input.", "result_label"], ["Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.", "result_label"]]]
[0, [["Deep convolutional neural networks (ConvNets) have been recently shown to attain state-of-the-art performance for action recognition on standard-resolution videos.", "background_label"], ["However, less attention has been paid to recognition performance at extremely low resolutions (eLR) (e.g., 16 x 12 pixels).", "background_label"], ["Reliable action recognition using eLR cameras would address privacy concerns in various application environments such as private homes, hospitals, nursing/rehabilitation facilities, etc.", "background_label"], ["In this paper, we propose a semi-coupled filter-sharing network that leverages high resolution (HR) videos during training in order to assist an eLR ConvNet.", "method_label"], ["We also study methods for fusing spatial and temporal ConvNets customized for eLR videos in order to take advantage of appearance and motion information.", "method_label"], ["Our method outperforms state-of-the-art methods at extremely low resolutions on IXMAS (93.7%) and HMDB (29.2%) datasets.", "result_label"]]]
[0, [["We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset.", "background_label"], ["Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks.", "result_label"], ["In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets.", "result_label"], ["Finally, they are conceptually very simple and easy to train and use.", "result_label"]]]
[0, [["Visual recognition research often assumes a sufficient resolution of the region of interest (ROI).", "background_label"], ["That is usually violated in practice, inspiring us to explore the Very Low Resolution Recognition (VLRR) problem.", "background_label"], ["Typically, the ROI in a VLRR problem can be smaller than $16 \\times 16$ pixels, and is challenging to be recognized even by human experts.", "background_label"], ["We attempt to solve the VLRR problem using deep learning methods.", "method_label"], ["Taking advantage of techniques primarily in super resolution, domain adaptation and robust regression, we formulate a dedicated deep learning method and demonstrate how these techniques are incorporated step by step.", "method_label"], ["Any extra complexity, when introduced, is fully justified by both analysis and simulation results.", "method_label"], ["The resulting \\textit{Robust Partially Coupled Networks} achieves feature enhancement and recognition simultaneously.", "method_label"], ["It allows for both the flexibility to combat the LR-HR domain mismatch, and the robustness to outliers.", "method_label"], ["Finally, the effectiveness of the proposed models is evaluated on three different VLRR tasks, including face identification, digit recognition and font recognition, all of which obtain very impressive performances.", "result_label"]]]
[0, [["We propose a simple discrete time semi-supervised graph embedding approach to link prediction in dynamic networks.", "objective_label"], ["The learned embedding reflects information from both the temporal and cross-sectional network structures, which is performed by defining the loss function as a weighted sum of the supervised loss from past dynamics and the unsupervised loss of predicting the neighborhood context in the current network.", "method_label"], ["Our model is also capable of learning different embeddings for both formation and dissolution dynamics.", "method_label"], ["These key aspects contributes to the predictive performance of our model and we provide experiments with three real--world dynamic networks showing that our method is comparable to state of the art methods in link formation prediction and outperforms state of the art baseline methods in link dissolution prediction.", "result_label"]]]
[0, [["Dynamic networks are used in a variety of fields to represent the structure and evolution of the relationships between entities.", "background_label"], ["We present a model which embeds longitudinal network data as trajectories in a latent Euclidean space.", "objective_label"], ["A Markov chain Monte Carlo algorithm is proposed to estimate the model parameters and latent positions of the actors in the network.", "method_label"], ["The model yields meaningful visualization of dynamic networks, giving the researcher insight into the evolution and the structure, both local and global, of the network.", "method_label"], ["The model handles directed or undirected edges, easily handles missing edges, and lends itself well to predicting future edges.", "method_label"], ["Further, a novel approach is given to detect and visualize an attracting influence between actors using only the edge information.", "method_label"], ["We use the case-control likelihood approximation to speed up the estimation algorithm, modifying it slightly to account for missing data.", "method_label"], ["We apply the latent space model to data collected from a Dutch classroom, and a cosponsorship network collected on members of the U.S. House of Representatives, illustrating the usefulness of the model by making insights into the networks.", "result_label"]]]
[0, [["We propose a nonparametric approach to link prediction in large-scale dynamic networks.", "objective_label"], ["Our model uses graph-based features of pairs of nodes as well as those of their local neighborhoods to predict whether those nodes will be linked at each time step.", "method_label"], ["The model allows for different types of evolution in different parts of the graph (e.g, growing or shrinking communities).", "method_label"], ["We focus on large-scale graphs and present an implementation of our model that makes use of locality-sensitive hashing to allow it to be scaled to large problems.", "method_label"], ["Experiments with simulated data as well as five real-world dynamic graphs show that we outperform the state of the art, especially when sharp fluctuations or nonlinearities are present.", "result_label"], ["We also establish theoretical properties of our estimator, in particular consistency and weak convergence, the latter making use of an elaboration of Stein's method for dependency graphs.", "result_label"]]]
[0, [["Models of dynamic networks --- networks that evolve over time --- have manifold applications.", "background_label"], ["We develop a discrete-time generative model for social network evolution that inherits the richness and flexibility of the class of exponential-family random graph models.", "method_label"], ["The model --- a Separable Temporal ERGM (STERGM) --- facilitates separable modeling of the tie duration distributions and the structural dynamics of tie formation.", "method_label"], ["We develop likelihood-based inference for the model, and provide computational algorithms for maximum likelihood estimation.", "method_label"], ["We illustrate the interpretability of the model in analyzing a longitudinal network of friendship ties within a school.", "result_label"]]]
[0, [["The prediction of graph evolution is an important and challenging problem in the analysis of networks and of the Web in particular.", "background_label"], ["But while the appearance of new links is part of virtually every model of Web growth, the disappearance of links has received much less attention in the literature.", "background_label"], ["To fill this gap, our approach DecLiNe (an acronym for DECay of LInks in NEtworks) aims to predict link decay in networks, based on structural analysis of corresponding graph models.", "objective_label"], ["In analogy to the link prediction problem, we show that analysis of graph structures can help to identify indicators for superfluous links under consideration of common network models.", "method_label"], ["In doing so, we introduce novel metrics that denote the likelihood of certain links in social graphs to remain in the network, and combine them with state-of-the-art machine learning methods for predicting link decay.", "method_label"], ["Our methods are independent of the underlying network type, and can be applied to such diverse networks as the Web, social networks and any other structure representable as a network, and can be easily combined with case-specific content analysis and adopted for a variety of social network mining, filtering and recommendation applications.", "method_label"], ["In systematic evaluations with large-scale datasets of Wikipedia we show the practical feasibility of the proposed structure-based link decay prediction algorithms.", "result_label"]]]
[0, [["The data in many disciplines such as social networks, web analysis, etc.", "background_label"], ["is link-based, and the link structure can be exploited for many different data mining tasks.", "background_label"], ["In this paper, we consider the problem of temporal link prediction: Given link data for times 1 through T, can we predict the links at time T+1?", "background_label"], ["If our data has underlying periodic structure, can we predict out even further in time, i.e., links at time T+2, T+3, etc.?", "background_label"], ["In this paper, we consider bipartite graphs that evolve over time and consider matrix- and tensor-based methods for predicting future links.", "method_label"], ["We present a weight-based method for collapsing multi-year data into a single matrix.", "method_label"], ["We show how the well-known Katz method for link prediction can be extended to bipartite graphs and, moreover, approximated in a scalable way using a truncated singular value decomposition.", "method_label"], ["Using a CANDECOMP/PARAFAC tensor decomposition of the data, we illustrate the usefulness of exploiting the natural three-dimensional structure of temporal link data.", "method_label"], ["Through several numerical experiments, we demonstrate that both matrix- and tensor-based techniques are effective for temporal link prediction despite the inherent difficulty of the problem.", "result_label"], ["Additionally, we show that tensor-based techniques are particularly effective for temporal data with varying periodic patterns.", "result_label"]]]
[0, [["Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base.", "background_label"], ["In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \"computer\", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space.", "method_label"], ["We apply REINFORCE to directly optimize the task reward of this structured prediction problem.", "method_label"], ["To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process.", "method_label"], ["NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.", "result_label"]]]
[0, [["A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation.", "background_label"], ["This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework.", "objective_label"], ["By establishing a link between the log-likelihood and expected reward objectives, we show that an optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated scaled rewards.", "method_label"], ["Accordingly, we present a framework to smooth the predictive probability of the outputs using their corresponding rewards.", "method_label"], ["We optimize the conditional log-probability of augmented outputs that are sampled proportionally to their exponentiated scaled rewards.", "method_label"], ["Experiments on neural sequence to sequence models for speech recognition and machine translation show notable improvements over a maximum likelihood baseline by using reward augmented maximum likelihood (RAML), where the rewards are defined as the negative edit distance between the outputs and the ground truth labels.", "result_label"]]]
[0, [["We describe a question answering model that applies to both images and structured knowledge bases.", "background_label"], ["The model uses natural language strings to automatically assemble neural networks from a collection of composable modules.", "method_label"], ["Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision.", "method_label"], ["Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.", "result_label"]]]
[0, [["Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes.", "background_label"], ["Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning.", "background_label"], ["In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue.", "objective_label"], ["The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function).", "method_label"], ["We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation.", "result_label"], ["This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.", "result_label"]]]
[0, [["Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning.", "background_label"], ["However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning.", "background_label"], ["A major limitation of these models is in their inability to learn even simple arithmetic and logic operations.", "background_label"], ["For example, it has been shown that neural networks fail to learn to add two binary numbers reliably.", "background_label"], ["In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations.", "objective_label"], ["Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations.", "background_label"], ["The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself.", "background_label"], ["The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer.", "background_label"], ["Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent.", "method_label"], ["We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient.", "result_label"], ["On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.", "result_label"]]]
[0, [["Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering.", "background_label"], ["Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents.", "background_label"], ["For example, it is difficult to use a recurrent network to read a book and answer questions about it.", "background_label"], ["In this paper, we present an approach of reading text while skipping irrelevant information if needed.", "objective_label"], ["The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text.", "method_label"], ["We employ a standard policy gradient method to train the model to make discrete jumping decisions.", "method_label"], ["In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.", "result_label"]]]
[0, [["We present a novel, general, optimally fast, incremental way of searching for a universal algorithm that solves each task in a sequence of tasks.", "background_label"], ["The Optimal Ordered Problem Solver (OOPS) continually organizes and exploits previously found solutions to earlier tasks, efficiently searching not only the space of domain-specific algorithms, but also the space of search algorithms.", "background_label"], ["Essentially we extend the principles of optimal nonincremental universal search to build an incremental universal learner that is able to improve itself through experience.", "method_label"], ["In illustrative experiments, our self-improver becomes the first general system that learns to solve all n disk Towers of Hanoi tasks (solution size 2^n-1) for n up to 30, profiting from previously solved, simpler tasks involving samples of a simple context free language.", "result_label"]]]
[0, [["Many natural language processing applications use language models to generate text.", "background_label"], ["These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image.", "background_label"], ["However, at test time the model is expected to generate the entire sequence from scratch.", "background_label"], ["This discrepancy makes generation brittle, as errors may accumulate along the way.", "background_label"], ["We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE.", "method_label"], ["On three different tasks, our approach outperforms several strong baselines for greedy generation.", "method_label"], ["The method is also competitive when these baselines employ beam search, while being several times faster.", "result_label"]]]
[0, [["Current person re-identification (re-id) methods assume that (1) pre-labelled training data are available for every camera pair, (2) the gallery size for re-identification is moderate.", "background_label"], ["Both assumptions scale poorly to real-world applications when camera network size increases and gallery size becomes large.", "background_label"], ["Human verification of automatic model ranked re-id results becomes inevitable.", "background_label"], ["In this work, a novel human-in-the-loop re-id model based on Human Verification Incremental Learning (HVIL) is formulated which does not require any pre-labelled training data to learn a model, therefore readily scalable to new camera pairs.", "method_label"], ["This HVIL model learns cumulatively from human feedback to provide instant improvement to re-id ranking of each probe on-the-fly enabling the model scalable to large gallery sizes.", "method_label"], ["We further formulate a Regularised Metric Ensemble Learning (RMEL) model to combine a series of incrementally learned HVIL models into a single ensemble model to be used when human feedback becomes unavailable.", "method_label"]]]
[0, [["Is strong supervision necessary for learning a good visual representation?", "background_label"], ["Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)?", "background_label"], ["In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN.", "objective_label"], ["Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations.", "background_label"], ["Our key idea is that visual tracking provides the supervision.", "objective_label"], ["That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part.", "method_label"], ["We design a Siamese-triplet network with a ranking loss function to train this CNN representation.", "method_label"], ["Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression).", "method_label"], ["This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%.", "method_label"], ["We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.", "result_label"]]]
[0, [["Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches.", "background_label"], ["In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity.", "background_label"], ["Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.", "method_label"], ["Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.", "method_label"], ["To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method.", "method_label"], ["The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face.", "method_label"], ["On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%.", "method_label"], ["On YouTube Faces DB it achieves 95.12%.", "result_label"], ["Our system cuts the error rate in comparison to the best published result by 30% on both datasets.", "result_label"], ["We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.", "result_label"]]]
[0, [["Learning the distance metric between pairs of examples is of great importance for learning and visual recognition.", "background_label"], ["With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart.", "background_label"], ["In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances.", "method_label"], ["This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem.", "method_label"], ["Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning.", "method_label"], ["Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network.", "result_label"]]]
[0, [["The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain.", "objective_label"], ["For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis.", "background_label"], ["We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all.", "method_label"], ["Instead, we suppose that vectors lie near the range of a generative model $G: \\mathbb{R}^k \\to \\mathbb{R}^n$.", "method_label"], ["Our main theorem is that, if $G$ is $L$-Lipschitz, then roughly $O(k \\log L)$ random Gaussian measurements suffice for an $\\ell_2/\\ell_2$ recovery guarantee.", "method_label"], ["We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks.", "result_label"], ["Our method can use $5$-$10$x fewer measurements than Lasso for the same accuracy.", "result_label"]]]
[0, [["We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process.", "method_label"], ["The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables.", "method_label"], ["An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network.", "method_label"], ["We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.", "result_label"]]]
[0, [["Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible.", "background_label"], ["To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights.", "objective_label"], ["We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs.", "method_label"], ["We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable re- construction results on real images.", "method_label"], ["We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "result_label"]]]
[0, [["Generative adversarial networks (GANs) transform latent vectors into visually plausible images.", "background_label"], ["It is generally thought that the original GAN formulation gives no out-of-the-box method to reverse the mapping, projecting images back into latent space.", "background_label"], ["We introduce a simple, gradient-based technique called stochastic clipping.", "method_label"], ["In experiments, for images generated by the GAN, we precisely recover their latent vector pre-images 100% of the time.", "method_label"], ["Additional experiments demonstrate that this method is robust to noise.", "result_label"], ["Finally, we show that even for unseen images, our method appears to recover unique encodings.", "result_label"]]]
[0, [["The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution.", "background_label"], ["Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant.", "background_label"], ["However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space.", "background_label"], ["We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.", "result_label"]]]
[0, [["Suppose that we observe noisy linear measurements of an unknown signal that can be modeled as the sum of two component signals, each of which arises from a nonlinear sub-manifold of a high dimensional ambient space.", "background_label"], ["We introduce SPIN, a first order projected gradient method to recover the signal components.", "method_label"], ["Despite the nonconvex nature of the recovery problem and the possibility of underdetermined measurements, SPIN provably recovers the signal components, provided that the signal manifolds are incoherent and that the measurement operator satisfies a certain restricted isometry property.", "method_label"], ["SPIN significantly extends the scope of current recovery models and algorithms for low dimensional linear inverse problems and matches (or exceeds) the current state of the art in terms of performance.", "result_label"]]]
[0, [["Generative adversarial networks (GANs) learn to synthesise new samples from a high-dimensional distribution by passing samples drawn from a latent space through a generative network.", "background_label"], ["When the high-dimensional distribution describes images of a particular data set, the network should learn to generate visually similar image samples for latent variables that are close to each other in the latent space.", "background_label"], ["For tasks such as image retrieval and image classification, it may be useful to exploit the arrangement of the latent space by projecting images into it, and using this as a representation for discriminative tasks.", "background_label"], ["GANs often consist of multiple layers of non-linear computations, making them very difficult to invert.", "background_label"], ["This paper introduces techniques for projecting image samples into the latent space using any pre-trained GAN, provided that the computational graph is available.", "method_label"], ["We evaluate these techniques on both MNIST digits and Omniglot handwritten characters.", "result_label"], ["In the case of MNIST digits, we show that projections into the latent space maintain information about the style and the identity of the digit.", "result_label"], ["In the case of Omniglot characters, we show that even characters from alphabets that have not been seen during training may be projected well into the latent space; this suggests that this approach may have applications in one-shot learning.", "result_label"]]]
[0, [["Spambot detection in online social networks is a long-lasting challenge involving the study and design of detection techniques capable of efficiently identifying ever-evolving spammers.", "background_label"], ["Recently, a new wave of social spambots has emerged, with advanced human-like characteristics that allow them to go undetected even by current state-of-the-art algorithms.", "background_label"], ["In this paper, we show that efficient spambots detection can be achieved via an in-depth analysis of their collective behaviors exploiting the digital DNA technique for modeling the behaviors of social network users.", "method_label"], ["Inspired by its biological counterpart, in the digital DNA representation the behavioral lifetime of a digital account is encoded in a sequence of characters.", "method_label"], ["Then, we define a similarity measure for such digital DNA sequences.", "method_label"], ["We build upon digital DNA and the similarity between groups of users to characterize both genuine accounts and spambots.", "method_label"], ["Leveraging such characterization, we design the Social Fingerprinting technique, which is able to discriminate among spambots and genuine accounts in both a supervised and an unsupervised fashion.", "method_label"], ["We finally evaluate the effectiveness of Social Fingerprinting and we compare it with three state-of-the-art detection algorithms.", "result_label"], ["Among the peculiarities of our approach is the possibility to apply off-the-shelf DNA analysis techniques to study online users behaviors and to efficiently rely on a limited number of lightweight account characteristics.", "result_label"]]]
[0, [["Sybil accounts are fake identities created to unfairly increase the power or resources of a single malicious user.", "background_label"], ["Researchers have long known about the existence of Sybil accounts in online communities such as file-sharing systems, but have not been able to perform large scale measurements to detect them or measure their activities.", "background_label"], ["In this paper, we describe our efforts to detect, characterize and understand Sybil account activity in the Renren online social network (OSN).", "objective_label"], ["We use ground truth provided by Renren Inc. to build measurement based Sybil account detectors, and deploy them on Renren to detect over 100,000 Sybil accounts.", "method_label"], ["We study these Sybil accounts, as well as an additional 560,000 Sybil accounts caught by Renren, and analyze their link creation behavior.", "method_label"], ["Most interestingly, we find that contrary to prior conjecture, Sybil accounts in OSNs do not form tight-knit communities.", "result_label"], ["Instead, they integrate into the social graph just like normal users.", "method_label"], ["Using link creation timestamps, we verify that the large majority of links between Sybil accounts are created accidentally, unbeknownst to the attacker.", "result_label"], ["Overall, only a very small portion of Sybil accounts are connected to other Sybils with social links.", "result_label"], ["Our study shows that existing Sybil defenses are unlikely to succeed in today's OSNs, and we must design new techniques to effectively detect and defend against Sybil attacks.", "result_label"]]]
[0, [["The Turing test aimed to recognize the behavior of a human from that of a computer algorithm.", "background_label"], ["Such challenge is more relevant than ever in today's social media context, where limited attention and technology constrain the expressive power of humans, while incentives abound to develop software agents mimicking humans.", "background_label"], ["These social bots interact, often unnoticed, with real people in social media ecosystems, but their abundance is uncertain.", "background_label"], ["While many bots are benign, one can design harmful bots with the goals of persuading, smearing, or deceiving.", "background_label"], ["Here we discuss the characteristics of modern, sophisticated social bots, and how their presence can endanger online ecosystems and our society.", "method_label"], ["We then review current efforts to detect social bots on Twitter.", "method_label"], ["Features related to content, network, sentiment, and temporal patterns of activity are imitated by bots but at the same time can help discriminate synthetic behaviors from human ones, yielding signatures of engineered social tampering.", "result_label"]]]
[0, [["$\\textit{Fake followers}$ are those Twitter accounts specifically created to inflate the number of followers of a target account.", "background_label"], ["Fake followers are dangerous for the social platform and beyond, since they may alter concepts like popularity and influence in the Twittersphere - hence impacting on economy, politics, and society.", "background_label"], ["In this paper, we contribute along different dimensions.", "objective_label"], ["First, we review some of the most relevant existing features and rules (proposed by Academia and Media) for anomalous Twitter accounts detection.", "method_label"], ["Second, we create a baseline dataset of verified human and fake follower accounts.", "method_label"], ["Such baseline dataset is publicly available to the scientific community.", "result_label"], ["Then, we exploit the baseline dataset to train a set of machine-learning classifiers built over the reviewed rules and features.", "background_label"], ["Our results show that most of the rules proposed by Media provide unsatisfactory performance in revealing fake followers, while features proposed in the past by Academia for spam detection provide good results.", "background_label"], ["Building on the most promising features, we revise the classifiers both in terms of reduction of overfitting and cost for gathering the data needed to compute the features.", "method_label"], ["The final result is a novel $\\textit{Class A}$ classifier, general enough to thwart overfitting, lightweight thanks to the usage of the less costly features, and still able to correctly classify more than 95% of the accounts of the original training set.", "result_label"], ["We ultimately perform an information fusion-based sensitivity analysis, to assess the global sensitivity of each of the features employed by the classifier.", "method_label"], ["The findings reported in this paper, other than being supported by a thorough experimental methodology and interesting on their own, also pave the way for further investigation on the novel issue of fake Twitter followers.", "result_label"]]]
[0, [["We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner.", "background_label"], ["Our scheme pits two versions of the same agent, Alice and Bob, against one another.", "background_label"], ["Alice proposes a task for Bob to complete; and then Bob attempts to complete the task.", "background_label"], ["In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset.", "objective_label"], ["Alice will \"propose\"the task by doing a sequence of actions and then Bob must undo or repeat them, respectively.", "method_label"], ["Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent.", "method_label"], ["When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.", "result_label"]]]
[0, [["In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances.", "background_label"], ["We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones.", "method_label"], ["The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.", "method_label"], ["In addition to adversarial training we describe a model for adversarial {\\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls.", "method_label"], ["Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.", "result_label"]]]
[0, [["Reinforcement learning is a powerful technique to train an agent to perform a task.", "background_label"], ["However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function.", "background_label"], ["Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.", "background_label"], ["Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing.", "method_label"], ["We use a generator network to propose tasks for the agent to try to achieve, specified as goal states.", "method_label"], ["The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent.", "method_label"], ["Our method thus automatically produces a curriculum of tasks for the agent to learn.", "method_label"], ["We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment.", "method_label"], ["Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.", "result_label"]]]
[0, [["Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL).", "background_label"], ["We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering.", "background_label"], ["It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.", "method_label"], ["We demonstrate our approach on the task of manipulating objects with a robotic arm.", "method_label"], ["In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed.", "method_label"], ["Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments.", "result_label"], ["We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.", "result_label"]]]
[0, [["Many relevant tasks require an agent to reach a certain state, or to manipulate objects into a desired configuration.", "background_label"], ["For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock.", "background_label"], ["These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal.", "background_label"], ["Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent.", "method_label"], ["Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved.", "method_label"], ["The robot is trained in reverse, gradually learning to reach the goal from a set of start states increasingly far from the goal.", "method_label"], ["Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks.", "method_label"], ["We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.", "result_label"]]]
[0, [["Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL).", "background_label"], ["However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses).", "background_label"], ["Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system.", "method_label"], ["This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system.", "method_label"], ["The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy.", "method_label"], ["We formulate the policy learning as a zero-sum, minimax objective function.", "method_label"], ["Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.", "result_label"]]]
[0, [["We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression.", "background_label"], ["Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals.", "method_label"], ["The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object.", "method_label"], ["In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression.", "method_label"], ["We incorporate both the spatial and temporal context information into the training procedure.", "method_label"], ["By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time.", "method_label"], ["A special action is defined to indicate when the agent finds the referred object, and terminate the procedure.", "method_label"], ["We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms.", "result_label"], ["Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.", "result_label"]]]
[0, [["Humans refer to objects in their environments all the time, especially in dialogue with other people.", "background_label"], ["We explore generating and comprehending natural language referring expressions for objects in images.", "background_label"], ["In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly.", "method_label"], ["We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly.", "method_label"], ["Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring expression generation and comprehension.", "result_label"]]]
[0, [["We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language.", "background_label"], ["The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image.", "background_label"], ["To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization.", "method_label"], ["The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences.", "method_label"], ["We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions.", "result_label"], ["We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.", "result_label"]]]
[0, [["In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object.", "background_label"], ["Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context.", "background_label"], ["To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network.", "method_label"], ["Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task.", "method_label"], ["Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.", "result_label"]]]
[0, [["Referring expressions usually describe an object using properties of the object and relationships of the object with other objects.", "background_label"], ["We propose a technique that integrates context between objects to understand referring expressions.", "objective_label"], ["Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region.", "method_label"], ["The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training.", "method_label"], ["We utilize max-margin based MIL objective functions for training the LSTM.", "method_label"], ["Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties.", "result_label"], ["We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.", "result_label"]]]
[0, [["Grounding (i.e.", "background_label"], ["localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution.", "background_label"], ["Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision.", "background_label"], ["We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly.", "method_label"], ["During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase.", "method_label"], ["At test time, the correct attention, i.e., the grounding, is evaluated.", "method_label"], ["If grounding supervision is available it can be directly applied via a loss over the attention mechanism.", "method_label"], ["We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision.", "result_label"], ["Our supervised variant improves by a large margin over the state-of-the-art on both datasets.", "result_label"]]]
[0, [["We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described.", "background_label"], ["We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene.", "method_label"], ["Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation.", "method_label"], ["We also present a new large-scale dataset for referring expressions, based on MS-COCO.", "method_label"], ["We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/mjhucla/Google_Refexp_toolbox", "other_label"]]]
[0, [["Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.", "background_label"], ["In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image.", "objective_label"], ["The model is trained to maximize the likelihood of the target description sentence given the training image.", "method_label"], ["Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions.", "method_label"], ["Our model is often quite accurate, which we verify both qualitatively and quantitatively.", "method_label"], ["For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69.", "result_label"], ["We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28.", "result_label"], ["Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.", "result_label"]]]
[0, [["We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers.", "background_label"], ["We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers.", "method_label"], ["The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU.", "method_label"], ["Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.", "result_label"]]]
[0, [["This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection.", "objective_label"], ["Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks.", "objective_label"], ["Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy.", "method_label"], ["Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012.", "method_label"], ["Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate.", "method_label"], ["Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.", "other_label"]]]
[0, [["We present an active detection model for localizing objects in scenes.", "background_label"], ["The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object.", "objective_label"], ["This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning.", "method_label"], ["The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset.", "method_label"], ["We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.", "result_label"]]]
[0, [["In this paper, we introduce transformations of deep rectifier networks, enabling the conversion of deep rectifier networks into shallow rectifier networks.", "background_label"], ["We subsequently prove that any rectifier net of any depth can be represented by a maximum of a number of functions that can be realized by a shallow network with a single hidden layer.", "method_label"], ["The transformations of both deep rectifier nets and deep residual nets are conducted to demonstrate the advantages of the residual nets over the conventional neural nets and the advantages of the deep neural nets over the shallow neural nets.", "method_label"], ["In summary, for two rectifier nets with different depths but with same total number of hidden units, the corresponding single hidden layer representation of the deeper net is much more complex than the corresponding single hidden representation of the shallower net.", "result_label"], ["Similarly, for a residual net and a conventional rectifier net with the same structure except for the skip connections in the residual net, the corresponding single hidden layer representation of the residual net is much more complex than the corresponding single hidden layer representation of the conventional net.", "result_label"]]]
[0, [["It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones.", "background_label"], ["Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited.", "background_label"], ["In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date.", "background_label"], ["In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling.", "objective_label"], ["An equivalence between the networks and hierarchical tensor factorizations is established.", "method_label"], ["We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition.", "method_label"], ["Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network.", "method_label"], ["Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance.", "result_label"], ["The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community.", "result_label"]]]
[0, [["While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower number of training parameters as well as VC-dimension.", "background_label"], ["This theorem settles an old conjecture by Bengio on the role of depth in networks.", "objective_label"], ["We then define a general class of scalable, shift-invariant algorithms to show a simple and natural set of requirements that justify deep convolutional networks.", "method_label"]]]
[0, [["In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length.", "background_label"], ["Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training.", "background_label"], ["To support this observation, we rewrite residual networks as an explicit collection of paths.", "method_label"], ["Unlike traditional models, paths through residual networks vary in length.", "method_label"], ["Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other.", "method_label"], ["Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient.", "result_label"], ["For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep.", "method_label"], ["Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.", "result_label"]]]
[0, [["We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have.", "background_label"], ["Deep networks are able to sequentially map portions of each layer's input-space to the same output.", "background_label"], ["In this way, deep models compute functions that react equally to complicated patterns of different inputs.", "background_label"], ["The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth.", "background_label"], ["This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions.", "objective_label"], ["In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks.", "method_label"], ["We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.", "result_label"]]]
[0, [["Recent two-stream deep Convolutional Neural Networks (ConvNets) have made significant progress in recognizing human actions in videos.", "background_label"], ["Despite their success, methods extending the basic two-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences.", "background_label"], ["Further, such networks often use different baseline two-stream networks.", "background_label"], ["Therefore, the differences and the distinguishing factors between various methods using Recurrent Neural Networks (RNN) or convolutional networks on temporally-constructed feature vectors (Temporal-ConvNet) are unclear.", "method_label"], ["In this work, we first demonstrate a strong baseline two-stream ConvNet using ResNet-101.", "result_label"], ["We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information.", "background_label"], ["Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: 1) temporal segment RNN and 2) Inception-style Temporal-ConvNet.", "method_label"], ["We demonstrate that using both RNNs (using LSTMs) and Temporal-ConvNets on spatiotemporal feature matrices are able to exploit spatiotemporal dynamics to improve the overall performance.", "method_label"], ["However, each of these methods require proper care to achieve state-of-the-art performance; for example, LSTMs require pre-segmented data or else they cannot fully exploit temporal information.", "method_label"], ["Our analysis identifies specific limitations for each method that could form the basis of future work.", "result_label"], ["Our experimental results on UCF101 and HMDB51 datasets achieve state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation.", "result_label"]]]
[0, [["What defines an action like \"kicking ball\"?", "background_label"], ["We argue that the true meaning of an action lies in the change or transformation an action brings to the environment.", "background_label"], ["In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect).", "objective_label"], ["Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space.", "method_label"], ["We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51.", "method_label"], ["More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.", "result_label"]]]
[0, [["Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval.", "background_label"], ["In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted.", "objective_label"], ["We propose two methods capable of handling full length videos.", "method_label"], ["The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task.", "method_label"], ["The second proposed method explicitly models the video as an ordered sequence of frames.", "method_label"], ["For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN.", "method_label"], ["Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%).", "result_label"]]]
[0, [["In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications.", "background_label"], ["When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process.", "background_label"], ["In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature.", "objective_label"], ["Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation.", "method_label"], ["We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition.", "method_label"], ["We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.", "result_label"]]]
[0, [["Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently.", "background_label"], ["Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches.", "background_label"], ["CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace.", "background_label"], ["AE based methods learn a common representation by minimizing the error of reconstructing the two views.", "method_label"], ["Each of these approaches has its own advantages and disadvantages.", "method_label"], ["For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter.", "method_label"], ["In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace.", "method_label"], ["Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations.", "method_label"], ["Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches.", "result_label"]]]
[0, [["Dynamic scene understanding is a challenging problem and motion segmentation plays a crucial role in solving it.", "background_label"], ["Incorporating semantics and motion enhances the overall perception of the dynamic scene.", "background_label"], ["For applications of outdoor robotic navigation, joint learning methods have not been extensively used for extracting spatio-temporal features or adding different priors into the formulation.", "background_label"], ["The task becomes even more challenging without stereo information being incorporated.", "background_label"], ["This paper proposes an approach to fuse semantic features and motion clues using CNNs, to address the problem of monocular semantic motion segmentation.", "objective_label"], ["We deduce semantic and motion labels by integrating optical flow as a constraint with semantic features into dilated convolution network.", "method_label"], ["The pipeline consists of three main stages i.e Feature extraction, Feature amplification and Multi Scale Context Aggregation to fuse the semantics and flow features.", "method_label"], ["Our joint formulation shows significant improvements in monocular motion segmentation over the state of the art methods on challenging KITTI tracking dataset.", "result_label"]]]
[0, [["While the literature has been fairly dense in the areas of scene understanding and semantic labeling there have been few works that make use of motion cues to embellish semantic performance and vice versa.", "background_label"], ["In this paper, we address the problem of semantic motion segmentation, and show how semantic and motion priors augments performance.", "objective_label"], ["We pro- pose an algorithm that jointly infers the semantic class and motion labels of an object.", "method_label"], ["Integrating semantic, geometric and optical ow based constraints into a dense CRF-model we infer both the object class as well as motion class, for each pixel.", "method_label"], ["We found improvement in performance using a fully connected CRF as compared to a standard clique-based CRFs.", "method_label"], ["For inference, we use a Mean Field approximation based algorithm.", "method_label"], ["Our method outperforms recently pro- posed motion detection algorithms and also improves the semantic labeling compared to the state-of-the-art Automatic Labeling Environment algorithm on the challenging KITTI dataset especially for object classes such as pedestrians and cars that are critical to an outdoor robotic navigation scenario.", "result_label"]]]
[0, [["We propose an approach to detect flying objects such as UAVs and aircrafts when they occupy a small portion of the field of view, possibly moving against complex backgrounds, and are filmed by a camera that itself moves.", "background_label"], ["Solving such a difficult problem requires combining both appearance and motion cues.", "background_label"], ["To this end we propose a regression-based approach to motion stabilization of local image patches that allows us to achieve effective classification on spatio-temporal image cubes and outperform state-of-the-art techniques.", "method_label"], ["As the problem is relatively new, we collected two challenging datasets for UAVs and Aircrafts, which can be used as benchmarks for flying objects detection and vision-guided collision avoidance.", "result_label"]]]
[0, [["We segment moving objects in videos by ranking spatio-temporal segment proposals according to \"moving objectness\": how likely they are to contain a moving object.", "background_label"], ["In each video frame, we compute segment proposals using multiple figure-ground segmentations on per frame motion boundaries.", "method_label"], ["We rank them with a Moving Objectness Detector trained on image and motion fields to detect moving objects and discard over/under segmentations or background parts of the scene.", "method_label"], ["We extend the top ranked segments into spatio-temporal tubes using random walkers on motion affinities of dense point trajectories.", "method_label"], ["Our final tube ranking consistently outperforms previous segmentation methods in the two largest video segmentation benchmarks currently available, for any number of proposals.", "result_label"], ["Further, our per frame moving object proposals increase the detection rate up to 7\\% over previous state-of-the-art static proposal methods.", "result_label"]]]
[0, [["Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition.", "background_label"], ["Optical flow estimation has not been among the tasks where CNNs were successful.", "background_label"], ["In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task.", "objective_label"], ["We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations.", "method_label"], ["Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset.", "method_label"], ["We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.", "result_label"]]]
[0, [["This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts.", "background_label"], ["Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass.", "method_label"], ["Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms.", "method_label"], ["It has several appealing properties.", "method_label"], ["First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF.", "method_label"], ["Second, DPN represents various types of pairwise terms, making many existing works as its special cases.", "method_label"], ["Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU).", "result_label"], ["DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy.", "result_label"]]]
[0, [["Real-time visual analysis tasks, like tracking and recognition, require swift execution of computationally intensive algorithms.", "background_label"], ["Visual sensor networks can be enabled to perform such tasks by augmenting the sensor network with processing nodes and distributing the computational burden in a way that the cameras contend for the processing nodes while trying to minimize their task completion times.", "background_label"], ["In this paper, we formulate the problem of minimizing the completion time of all camera sensors as an optimization problem.", "objective_label"], ["We propose algorithms for fully distributed optimization, analyze the existence of equilibrium allocations, evaluate the effect of the network topology and of the video characteristics, and the benefits of central coordination.", "method_label"], ["Our results demonstrate that with sufficient information available, distributed optimization can provide low completion times, moreover predictable and stable performance can be achieved with additional, sparse central coordination.", "result_label"]]]
[0, [["The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application.", "background_label"], ["The repeatability is importand because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations [Schmid et al 2000].", "background_label"], ["The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate.", "background_label"], ["Three advances are described in this paper.", "method_label"], ["First, we present a new heuristic for feature detection, and using machine learning we derive a feature detector from this which can fully process live PAL video using less than 5% of the available processing time.", "method_label"], ["By comparison, most other detectors cannot even operate at frame rate (Harris detector 115%, SIFT 195%).", "method_label"], ["Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency.", "method_label"], ["Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes.", "method_label"], ["We show that despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors.", "result_label"], ["Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and very high quality.", "result_label"]]]
[0, [["This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA).", "background_label"], ["The AVA dataset densely annotates 80 atomic visual actions in 430 15-minute video clips, where actions are localized in space and time, resulting in 1.58M action labels with multiple labels per person occurring frequently.", "background_label"], ["The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations.", "method_label"], ["This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips.", "method_label"], ["We will release the dataset publicly.", "method_label"], ["AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition.", "method_label"], ["To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories.", "result_label"], ["While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.6% mAP, underscoring the need for developing new approaches for video understanding.", "result_label"]]]
[0, [["In this paper we introduce the problem of Visual Semantic Role Labeling: given an image we want to detect people doing actions and localize the objects of interaction.", "background_label"], ["Classical approaches to action recognition either study the task of action classification at the image or video clip level or at best produce a bounding box around the person doing the action.", "background_label"], ["We believe such an output is inadequate and a complete understanding can only come when we are able to associate objects in the scene to the different semantic roles of the action.", "method_label"], ["To enable progress towards this goal, we annotate a dataset of 16K people instances in 10K images with actions they are doing and associate objects in the scene with different semantic roles for each action.", "method_label"], ["Finally, we provide a set of baseline algorithms for this task and analyze error modes providing directions for future work.", "result_label"]]]
[0, [["We describe the DeepMind Kinetics human action video dataset.", "background_label"], ["The dataset contains 400 human action classes, with at least 400 video clips for each action.", "background_label"], ["Each clip lasts around 10s and is taken from a different YouTube video.", "background_label"], ["The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands.", "method_label"], ["We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset.", "method_label"], ["We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.", "result_label"]]]
[0, [["We address the problem of action detection in videos.", "background_label"], ["Driven by the latest progress in object detection from 2D images, we build action models using rich feature hierarchies derived from shape and kinematic cues.", "background_label"], ["We incorporate appearance and motion in two ways.", "method_label"], ["First, starting from image region proposals we select those that are motion salient and thus are more likely to contain the action.", "method_label"], ["This leads to a significant reduction in the number of regions being processed and allows for faster computations.", "method_label"], ["Second, we extract spatio-temporal feature representations to build strong classifiers using Convolutional Neural Networks.", "method_label"], ["We link our predictions to produce detections consistent in time, which we call action tubes.", "method_label"], ["We show that our approach outperforms other techniques in the task of action detection.", "result_label"]]]
[0, [["Deep learning has been demonstrated to achieve excellent results for image classification and object detection.", "background_label"], ["However, the impact of deep learning on video analysis (e.g.", "background_label"], ["action detection and recognition) has been limited due to complexity of video data and lack of annotations.", "background_label"], ["Previous convolutional neural networks (CNN) based video action detection approaches usually consist of two major steps: frame-level action proposal detection and association of proposals across frames.", "background_label"], ["Also, these methods employ two-stream CNN framework to handle spatial and temporal feature separately.", "method_label"], ["In this paper, we propose an end-to-end deep network called Tube Convolutional Neural Network (T-CNN) for action detection in videos.", "objective_label"], ["The proposed architecture is a unified network that is able to recognize and localize action based on 3D convolution features.", "method_label"], ["A video is first divided into equal length clips and for each clip a set of tube proposals are generated next based on 3D Convolutional Network (ConvNet) features.", "method_label"], ["Finally, the tube proposals of different clips are linked together employing network flow and spatio-temporal action detection is performed using these linked video proposals.", "method_label"], ["Extensive experiments on several video datasets demonstrate the superior performance of T-CNN for classifying and localizing actions in both trimmed and untrimmed videos compared to state-of-the-arts.", "result_label"]]]
[0, [["We introduce UCF101 which is currently the largest dataset of human actions.", "background_label"], ["It consists of 101 action classes, over 13k clips and 27 hours of video data.", "background_label"], ["The database consists of realistic user uploaded videos containing camera motion and cluttered background.", "method_label"], ["Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%.", "result_label"], ["To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.", "result_label"]]]
[0, [["The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks.", "background_label"], ["This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset.", "objective_label"], ["Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos.", "method_label"], ["We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.", "method_label"], ["We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters.", "method_label"], ["We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101.", "result_label"]]]
[0, [["Due to recent advances in technology, the recording and analysis of video data has become an increasingly common component of athlete training programmes.", "background_label"], ["Today it is incredibly easy and affordable to set up a fixed camera and record athletes in a wide range of sports, such as diving, gymnastics, golf, tennis, etc.", "background_label"], ["However, the manual analysis of the obtained footage is a time-consuming task which involves isolating actions of interest and categorizing them using domain-specific knowledge.", "background_label"], ["In order to automate this kind of task, three challenging sub-problems are often encountered: 1) temporally cropping events/actions of interest from continuous video; 2) tracking the object of interest; and 3) classifying the events/actions of interest.", "background_label"], ["Most previous work has focused on solving just one of the above sub-problems in isolation.", "method_label"], ["In contrast, this paper provides a complete solution to the overall action monitoring task in the context of a challenging real-world exemplar.", "result_label"], ["Specifically, we address the problem of diving classification.", "background_label"], ["This is a challenging problem since the person (diver) of interest typically occupies fewer than 1% of the pixels in each frame.", "background_label"], ["The model is required to learn the temporal boundaries of a dive, even though other divers and bystanders may be in view.", "background_label"], ["Finally, the model must be sensitive to subtle changes in body pose over a large number of frames to determine the classification code.", "background_label"], ["We provide effective solutions to each of the sub-problems which combine to provide a highly functional solution to the task as a whole.", "method_label"], ["The techniques proposed can be easily generalized to video footage recorded from other sports.", "method_label"]]]
[0, [["In this paper we present a new dataset and user simulator e-QRAQ (explainable Query, Reason, and Answer Question) which tests an Agent's ability to read an ambiguous text; ask questions until it can answer a challenge question; and explain the reasoning behind its questions and answer.", "background_label"], ["The User simulator provides the Agent with a short, ambiguous story and a challenge question about the story.", "method_label"], ["The story is ambiguous because some of the entities have been replaced by variables.", "method_label"], ["At each turn the Agent may ask for the value of a variable or try to answer the challenge question.", "method_label"], ["In response the User simulator provides a natural language explanation of why the Agent's query or answer was useful in narrowing down the set of possible answers, or not.", "method_label"], ["To demonstrate one potential application of the e-QRAQ dataset, we train a new neural architecture based on End-to-End Memory Networks to successfully generate both predictions and partial explanations of its current understanding of the problem.", "method_label"], ["We observe a strong correlation between the quality of the prediction and explanation.", "result_label"]]]
[0, [["We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding.", "objective_label"], ["This is achieved by gathering images of complex everyday scenes containing common objects in their natural context.", "objective_label"], ["Objects are labeled using per-instance segmentations to aid in precise object localization.", "method_label"], ["Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old.", "method_label"], ["With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation.", "method_label"], ["We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN.", "method_label"], ["Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "result_label"]]]
[0, [["Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations.", "background_label"], ["However, machine learning research in this area has been dramatically limited by the lack of large-scale resources.", "background_label"], ["To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning.", "method_label"], ["At 570K pairs, it is two orders of magnitude larger than all other resources of its type.", "background_label"], ["This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "result_label"]]]
[0, [["In recent years there is a growing interest in using deep representations for reinforcement learning.", "background_label"], ["In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter.", "objective_label"], ["Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically.", "method_label"], ["The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work.", "method_label"], ["Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success.", "method_label"], ["Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.", "result_label"]]]
[0, [["Integrating vision and language has long been a dream in work on artificial intelligence (AI).", "background_label"], ["In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond.", "background_label"], ["The available corpora have played a crucial role in advancing this area of research.", "background_label"], ["In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly.", "objective_label"], ["Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.", "result_label"]]]
[0, [["We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations.", "background_label"], ["Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions.", "method_label"], ["These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM.", "method_label"], ["These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models.", "method_label"], ["We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps.", "result_label"], ["Grad-CAM provides a new way to understand CNN-based models.", "result_label"], ["We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.", "result_label"]]]
[0, [["We propose a general model explanation system (MES) for \"explaining\"the output of black box classifiers.", "background_label"], ["This paper describes extensions to Turner (2015), which is referred to frequently in the text.", "objective_label"], ["We use the motivating example of a classifier trained to detect fraud in a credit card transaction history.", "method_label"], ["The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters.", "method_label"], ["We focus on explaining positive predictions (alerts).", "method_label"], ["However, the presented methodology is symmetrically applicable to negative predictions.", "result_label"]]]
[0, [["Tree ensembles, such as random forest and boosted trees, are renowned for their high prediction performance, whereas their interpretability is critically limited.", "background_label"], ["In this paper, we propose a post processing method that improves the model interpretability of tree ensembles.", "objective_label"], ["After learning a complex tree ensembles in a standard way, we approximate it by a simpler model that is interpretable for human.", "method_label"], ["To obtain the simpler model, we derive the EM algorithm minimizing the KL divergence from the complex ensemble.", "method_label"], ["A synthetic experiment showed that a complicated tree ensemble was approximated reasonably as interpretable.", "result_label"]]]
[0, [["This paper presents a generalized framework for the simulation of multiple robots and drones in highly realistic models of natural environments.", "background_label"], ["The proposed simulation architecture uses the Unreal Engine4 for generating both optical and depth sensor outputs from any position and orientation within the environment and provides several key domain specific simulation capabilities.", "method_label"], ["Various components and functionalities of the system have been discussed in detail.", "method_label"], ["The simulation engine also allows users to test and validate a wide range of computer vision algorithms involving different drone configurations under many types of environmental effects such as wind gusts.", "method_label"], ["The paper demonstrates the effectiveness of the system by giving experimental results for a test scenario where one drone tracks the simulated motion of another in a complex natural environment.", "result_label"]]]
[0, [["Computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which: (i) an agent can perceive, navigate, and take actions guided by AI algorithms, (ii) properties of the worlds can be modified (e.g., material and reflectance), (iii) physical simulations can be performed, and (iv) algorithms can be learnt and evaluated.", "background_label"], ["But creating realistic virtual worlds is not easy.", "background_label"], ["The game industry, however, has spent a lot of effort creating 3D worlds, which a player can interact with.", "background_label"], ["So researchers can build on these resources to create virtual worlds, provided we can access and modify the internal data structures of the games.", "method_label"], ["To enable this we created an open-source plugin UnrealCV (http://unrealcv.github.io) for a popular game engine Unreal Engine 4 (UE4).", "method_label"], ["We show two applications: (i) a proof of concept image dataset, and (ii) linking Caffe with the virtual world to test deep network algorithms.", "result_label"]]]
[0, [["Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world.", "background_label"], ["In this paper, we explore the ability of deep feed-forward models to learn such intuitive physics.", "objective_label"], ["Using a 3D game engine, we create small towers of wooden blocks whose stability is randomized and render them collapsing (or remaining upright).", "method_label"], ["This data allows us to train large convolutional network models which can accurately predict the outcome, as well as estimating the block trajectories.", "method_label"], ["The models are also able to generalize in two important ways: (i) to new physical scenarios, e.g.", "method_label"], ["towers with an additional block and (ii) to images of real wooden blocks, where it obtains a performance comparable to human subjects.", "result_label"]]]
[0, [["Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance.", "background_label"], ["Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks.", "background_label"], ["In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks.", "method_label"], ["We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods.", "method_label"], ["We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents.", "result_label"], ["Videos can be found at https://sites.google.com/view/mbmf", "other_label"]]]
[0, [["We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions.", "background_label"], ["Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories.", "method_label"], ["Our approach is based on convolutional autoregressive models and variational autoencoders.", "method_label"], ["It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays.", "method_label"], ["The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.", "result_label"]]]
[0, [["We present a unified framework for learning continuous control policies using backpropagation.", "background_label"], ["It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise.", "background_label"], ["The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions.", "method_label"], ["We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors.", "method_label"], ["We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation.", "method_label"], ["One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.", "result_label"]]]
[0, [["We study a seemingly unexpected and relatively less understood overfitting aspect of a fundamental tool in sparse linear modeling - best subset selection, which minimizes the residual sum of squares subject to a constraint on the number of nonzero coefficients.", "background_label"], ["While the best subset selection procedure is often perceived as the \"gold standard\"in sparse learning when the signal to noise ratio (SNR) is high, its predictive performance deteriorates when the SNR is low.", "background_label"], ["In particular, it is outperformed by continuous shrinkage methods, such as ridge regression and the Lasso.", "method_label"], ["We investigate the behavior of best subset selection in the high-noise regimes and propose an alternative approach based on a regularized version of the least-squares criterion.", "method_label"], ["Our proposed estimators (a) mitigate, to a large extent, the poor predictive performance of best subset selection in the high-noise regimes; and (b) perform favorably, while generally delivering substantially sparser models, relative to the best predictive models available via ridge regression and the Lasso.", "method_label"], ["We conduct an extensive theoretical analysis of the predictive properties of the proposed approach and provide justification for its superior predictive performance relative to best subset selection when the noise-level is high.", "result_label"], ["Our estimators can be expressed as solutions to mixed integer second order conic optimization problems and, hence, are amenable to modern computational tools from mathematical optimization.", "result_label"]]]
[0, [["We propose MC+, a fast, continuous, nearly unbiased and accurate method of penalized variable selection in high-dimensional linear regression.", "objective_label"], ["The LASSO is fast and continuous, but biased.", "background_label"], ["The bias of the LASSO may prevent consistent variable selection.", "background_label"], ["Subset selection is unbiased but computationally costly.", "background_label"], ["The MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear unbiased selection (PLUS) algorithm.", "method_label"], ["The MCP provides the convexity of the penalized loss in sparse regions to the greatest extent given certain thresholds for variable selection and unbiasedness.", "result_label"], ["The PLUS computes multiple exact local minimizers of a possibly nonconvex penalized loss function in a certain main branch of the graph of critical points of the penalized loss.", "background_label"], ["Its output is a continuous piecewise linear path encompassing from the origin for infinite penalty to a least squares solution for zero penalty.", "background_label"], ["We prove that at a universal penalty level, the MC+ has high probability of matching the signs of the unknowns, and thus correct selection, without assuming the strong irrepresentable condition required by the LASSO.", "method_label"], ["This selection consistency applies to the case of $p\\gg n$, and is proved to hold for exactly the MC+ solution among possibly many local minimizers.", "method_label"], ["We prove that the MC+ attains certain minimax convergence rates in probability for the estimation of regression coefficients in $\\ell_r$ balls.", "method_label"], ["We use the SURE method to derive degrees of freedom and $C_p$-type risk estimates for general penalized LSE, including the LASSO and MC+ estimators, and prove their unbiasedness.", "method_label"], ["Based on the estimated degrees of freedom, we propose an estimator of the noise level for proper choice of the penalty level.", "result_label"]]]
[0, [["In the last twenty-five years (1990-2014), algorithmic advances in integer optimization combined with hardware improvements have resulted in an astonishing 200 billion factor speedup in solving Mixed Integer Optimization (MIO) problems.", "background_label"], ["We present a MIO approach for solving the classical best subset selection problem of choosing $k$ out of $p$ features in linear regression given $n$ observations.", "objective_label"], ["We develop a discrete extension of modern first order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions.", "method_label"], ["The resulting algorithm (a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function.", "method_label"], ["Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with $n$ in the 1000s and $p$ in the 100s in minutes to provable optimality, and finds near optimal solutions for $n$ in the 100s and $p$ in the 1000s in minutes.", "result_label"], ["We also establish via numerical experiments that the MIO approach performs better than {\\texttt {Lasso}} and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power.", "result_label"]]]
[0, [["In regression settings where explanatory variables have very low correlations and there are relatively few effects, each of large magnitude, we expect the Lasso to find the important variables with few errors, if any.", "background_label"], ["This paper shows that in a regime of linear sparsity---meaning that the fraction of variables with a non-vanishing effect tends to a constant, however small---this cannot really be the case, even when the design variables are stochastically independent.", "background_label"], ["We demonstrate that true features and null features are always interspersed on the Lasso path, and that this phenomenon occurs no matter how strong the effect sizes are.", "method_label"], ["We derive a sharp asymptotic trade-off between false and true positive rates or, equivalently, between measures of type I and type II errors along the Lasso path.", "method_label"], ["This trade-off states that if we ever want to achieve a type II error (false negative rate) under a critical value, then anywhere on the Lasso path the type I error (false positive rate) will need to exceed a given threshold so that we can never have both errors at a low level at the same time.", "method_label"], ["Our analysis uses tools from approximate message passing (AMP) theory as well as novel elements to deal with a possibly adaptive selection of the Lasso regularizing parameter.", "result_label"]]]
[0, [["Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing.", "background_label"], ["At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics.", "background_label"], ["Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation.", "background_label"], ["However, such models are hard to tune, scale, and add exogenous variables to.", "background_label"], ["Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation.", "method_label"], ["We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.", "result_label"]]]
[0, [["Neural networks are known to be effective function approximators.", "background_label"], ["Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems.", "background_label"], ["This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior.", "objective_label"], ["Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems.", "method_label"], ["We carry out similar evaluations on select publicly available system identification datasets.", "method_label"], ["We demonstrate that deep neural networks are effective model estimators from input-output data", "result_label"]]]
[0, [["Mean-field variational inference is a method for approximate Bayesian posterior inference.", "background_label"], ["It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood.", "background_label"], ["This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution.", "background_label"], ["Often not all integrals are in closed form, which is typically handled by using a lower bound.", "method_label"], ["We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound.", "method_label"], ["This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role.", "method_label"], ["We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.", "result_label"]]]
[0, [["In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks.", "background_label"], ["Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\\%.", "method_label"], ["Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs.", "method_label"], ["We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics.", "method_label"], ["We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks.", "method_label"], ["We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them.", "result_label"], ["We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.", "result_label"]]]
[0, [["We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop.", "background_label"], ["It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood.", "method_label"], ["We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification.", "method_label"], ["We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.", "result_label"]]]
[0, [["Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks.", "background_label"], ["But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL.", "background_label"], ["We propose a new dropout variant which gives improved performance and better calibrated uncertainties.", "objective_label"], ["Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks.", "method_label"], ["Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles.", "method_label"], ["In RL this allows the agent to adapt its uncertainty dynamically as more data is observed.", "method_label"], ["We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.", "result_label"]]]
[0, [["In this paper a binary feature based Loop Closure Detection (LCD) method is proposed, which for the first time achieves higher precision-recall (PR) performance compared with state-of-the-art SIFT feature based approaches.", "background_label"], ["The proposed system originates from our previous work Multi-Index hashing for Loop closure Detection (MILD), which employs Multi-Index Hashing (MIH)~\\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of binary features.", "method_label"], ["As the accuracy of MILD is limited by repeating textures and inaccurate image similarity measurement, burstiness handling is introduced to solve this problem and achieves considerable accuracy improvement.", "method_label"], ["Additionally, a comprehensive theoretical analysis on MIH used in MILD is conducted to further explore the potentials of hashing methods for ANN search of binary features from probabilistic perspective.", "method_label"], ["This analysis provides more freedom on best parameter choosing in MIH for different application scenarios.", "result_label"], ["Experiments on popular public datasets show that the proposed approach achieved the highest accuracy compared with state-of-the-art while running at 30Hz for databases containing thousands of images.", "result_label"]]]
[0, [["This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments.", "background_label"], ["The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization.", "background_label"], ["Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing.", "method_label"], ["A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation.", "method_label"], ["We present an exhaustive evaluation in 27 sequences from the most popular datasets.", "result_label"], ["ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches.", "result_label"], ["For the benefit of the community, we make the source code public.", "result_label"]]]
[0, [["Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory.", "background_label"], ["This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups.", "method_label"], ["Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets.", "method_label"], ["Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process.", "method_label"], ["To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning.", "method_label"], ["The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations.", "result_label"], ["We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory", "result_label"]]]
[0, [["Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data.", "background_label"], ["This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning.", "objective_label"], ["By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations.", "method_label"], ["The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details.", "method_label"], ["Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation.", "result_label"]]]
[0, [["An efficient learner is one who reuses what they already know to tackle a new problem.", "background_label"], ["For a machine learner, this means understanding the similarities amongst datasets.", "background_label"], ["In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model.", "background_label"], ["Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion.", "objective_label"], ["The network is trained to produce statistics that encapsulate a generative model for each dataset.", "method_label"], ["Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks.", "method_label"], ["We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes.", "method_label"], ["We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.", "result_label"]]]
[0, [["We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations.", "background_label"], ["A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future.", "background_label"], ["To create such models, we introduce Generative Temporal Models augmented with external memory systems.", "method_label"], ["They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation.", "method_label"], ["We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently.", "method_label"], ["This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.", "result_label"]]]
[0, [["Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples.", "background_label"], ["We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks.", "background_label"], ["By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a similar generative process.", "method_label"], ["The proposed framework does not explicitly restrict diversity of the conditioning data and also does not require an extensive inference procedure for training or adaptation.", "method_label"], ["Our experiments on the Omniglot dataset demonstrate that Generative Matching Networks significantly improve predictive performance on the fly as more additional data is available and outperform existing state of the art conditional generative models.", "result_label"]]]
[0, [["For intelligent robotics applications, extending 3D mapping to 3D semantic mapping enables robots to, not only localize themselves with respect to the scene's geometrical features but also simultaneously understand the higher level meaning of the scene contexts.", "background_label"], ["Most previous methods focus on geometric 3D reconstruction and scene understanding independently notwithstanding the fact that joint estimation can boost the accuracy of the semantic mapping.", "background_label"], ["In this paper, a dense RGB-D semantic mapping system with a Pixel-Voxel network is proposed, which can perform dense 3D mapping while simultaneously recognizing and semantically labelling each point in the 3D map.", "method_label"], ["The proposed Pixel-Voxel network obtains global context information by using PixelNet to exploit the RGB image and meanwhile, preserves accurate local shape information by using VoxelNet to exploit the corresponding 3D point cloud.", "method_label"], ["Unlike the existing architecture that fuses score maps from different models with equal weights, we proposed a Softmax weighted fusion stack that adaptively learns the varying contributions of PixelNet and VoxelNet, and fuses the score maps of the two models according to their respective confidence levels.", "method_label"], ["The proposed Pixel-Voxel network achieves the state-of-the-art semantic segmentation performance on the SUN RGB-D benchmark dataset.", "method_label"], ["The runtime of the proposed system can be boosted to 11-12Hz, enabling near to real-time performance using an i7 8-cores PC with Titan X GPU.", "result_label"]]]
[0, [["3D scene understanding is important for robots to interact with the 3D world in a meaningful way.", "background_label"], ["Most previous works on 3D scene understanding focus on recognizing geometrical or semantic properties of the scene independently.", "background_label"], ["In this work, we introduce Data Associated Recurrent Neural Networks (DA-RNNs), a novel framework for joint 3D scene mapping and semantic labeling.", "objective_label"], ["DA-RNNs use a new recurrent neural network architecture for semantic labeling on RGB-D videos.", "method_label"], ["The output of the network is integrated with mapping techniques such as KinectFusion in order to inject semantic information into the reconstructed 3D scene.", "method_label"], ["Experiments conducted on a real world dataset and a synthetic dataset with RGB-D videos demonstrate the ability of our method in semantic 3D scene mapping.", "result_label"]]]
[0, [["We propose a novel semantic segmentation algorithm by learning a deconvolution network.", "objective_label"], ["We learn the network on top of the convolutional layers adopted from VGG 16-layer net.", "method_label"], ["The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks.", "method_label"], ["We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner.", "method_label"], ["The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally.", "method_label"], ["Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained with no external data through ensemble with the fully convolutional network.", "result_label"]]]
[0, [["We propose to use question answering (QA) data from Web forums to train chatbots from scratch, i.e., without dialog training data.", "objective_label"], ["First, we extract pairs of question and answer sentences from the typically much longer texts of questions and answers in a forum.", "method_label"], ["We then use these shorter texts to train seq2seq models in a more efficient way.", "method_label"], ["We further improve the parameter optimization using a new model selection strategy based on QA measures.", "method_label"], ["Finally, we propose to use extrinsic evaluation with respect to a QA task as an automatic evaluation method for chatbots.", "method_label"], ["The evaluation shows that the model achieves a MAP of 63.5% on the extrinsic task.", "result_label"], ["Moreover, it can answer correctly 49.5% of the questions when they are similar to questions asked in the forum, and 47.3% of the questions when they are more conversational in style.", "result_label"]]]
[0, [["Neural machine translation is a recently proposed approach to machine translation.", "background_label"], ["Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.", "background_label"], ["The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.", "background_label"], ["In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.", "method_label"], ["With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation.", "result_label"], ["Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "result_label"]]]
[0, [["Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks.", "background_label"], ["Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences.", "background_label"], ["In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.", "objective_label"], ["Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.", "method_label"], ["Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words.", "method_label"], ["Additionally, the LSTM did not have difficulty on long sentences.", "method_label"], ["For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset.", "method_label"], ["When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task.", "method_label"], ["The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice.", "method_label"], ["Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "result_label"]]]
[0, [["Accurate knowledge of object poses is crucial to successful robotic manipulation tasks, and yet most current approaches only work in laboratory settings.", "background_label"], ["Noisy sensors and cluttered scenes interfere with accurate pose recognition, which is problematic especially when performing complex tasks involving object interactions.", "background_label"], ["This is because most pose estimation algorithms focus only on estimating objects from a single frame, which means they lack continuity between frames.", "background_label"], ["Further, they often do not consider resulting physical properties of the predicted scene such as intersecting objects or objects in unstable positions.", "background_label"], ["In this work, we enhance the accuracy and stability of estimated poses for a whole scene by enforcing these physical constraints over time through the integration of a physics simulation.", "method_label"], ["This allows us to accurately determine relationships between objects for a construction task.", "method_label"], ["Scene parsing performance was evaluated on both simulated and real- world data.", "result_label"], ["We apply our method to a real-world block stacking task, where the robot must build a tall tower of colored blocks.", "result_label"]]]
[0, [["This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation.", "background_label"], ["Previous work has considered scene completion and semantic labeling of depth maps separately.", "background_label"], ["However, we observe that these two problems are tightly intertwined.", "background_label"], ["To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum.", "method_label"], ["Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning.", "method_label"], ["To train our network, we construct SUNCG - a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations.", "method_label"], ["Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task.", "result_label"]]]
[0, [["We study the task of image inpainting, which is to fill in the missing region of an incomplete image with plausible contents.", "background_label"], ["To this end, we propose a learning-based approach to generate visually coherent completion given a high-resolution image with missing components.", "objective_label"], ["In order to overcome the difficulty to directly learn the distribution of high-dimensional image data, we divide the task into inference and translation as two separate steps and model each step with a deep neural network.", "method_label"], ["We also use simple heuristics to guide the propagation of local textures from the boundary to the hole.", "method_label"], ["We show that, by using such techniques, inpainting reduces to the problem of learning two image-feature translation functions in much smaller space and hence easier to train.", "method_label"], ["We evaluate our method on several public datasets and show that we generate results of better visual quality than previous state-of-the-art methods.", "result_label"]]]
[0, [["Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability.", "background_label"], ["The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge.", "background_label"], ["We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior.", "background_label"], ["We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input.", "method_label"], ["Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.", "method_label"], ["We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.", "result_label"]]]
[0, [["We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction.", "background_label"], ["By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings.", "background_label"], ["In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s).", "background_label"], ["When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss.", "method_label"], ["The latter produces much sharper results because it can better handle multiple modes in the output.", "method_label"], ["We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures.", "method_label"], ["We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks.", "result_label"], ["Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.", "result_label"]]]
[0, [["This work explores conditional image generation with a new image density model based on the PixelCNN architecture.", "background_label"], ["The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks.", "background_label"], ["When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures.", "method_label"], ["When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions.", "method_label"], ["We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder.", "method_label"], ["Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.", "result_label"]]]
[0, [["Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data.", "background_label"], ["Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context.", "background_label"], ["In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data.", "method_label"], ["Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses.", "method_label"], ["This encoding is then passed through the generative model to infer the missing content.", "method_label"], ["In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase.", "method_label"], ["Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.", "result_label"]]]
[0, [["In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image.", "background_label"], ["Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities.", "background_label"], ["However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.", "background_label"], ["Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality.", "method_label"], ["The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images.", "method_label"], ["Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.", "result_label"]]]
[0, [["We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework.", "background_label"], ["We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic.", "background_label"], ["Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels.", "objective_label"], ["Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN.", "method_label"], ["The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%.", "method_label"], ["We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.", "result_label"]]]
[0, [["Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful.", "background_label"], ["Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function.", "background_label"], ["However, we found that this loss function may lead to the vanishing gradients problem during the learning process.", "background_label"], ["To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator.", "method_label"], ["We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^2$ divergence.", "method_label"], ["There are two benefits of LSGANs over regular GANs.", "method_label"], ["First, LSGANs are able to generate higher quality images than regular GANs.", "method_label"], ["Second, LSGANs perform more stable during the learning process.", "method_label"], ["We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs.", "result_label"], ["We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.", "result_label"]]]
[0, [["We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks.", "method_label"], ["This method balances the generator and discriminator during training.", "method_label"], ["Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality.", "method_label"], ["We also derive a way of controlling the trade-off between image diversity and visual quality.", "method_label"], ["We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions.", "method_label"], ["This is achieved while using a relatively simple model architecture and a standard training procedure.", "method_label"]]]
[0, [["We introduce a new algorithm named WGAN, an alternative to traditional GAN training.", "background_label"], ["In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches.", "method_label"], ["Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.", "result_label"]]]
[0, [["Stochastic Gradient Descent (SGD) is the central workhorse for training modern CNNs.", "background_label"], ["Although giving impressive empirical performance it can be slow to converge.", "background_label"], ["In this paper we explore a novel strategy for training a CNN using an alternation strategy that offers substantial speedups during training.", "objective_label"], ["We make the following contributions: (i) replace the ReLU non-linearity within a CNN with positive hard-thresholding, (ii) reinterpret this non-linearity as a binary state vector making the entire CNN linear if the multi-layer support is known, and (iii) demonstrate that under certain conditions a global optima to the CNN can be found through local descent.", "method_label"], ["We then employ a novel alternation strategy (between weights and support) for CNN training that leads to substantially faster convergence rates, nice theoretical properties, and achieving state of the art results across large scale datasets (e.g.", "result_label"], ["ImageNet) as well as other standard benchmarks.", "result_label"]]]
[0, [["While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points.", "background_label"], ["It has been argued that this is the case as all local minima are close to being globally optimal.", "background_label"], ["We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.", "result_label"]]]
[0, [["Deeper neural networks are more difficult to train.", "background_label"], ["We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.", "background_label"], ["We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.", "method_label"], ["We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.", "method_label"], ["On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.", "result_label"], ["An ensemble of these residual nets achieves 3.57% error on the ImageNet test set.", "background_label"], ["This result won the 1st place on the ILSVRC 2015 classification task.", "background_label"], ["We also present analysis on CIFAR-10 with 100 and 1000 layers.", "method_label"], ["The depth of representations is of central importance for many visual recognition tasks.", "method_label"], ["Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.", "result_label"], ["Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "result_label"]]]
[0, [["Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models.", "background_label"], ["DL is a form of machine learning that uses hierarchical layers of latent features.", "background_label"], ["In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective.", "objective_label"], ["We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation.", "method_label"], ["Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "result_label"]]]
[0, [["Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields.", "background_label"], ["However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking.", "background_label"], ["In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years.", "background_label"], ["A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing.", "background_label"], ["Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers.", "method_label"], ["This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model.", "method_label"], ["This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions.", "method_label"], ["Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional, recurrent and residual networks, and has better theoretical guarantees.", "method_label"]]]
[0, [["We study the error landscape of deep linear and nonlinear neural networks with the squared error loss.", "background_label"], ["Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete.", "background_label"], ["For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum.", "method_label"], ["Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization.", "method_label"], ["We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.", "result_label"]]]
[0, [["In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015.", "background_label"], ["With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist \"bad\"saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers).", "background_label"], ["Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work.", "method_label"], ["As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory?", "result_label"], ["It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima).", "method_label"], ["Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem.", "result_label"], ["We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.", "result_label"]]]
[0, [["Existing person re-identification (re-id) methods either assume the availability of well-aligned person bounding box images as model input or rely on constrained attention selection mechanisms to calibrate misaligned images.", "background_label"], ["They are therefore sub-optimal for re-id matching in arbitrarily aligned person images potentially with large human pose variations and unconstrained auto-detection errors.", "background_label"], ["In this work, we show the advantages of jointly learning attention selection and feature representation in a Convolutional Neural Network (CNN) by maximising the complementary information of different levels of visual attention subject to re-id discriminative learning constraints.", "objective_label"], ["Specifically, we formulate a novel Harmonious Attention CNN (HA-CNN) model for joint learning of soft pixel attention and hard regional attention along with simultaneous optimisation of feature representations, dedicated to optimise person re-id in uncontrolled (misaligned) images.", "method_label"], ["Extensive comparative evaluations validate the superiority of this new HA-CNN model for person re-id over a wide variety of state-of-the-art methods on three large-scale benchmarks including CUHK03, Market-1501, and DukeMTMC-ReID.", "result_label"]]]
[0, [["Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.", "background_label"], ["In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.", "objective_label"], ["This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process.", "method_label"], ["We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.", "result_label"]]]
[0, [["Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems.", "background_label"], ["Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem.", "background_label"], ["In this study, we propose a new attention-based deep neural network, named as HydraPlus-Net (HP-net), that multi-directionally feeds the multi-level attention maps to different feature layers.", "method_label"], ["The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image.", "method_label"], ["We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e.", "result_label"], ["pedestrian attribute recognition and person re-identification.", "result_label"], ["Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-the-art methods on various datasets.", "result_label"]]]
[0, [["Recent work has explored the problem of autonomous navigation by imitating a teacher and learning an end-to-end policy, which directly predicts controls from raw images.", "background_label"], ["However, these approaches tend to be sensitive to mistakes by the teacher and do not scale well to other environments or vehicles.", "background_label"], ["To this end, we propose Observational Imitation Learning (OIL), a novel imitation learning variant that supports online training and automatic selection of optimal behavior by observing multiple imperfect teachers.", "objective_label"], ["We apply our proposed methodology to the challenging problems of autonomous driving and UAV racing.", "method_label"], ["For both tasks, we utilize the Sim4CV simulator that enables the generation of large amounts of synthetic training data and also allows for online learning and evaluation.", "method_label"], ["We train a perception network to predict waypoints from raw image data and use OIL to train another network to predict controls from these waypoints.", "method_label"], ["Extensive experiments demonstrate that our trained network outperforms its teachers, conventional imitation learning (IL) and reinforcement learning (RL) baselines and even humans in simulation.", "result_label"], ["The project website is available at https://sites.google.com/kaust.edu.sa/oil/ and a video at https://youtu.be/_rhq8a0qgeg", "other_label"]]]
[0, [["We present an approach to sensorimotor control in immersive environments.", "objective_label"], ["Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream.", "background_label"], ["The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment.", "method_label"], ["The model is trained using supervised learning techniques, but without extraneous supervision.", "method_label"], ["It learns to act based on raw sensory input from a complex three-dimensional environment.", "method_label"], ["The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time.", "method_label"], ["We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom.", "method_label"], ["The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks.", "result_label"], ["The results also show that trained models successfully generalize across environments and goals.", "result_label"], ["A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "result_label"]]]
[0, [["Robust real-world learning should benefit from both demonstrations and interactions with the environment.", "background_label"], ["Current approaches to learning from demonstration and reward perform supervised learning on expert demonstration data and use reinforcement learning to further improve performance based on the reward received from the environment.", "background_label"], ["These tasks have divergent losses which are difficult to jointly optimize and such methods can be very sensitive to noisy demonstrations.", "background_label"], ["We propose a unified reinforcement learning algorithm, Normalized Actor-Critic (NAC), that effectively normalizes the Q-function, reducing the Q-values of actions unseen in the demonstration data.", "method_label"], ["NAC learns an initial policy network from demonstrations and refines the policy in the environment, surpassing the demonstrator's performance.", "method_label"], ["Crucially, both learning from demonstration and interactive refinement use the same objective, unlike prior approaches that combine distinct supervised and reinforcement losses.", "method_label"], ["This makes NAC robust to suboptimal demonstration data since the method is not forced to mimic all of the examples in the dataset.", "method_label"], ["We show that our unified reinforcement learning algorithm can learn robustly and outperform existing baselines when evaluated on several realistic driving games.", "result_label"]]]
[0, [["Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations.", "background_label"], ["While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly.", "background_label"], ["In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects.", "objective_label"], ["First, we allow the agent to acquire experience in a self-supervised fashion.", "method_label"], ["This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken.", "method_label"], ["We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.", "result_label"]]]
[0, [["We introduce CARLA, an open-source simulator for autonomous driving research.", "background_label"], ["CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems.", "background_label"], ["In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely.", "background_label"], ["The simulation platform supports flexible specification of sensor suites and environmental conditions.", "method_label"], ["We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning.", "method_label"], ["The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research.", "result_label"], ["The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E", "other_label"]]]
[0, [["We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain.", "background_label"], ["We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces.", "method_label"], ["Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving.", "method_label"], ["Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives.", "method_label"], ["We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.", "result_label"]]]
[0, [["We propose a model-free deep reinforcement learning method that leverages a small amount of demonstration data to assist a reinforcement learning agent.", "background_label"], ["We apply this approach to robotic manipulation tasks and train end-to-end visuomotor policies that map directly from RGB camera inputs to joint velocities.", "method_label"], ["We demonstrate that our approach can solve a wide variety of visuomotor tasks, for which engineering a scripted controller would be laborious.", "method_label"], ["In experiments, our reinforcement and imitation agent achieves significantly better performances than agents trained with reinforcement learning or imitation learning alone.", "method_label"], ["We also illustrate that these policies, trained with large visual and dynamics variations, can achieve preliminary successes in zero-shot sim2real transfer.", "method_label"], ["A brief visual description of this work can be viewed in https://youtu.be/EDl8SQUNjj0", "other_label"]]]
[0, [["Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference.", "background_label"], ["This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it.", "objective_label"], ["Can learning to search work even when the reference is poor?", "objective_label"], ["We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee.", "method_label"], ["Consequently, LOLS can improve upon the reference policy, unlike previous algorithms.", "method_label"], ["This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.", "result_label"]]]
[0, [["Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal.", "background_label"], ["One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning.", "objective_label"], ["This approach is indirect and can be slow.", "objective_label"], ["We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning.", "method_label"], ["We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.", "result_label"]]]
[0, [["Autonomous agile flight brings up fundamental challenges in robotics, such as coping with unreliable state estimation, reacting optimally to dynamically changing environments, and coupling perception and action in real time under severe resource constraints.", "background_label"], ["In this paper, we consider these challenges in the context of autonomous, vision-based drone racing in dynamic environments.", "objective_label"], ["Our approach combines a convolutional neural network (CNN) with a state-of-the-art path-planning and control system.", "method_label"], ["The CNN directly maps raw images into a robust representation in the form of a waypoint and desired speed.", "method_label"], ["This information is then used by the planner to generate a short, minimum-jerk trajectory segment and corresponding motor commands to reach the desired goal.", "method_label"], ["We demonstrate our method in autonomous agile flight scenarios, in which a vision-based quadrotor traverses drone-racing tracks with possibly moving gates.", "method_label"], ["Our method does not require any explicit map of the environment and runs fully onboard.", "method_label"], ["We extensively test the precision and robustness of the approach in simulation and in the physical world.", "result_label"], ["We also evaluate our method against state-of-the-art navigation approaches and professional human drone pilots.", "result_label"]]]
[0, [["Recent progress in computer vision has been driven by high-capacity models trained on large datasets.", "background_label"], ["Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required.", "background_label"], ["In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games.", "objective_label"], ["Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware.", "method_label"], ["This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content.", "method_label"], ["We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game.", "method_label"], ["Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just 1/3 of the CamVid training set outperform models trained on the complete CamVid training set.", "result_label"]]]
[0, [["Latest deep learning methods for object detection provide remarkable performance, but have limits when used in robotic applications.", "background_label"], ["One of the most relevant issues is the long training time, which is due to the large size and imbalance of the associated training sets, characterized by few positive and a large number of negative examples (i.e.", "background_label"], ["background).", "background_label"], ["Proposed approaches are based on end-to-end learning by back-propagation [22] or kernel methods trained with Hard Negatives Mining on top of deep features [8].", "method_label"], ["These solutions are effective, but prohibitively slow for on-line applications.", "background_label"], ["In this paper we propose a novel pipeline for object detection that overcomes this problem and provides comparable performance, with a 60x training speedup.", "objective_label"], ["Our pipeline combines (i) the Region Proposal Network and the deep feature extractor from [22] to efficiently select candidate RoIs and encode them into powerful representations, with (ii) the FALKON [23] algorithm, a novel kernel-based method that allows fast training on large scale problems (millions of points).", "method_label"], ["We address the size and imbalance of training data by exploiting the stochastic subsampling intrinsic into the method and a novel, fast, bootstrapping approach.", "method_label"], ["We assess the effectiveness of the approach on a standard Computer Vision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a real robotic scenario with the iCubWorld Transformations [18] dataset.", "result_label"]]]
[0, [["Kernel methods provide a principled way to perform non linear, nonparametric learning.", "background_label"], ["They rely on solid functional analytic foundations and enjoy optimal statistical properties.", "background_label"], ["However, at least in their basic form, they have limited applicability in large scale scenarios because of stringent computational requirements in terms of time and especially memory.", "background_label"], ["In this paper, we take a substantial step in scaling up kernel methods, proposing FALKON, a novel algorithm that allows to efficiently process millions of points.", "objective_label"], ["FALKON is derived combining several algorithmic principles, namely stochastic subsampling, iterative solvers and preconditioning.", "method_label"], ["Our theoretical analysis shows that optimal statistical accuracy is achieved requiring essentially $O(n)$ memory and $O(n\\sqrt{n})$ time.", "result_label"], ["An extensive experimental analysis on large scale datasets shows that, even with a single machine, FALKON outperforms previous state of the art solutions, which exploit parallel/distributed architectures.", "result_label"]]]
[0, [["We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories.", "background_label"], ["First we propose various improvements to the YOLO detection method, both novel and drawn from prior work.", "method_label"], ["The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO.", "method_label"], ["At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007.", "method_label"], ["At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster.", "method_label"], ["Finally we propose a method to jointly train on object detection and classification.", "method_label"], ["Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset.", "method_label"], ["Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data.", "method_label"], ["We validate our approach on the ImageNet detection task.", "method_label"], ["YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes.", "result_label"], ["On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP.", "result_label"], ["But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories.", "result_label"], ["And it still runs in real-time.", "result_label"]]]
[0, [["We present a method for detecting objects in images using a single deep neural network.", "background_label"], ["Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location.", "method_label"], ["At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape.", "method_label"], ["Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.", "method_label"], ["Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network.", "method_label"], ["This makes SSD easy to train and straightforward to integrate into systems that require a detection component.", "method_label"], ["Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference.", "result_label"], ["Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size.", "result_label"], ["For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model.", "result_label"], ["Code is available at https://github.com/weiliu89/caffe/tree/ssd .", "other_label"]]]
[0, [["We present a conceptually simple, flexible, and general framework for object instance segmentation.", "background_label"], ["Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.", "method_label"], ["The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition.", "method_label"], ["Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps.", "method_label"], ["Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework.", "method_label"], ["We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection.", "result_label"], ["Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners.", "result_label"], ["We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition.", "result_label"], ["Code has been made available at: https://github.com/facebookresearch/Detectron", "other_label"]]]
[0, [["The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations.", "background_label"], ["In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far.", "background_label"], ["In this paper, we investigate why this is the case.", "objective_label"], ["We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause.", "objective_label"], ["We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples.", "objective_label"], ["Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.", "method_label"], ["To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet.", "method_label"], ["Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.", "result_label"], ["Code is at: https://github.com/facebookresearch/Detectron.", "other_label"]]]
[0, [["Video Question Answering (QA) is an important task in understanding video temporal structure.", "background_label"], ["We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer.", "background_label"], ["Based these observations, we propose a motion-appearance comemory network for video QA.", "objective_label"], ["Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA.", "method_label"], ["Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions.", "method_label"], ["We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.", "result_label"]]]
[0, [["This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images.", "background_label"], ["SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer.", "background_label"], ["We argue that image question answering (QA) often requires multiple steps of reasoning.", "background_label"], ["Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively.", "method_label"], ["Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches.", "result_label"], ["The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.", "result_label"]]]
[0, [["Most tasks in natural language processing can be cast into question answering (QA) problems over language input.", "background_label"], ["We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers.", "method_label"], ["Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations.", "method_label"], ["These results are then reasoned over in a hierarchical recurrent sequence model to generate answers.", "method_label"], ["The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB).", "result_label"], ["The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.", "result_label"]]]
[0, [["Temporal Action Proposal (TAP) generation is an important problem, as fast and accurate extraction of semantically important (e.g.", "background_label"], ["human actions) segments from untrimmed videos is an important step for large-scale video analysis.", "background_label"], ["We propose a novel Temporal Unit Regression Network (TURN) model.", "objective_label"], ["There are two salient aspects of TURN: (1) TURN jointly predicts action proposals and refines the temporal boundaries by temporal coordinate regression; (2) Fast computation is enabled by unit feature reuse: a long untrimmed video is decomposed into video units, which are reused as basic building blocks of temporal proposals.", "method_label"], ["TURN outperforms the state-of-the-art methods under average recall (AR) by a large margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames per second (FPS) on a TITAN X GPU.", "method_label"], ["We further apply TURN as a proposal generation stage for existing temporal action localization pipelines, it outperforms state-of-the-art performance on THUMOS-14 and ActivityNet.", "result_label"]]]
[0, [["Temporal action detection in long videos is an important problem.", "background_label"], ["State-of-the-art methods address this problem by applying action classifiers on sliding windows.", "background_label"], ["Although sliding windows may contain an identifiable portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance.", "background_label"], ["We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model.", "method_label"], ["Class-agnostic proposals and specific actions are detected respectively in the first and the second stage.", "method_label"], ["CBR uses temporal coordinate regression to refine the temporal boundaries of the sliding windows.", "method_label"], ["The salient aspect of the refinement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the refined windows back to the system for further boundary refinement.", "method_label"], ["We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets.", "result_label"], ["The performance gain is especially remarkable under high IoU thresholds, e.g.", "result_label"], ["map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%.", "result_label"]]]
[0, [["Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines.", "background_label"], ["Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers.", "background_label"], ["Regarding visual content modeling, most of existing VQA methods adopt the strategy of extracting global features from the image or video, which inevitably fails in capturing fine-grained information such as spatial configuration of multiple objects.", "method_label"], ["Extracting features from auto-generated regions -- as some region-based image recognition methods do -- cannot essentially address this problem and may introduce some overwhelming irrelevant features with the question.", "method_label"], ["In this work, we propose a novel Focused Dynamic Attention (FDA) model to provide better aligned image content representation with proposed questions.", "objective_label"], ["Being aware of the key words in the question, FDA employs off-the-shelf object detector to identify important regions and fuse the information from the regions and global features via an LSTM unit.", "method_label"], ["Such question-driven representations are then combined with question representation and fed into a reasoning unit for generating the answers.", "method_label"], ["Extensive evaluation on a large-scale benchmark dataset, VQA, clearly demonstrate the superior performance of FDA over well-established baselines.", "result_label"]]]
[0, [["Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods.", "background_label"], ["However, applications of these methods to multimodality remain limited.", "background_label"], ["We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning.", "method_label"], ["Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information.", "method_label"], ["The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies.", "method_label"], ["Various alternative models introduced by multimodality are explored based on our study.", "method_label"], ["We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks.", "method_label"], ["Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.", "method_label"]]]
[0, [["Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering.", "background_label"], ["One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks.", "background_label"], ["However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images.", "background_label"], ["Based on an analysis of the DMN, we propose several improvements to its memory and input modules.", "objective_label"], ["Together with these changes we introduce a novel input module for images in order to be able to answer visual questions.", "method_label"], ["Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision.", "result_label"]]]
[0, [["Question-answering (QA) on video contents is a significant challenge for achieving human-level intelligence as it involves both vision and language in real-world settings.", "background_label"], ["Here we demonstrate the possibility of an AI agent performing video story QA by learning from a large amount of cartoon videos.", "background_label"], ["We develop a video-story learning model, i.e.", "method_label"], ["Deep Embedded Memory Networks (DEMN), to reconstruct stories from a joint scene-dialogue video stream using a latent embedding space of observed data.", "method_label"], ["The video stories are stored in a long-term memory component.", "method_label"], ["For a given question, an LSTM-based attention model uses the long-term memory to recall the best question-story-answer triplet by focusing on specific words containing key information.", "background_label"], ["We trained the DEMN on a novel QA dataset of children's cartoon video series, Pororo.", "method_label"], ["The dataset contains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained sentences for scene description, and 8,913 story-related QA pairs.", "method_label"], ["Our experimental results show that the DEMN outperforms other QA models.", "result_label"], ["This is mainly due to 1) the reconstruction of video stories in a scene-dialogue combined form that utilize the latent embedding and 2) attention.", "result_label"], ["DEMN also achieved state-of-the-art results on the MovieQA benchmark.", "result_label"]]]
[0, [["Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image.", "background_label"], ["In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set.", "background_label"], ["Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals.", "method_label"], ["However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge.", "method_label"], ["In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding.", "objective_label"], ["We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information.", "method_label"], ["To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals.", "method_label"], ["Experiments show that KAC Net provides a significant improvement on two popular datasets.", "result_label"]]]
[0, [["We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description.", "background_label"], ["Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when.", "background_label"], ["To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time.", "objective_label"], ["A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment.", "method_label"], ["Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions.", "method_label"], ["We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language.", "result_label"]]]
[0, [["Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\"", "background_label"], ["shares substructure with questions like \"what color is the dog?\"", "background_label"], ["and \"where is the cat?\"", "background_label"], ["This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions.", "objective_label"], ["We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural \"modules\"into deep networks for question answering.", "method_label"], ["Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.).", "method_label"], ["The resulting compound networks are jointly trained.", "method_label"], ["We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.", "result_label"]]]
[0, [["We have seen great progress in basic perceptual tasks such as object recognition and detection.", "background_label"], ["However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning.", "background_label"], ["Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding.", "background_label"], ["Previous works have established a loose, global association between QA sentences and images.", "background_label"], ["However, many questions and answers, in practice, relate to local regions in the images.", "background_label"], ["We establish a semantic link between textual descriptions and image regions by object-level grounding.", "method_label"], ["It enables a new type of QA with visual answers, in addition to textual answers used in previous work.", "method_label"], ["We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs.", "method_label"], ["Furthermore, we evaluate human performance and several baseline models on the QA tasks.", "method_label"], ["Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.", "result_label"]]]
[0, [["We propose the task of free-form and open-ended Visual Question Answering (VQA).", "objective_label"], ["Given an image and a natural language question about the image, the task is to provide an accurate natural language answer.", "objective_label"], ["Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended.", "objective_label"], ["Visual questions selectively target different areas of an image, including background details and underlying context.", "background_label"], ["As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions.", "method_label"], ["Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format.", "method_label"], ["We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides.", "method_label"], ["Numerous baselines and methods for VQA are provided and compared with human performance.", "method_label"], ["Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).", "result_label"]]]
[0, [["A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world.", "background_label"], ["In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state.", "objective_label"], ["Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects.", "method_label"], ["We propose learning these through interacting with a visual and dynamic environment.", "method_label"], ["Our proposed solution involves bootstrapping reinforcement learning with imitation learning.", "method_label"], ["To ensure cross task generalization, we develop a deep predictive model based on successor representations.", "method_label"], ["Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment.", "result_label"]]]
[0, [["This paper focuses on temporal localization of actions in untrimmed videos.", "background_label"], ["Existing methods typically train classifiers for a pre-defined list of actions and apply them in a sliding window fashion.", "background_label"], ["However, activities in the wild consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets users' needs.", "background_label"], ["We propose to localize activities by natural language queries.", "objective_label"], ["Temporal Activity Localization via Language (TALL) is challenging as it requires: (1) suitable design of text and video representations to allow cross-modal matching of actions and language queries; (2) ability to locate actions accurately given features from sliding windows of limited granularity.", "method_label"], ["We propose a novel Cross-modal Temporal Regression Localizer (CTRL) to jointly model text query and video clips, output alignment scores and action boundary regression results for candidate clips.", "method_label"], ["For evaluation, we adopt TaCoS dataset, and build a new dataset for this task on top of Charades by adding sentence temporal annotations, called Charades-STA.", "method_label"], ["We also build complex sentence queries in Charades-STA for test.", "result_label"], ["Experimental results show that CTRL outperforms previous methods significantly on both datasets.", "result_label"]]]
[0, [["We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query.", "objective_label"], ["Our method exhibits significant improvements in answering questions such as \"what color,\"where it is necessary to evaluate a specific location, and \"what room,\"where it selectively identifies informative image regions.", "method_label"], ["Our model is tested on the VQA dataset which is the largest human-annotated visual question answering dataset to our knowledge.", "result_label"]]]
[0, [["Visual attention, which assigns weights to image regions according to their relevance to a question, is considered as an indispensable part by most Visual Question Answering models.", "background_label"], ["Although the questions may involve complex relations among multiple regions, few attention models can effectively encode such cross-region relations.", "background_label"], ["In this paper, we demonstrate the importance of encoding such relations by showing the limited effective receptive field of ResNet on two datasets, and propose to model the visual attention as a multivariate distribution over a grid-structured Conditional Random Field on image regions.", "objective_label"], ["We demonstrate how to convert the iterative inference algorithms, Mean Field and Loopy Belief Propagation, as recurrent layers of an end-to-end neural network.", "method_label"], ["We empirically evaluated our model on 3 datasets, in which it surpasses the best baseline model of the newly released CLEVR dataset by 9.5%, and the best published model on the VQA dataset by 1.25%.", "result_label"], ["Source code is available at https: //github.com/zhuchen03/vqa-sva.", "other_label"]]]
[0, [["We present a framework to analyze various aspects of models for video question answering (VideoQA) using customizable synthetic datasets, which are constructed automatically from gameplay videos.", "background_label"], ["Our work is motivated by the fact that existing models are often tested only on datasets that require excessively high-level reasoning or mostly contain instances accessible through single frame inferences.", "background_label"], ["Hence, it is difficult to measure capacity and flexibility of trained models, and existing techniques often rely on ad-hoc implementations of deep neural networks without clear insight into datasets and models.", "background_label"], ["We are particularly interested in understanding temporal relationships between video events to solve VideoQA problems; this is because reasoning temporal dependency is one of the most distinct components in videos from images.", "background_label"], ["To address this objective, we automatically generate a customized synthetic VideoQA dataset using {\\em Super Mario Bros.} gameplay videos so that it contains events with different levels of reasoning complexity.", "method_label"], ["Using the dataset, we show that properly constructed datasets with events in various complexity levels are critical to learn effective models and improve overall performance.", "result_label"]]]
[0, [["Nearest neighbor search and k-nearest neighbor graph construction are two fundamental issues arise from many disciplines such as multimedia information retrieval, data-mining and machine learning.", "background_label"], ["They become more and more imminent given the big data emerge in various fields in recent years.", "background_label"], ["In this paper, a simple but effective solution both for approximate k-nearest neighbor search and approximate k-nearest neighbor graph construction is presented.", "objective_label"], ["These two issues are addressed jointly in our solution.", "method_label"], ["On the one hand, the approximate k-nearest neighbor graph construction is treated as a search task.", "method_label"], ["Each sample along with its k-nearest neighbors are joined into the k-nearest neighbor graph by performing the nearest neighbor search sequentially on the graph under construction.", "method_label"], ["On the other hand, the built k-nearest neighbor graph is used to support k-nearest neighbor search.", "method_label"], ["Since the graph is built online, the dynamic update on the graph, which is not possible from most of the existing solutions, is supported.", "method_label"], ["This solution is feasible for various distance measures.", "method_label"], ["Its effectiveness both as k-nearest neighbor construction and k-nearest neighbor search approaches is verified across different types of data in different scales, various dimensions and under different metrics.", "result_label"]]]
[0, [["Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), a generalization of Product Quantization (PQ) where a non-independent set of codebooks is used to compress vectors into small binary codes.", "background_label"], ["Unfortunately, under this scheme encoding cannot be done independently in each codebook, and optimal encoding is an NP-hard problem.", "background_label"], ["In this paper, we observe that PQ and AQ are both compositional quantizers that lie on the extremes of the codebook dependence-independence assumption, and explore an intermediate approach that exploits a hierarchical structure in the codebooks.", "method_label"], ["This results in a method that achieves quantization error on par with or lower than AQ, while being several orders of magnitude faster.", "result_label"], ["We perform a complexity analysis of PQ, AQ and our method, and evaluate our approach on standard benchmarks of SIFT and GIST descriptors, as well as on new datasets of features obtained from state-of-the-art convolutional neural networks.", "result_label"]]]
[0, [["We investigate a novel approach for image restoration by reinforcement learning.", "background_label"], ["Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks.", "method_label"], ["Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image.", "method_label"], ["We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy.", "method_label"], ["We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty.", "method_label"], ["In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain.", "result_label"]]]
[0, [["Visual object tracking is a fundamental and time-critical vision task.", "background_label"], ["Recent years have seen many shallow tracking methods based on real-time pixel-based correlation filters, as well as deep methods that have top performance but need a high-end GPU.", "background_label"], ["In this paper, we learn to improve the speed of deep trackers without losing accuracy.", "objective_label"], ["Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features.", "objective_label"], ["We formulate the adaptive tracking problem as a decision-making process, and learn an agent to decide whether to locate objects with high confidence on an early layer, or continue processing subsequent layers of a network.", "method_label"], ["This significantly reduces the feed-forward cost for easy frames with distinct or slow-moving objects.", "method_label"], ["We train the agent offline in a reinforcement learning fashion, and further demonstrate that learning all deep layers (so as to provide good features for adaptive tracking) can lead to near real-time average tracking speed of 23 fps on a single CPU while achieving state-of-the-art performance.", "result_label"], ["Perhaps most tellingly, our approach provides a 100X speedup for almost 50% of the time, indicating the power of an adaptive approach.", "result_label"]]]
[0, [["We present an attention-based model for recognizing multiple objects in images.", "background_label"], ["The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image.", "method_label"], ["We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training.", "method_label"], ["We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.", "result_label"]]]
[0, [["Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding.", "background_label"], ["Despite their success, neural networks are still hard to design.", "background_label"], ["In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.", "method_label"], ["On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy.", "method_label"], ["Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme.", "method_label"], ["On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.", "method_label"], ["Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model.", "result_label"], ["The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "result_label"]]]
[0, [["Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels.", "background_label"], ["We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution.", "objective_label"], ["Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size.", "method_label"], ["While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies.", "method_label"], ["We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.", "result_label"]]]
[0, [["With increasing demand for efficient image and video analysis, test-time cost of scene parsing becomes critical for many large-scale or time-sensitive vision applications.", "background_label"], ["We propose a dynamic hierarchical model for anytime scene labeling that allows us to achieve flexible trade-offs between efficiency and accuracy in pixel-level prediction.", "objective_label"], ["In particular, our approach incorporates the cost of feature computation and model inference, and optimizes the model performance for any given test-time budget by learning a sequence of image-adaptive hierarchical models.", "method_label"], ["We formulate this anytime representation learning as a Markov Decision Process with a discrete-continuous state-action space.", "method_label"], ["A high-quality policy of feature and model selection is learned based on an approximate policy iteration method with action proposal mechanism.", "method_label"], ["We demonstrate the advantages of our dynamic non-myopic anytime scene parsing on three semantic segmentation datasets, which achieves $90\\%$ of the state-of-the-art performances by using $15\\%$ of their overall costs.", "result_label"]]]
[0, [["At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor.", "background_label"], ["New architectures are handcrafted by careful experimentation or modified from a handful of existing networks.", "background_label"], ["We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task.", "method_label"], ["The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay.", "method_label"], ["The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task.", "method_label"], ["On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types.", "method_label"], ["We also outperform existing meta-modeling approaches for network design on image classification tasks.", "result_label"]]]
[0, [["As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models.", "background_label"], ["We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes.", "objective_label"], ["HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value.", "method_label"], ["These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training.", "method_label"], ["Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.", "result_label"]]]
[0, [["Given a large number of unlabeled face images, face grouping aims at clustering the images into individual identities present in the data.", "background_label"], ["This task remains a challenging problem despite the remarkable capability of deep learning approaches in learning face representation.", "background_label"], ["In particular, grouping results can still be egregious given profile faces and a large number of uninteresting faces and noisy detections.", "background_label"], ["Often, a user needs to correct the erroneous grouping manually.", "background_label"], ["In this study, we formulate a novel face grouping framework that learns clustering strategy from ground-truth simulated behavior.", "method_label"], ["This is achieved through imitation learning (a.k.a apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL).", "method_label"], ["In contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short- and long-term rewards.", "method_label"], ["Extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.", "result_label"]]]
[0, [["Recently, very deep convolutional neural networks (CNNs) have been attracting considerable attention in image restoration.", "background_label"], ["However, as the depth grows, the long-term dependency problem is rarely realized for these very deep models, which results in the prior states/layers having little influence on the subsequent ones.", "background_label"], ["Motivated by the fact that human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive unit and a gate unit, to explicitly mine persistent memory through an adaptive learning process.", "method_label"], ["The recursive unit learns multi-level representations of the current state under different receptive fields.", "method_label"], ["The representations and the outputs from the previous memory blocks are concatenated and sent to the gate unit, which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored.", "method_label"], ["We apply MemNet to three image restoration tasks, i.e., image denosing, super-resolution and JPEG deblocking.", "method_label"], ["Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts.", "result_label"], ["Code is available at https://github.com/tyshiwo/MemNet.", "other_label"]]]
[0, [["Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring.", "background_label"], ["Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects.", "background_label"], ["Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts.", "method_label"], ["We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network.", "method_label"], ["Following a similar \"easy to hard\"idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low-level vision problems.", "method_label"], ["Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use case (i.e.", "result_label"], ["Twitter).", "result_label"], ["In addition, we show that our method can be applied as pre-processing to facilitate other low-level vision routines when they take compressed images as input.", "result_label"]]]
[0, [["Salient segmentation aims to segment out attention-grabbing regions, a critical yet challenging task and the foundation of many high-level computer vision applications.", "objective_label"], ["It requires semantic-aware grouping of pixels into salient regions and benefits from the utilization of global multi-scale contexts to achieve good local reasoning.", "background_label"], ["Previous works often address it as two-class segmentation problems utilizing complicated multi-step procedures including refinement networks and complex graphical models.", "background_label"], ["We argue that semantic salient segmentation can instead be effectively resolved by reformulating it as a simple yet intuitive pixel-pair based connectivity prediction task.", "method_label"], ["Following the intuition that salient objects can be naturally grouped via semantic-aware connectivity between neighboring pixels, we propose a pure Connectivity Net (ConnNet).", "method_label"], ["ConnNet predicts connectivity probabilities of each pixel with its neighboring pixels by leveraging multi-level cascade contexts embedded in the image and long-range pixel relations.", "method_label"], ["We investigate our approach on two tasks, namely salient object segmentation and salient instance-level segmentation, and illustrate that consistent improvements can be obtained by modeling these tasks as connectivity instead of binary segmentation tasks for a variety of network architectures.", "method_label"], ["We achieve state-of-the-art performance, outperforming or being comparable to existing approaches while reducing inference time due to our less complex approach.", "result_label"]]]
[0, [["Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision.", "background_label"], ["In this CVPR 2015 paper, we discover that a high-quality visual saliency model can be trained with multiscale features extracted using a popular deep learning architecture, convolutional neural networks (CNNs), which have had many successes in visual recognition tasks.", "method_label"], ["For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for extracting features at three different scales.", "method_label"], ["We then propose a refinement method to enhance the spatial coherence of our saliency results.", "method_label"], ["Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation.", "method_label"], ["To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotation.", "method_label"], ["Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets.", "result_label"]]]
[0, [["In this paper we provide an extensive evaluation of fixation prediction and salient object segmentation algorithms as well as statistics of major datasets.", "background_label"], ["Our analysis identifies serious design flaws of existing salient object benchmarks, called the dataset design bias, by over emphasizing the stereotypical concepts of saliency.", "background_label"], ["The dataset design bias does not only create the discomforting disconnection between fixations and salient object segmentation, but also misleads the algorithm designing.", "background_label"], ["Based on our analysis, we propose a new high quality dataset that offers both fixation and salient object segmentation ground-truth.", "method_label"], ["With fixations and salient object being presented simultaneously, we are able to bridge the gap between fixations and salient objects, and propose a novel method for salient object segmentation.", "method_label"], ["Finally, we report significant benchmark progress on three existing datasets of segmenting salient objects", "result_label"]]]
[0, [["Recent progress on saliency detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs).", "background_label"], ["Semantic segmentation and saliency detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs).", "background_label"], ["There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem.", "background_label"], ["Holistically-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on salience detection is not obvious.", "background_label"], ["In this paper, we propose a new method for saliency detection by introducing short connections to the skip-layer structures within the HED architecture.", "method_label"], ["Our framework provides rich multi-scale feature maps at each layer, a property that is critically needed to perform segment detection.", "method_label"], ["Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.15 seconds per image), effectiveness, and simplicity over the existing algorithms.", "result_label"]]]
[0, [["In the past few years, Generative Adversarial Network (GAN) became a prevalent research topic.", "background_label"], ["By defining two convolutional neural networks (G-Network and D-Network) and introducing an adversarial procedure between them during the training process, GAN has ability to generate good quality images that look like natural images from a random vector.", "background_label"], ["Besides image generation, GAN may have potential to deal with wide range of real world problems.", "background_label"], ["In this paper, we follow the basic idea of GAN and propose a novel model for image saliency detection, which is called Supervised Adversarial Networks (SAN).", "method_label"], ["Specifically, SAN also trains two models simultaneously: the G-Network takes natural images as inputs and generates corresponding saliency maps (synthetic saliency maps), and the D-Network is trained to determine whether one sample is a synthetic saliency map or ground-truth saliency map.", "method_label"], ["However, different from GAN, the proposed method uses fully supervised learning to learn both G-Network and D-Network by applying class labels of the training set.", "method_label"], ["Moreover, a novel kind of layer call conv-comparison layer is introduced into the D-Network to further improve the saliency performance by forcing the high-level feature of synthetic saliency maps and ground-truthes as similar as possible.", "method_label"], ["Experimental results on Pascal VOC 2012 database show that the SAN model can generate high quality saliency maps for many complicate natural images.", "result_label"]]]
[0, [["Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks (CNNs).", "background_label"], ["However, existing CNN-based methods operate at the patch level instead of the pixel level.", "background_label"], ["Resulting saliency maps are typically blurry, especially near the boundary of salient objects.", "background_label"], ["Furthermore, image patches are treated as independent samples even when they are overlapping, giving rise to significant redundancy in computation and storage.", "background_label"], ["In this CVPR 2016 paper, we propose an end-to-end deep contrast network to overcome the aforementioned limitations.", "objective_label"], ["Our deep network consists of two complementary components, a pixel-level fully convolutional stream and a segment-wise spatial pooling stream.", "method_label"], ["The first stream directly produces a saliency map with pixel-level accuracy from an input image.", "method_label"], ["The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries.", "method_label"], ["Finally, a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams.", "method_label"], ["Experimental results demonstrate that our deep model significantly improves the state of the art.", "result_label"]]]
[0, [["We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it.", "objective_label"], ["We call this task Simultaneous Detection and Segmentation (SDS).", "objective_label"], ["Unlike classical bounding box detection, SDS requires a segmentation and not just a box.", "background_label"], ["Unlike classical semantic segmentation, we require individual object instances.", "background_label"], ["We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS.", "method_label"], ["We then use category-specific, top- down figure-ground predictions to refine our bottom-up proposals.", "method_label"], ["We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection.", "result_label"], ["Finally, we provide diagnostic tools that unpack performance and provide directions for future work.", "result_label"]]]
[0, [["Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image.", "background_label"], ["Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing opportunities for joint learning.", "background_label"], ["Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially.", "method_label"], ["The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time.", "method_label"], ["This net is provided with a spatial memory that keeps track of what pixels have been explained and allows occlusion handling.", "method_label"], ["In order to train the model we designed a principled loss function that accurately represents the properties of the instance segmentation problem.", "method_label"], ["In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting.", "result_label"]]]
[0, [["We introduce SalGAN, a deep convolutional neural network for visual saliency prediction trained with adversarial examples.", "background_label"], ["The first stage of the network consists of a generator model whose weights are learned by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency maps.", "method_label"], ["The resulting prediction is processed by a discriminator network trained to solve a binary classification task between the saliency maps generated by the generative stage and the ground truth ones.", "method_label"], ["Our experiments show how adversarial training allows reaching state-of-the-art performance across different metrics when combined with a widely-used loss function like BCE.", "result_label"], ["Our results can be reproduced with the source code and trained models available at https://imatge-upc.github.io/saliency-salgan-2017/.", "result_label"]]]
[0, [["Human motion prediction aims at generating future frames of human motion based on an observed sequence of skeletons.", "background_label"], ["Recent methods employ the latest hidden states of a recurrent neural network (RNN) to encode the historical skeletons, which can only address short-term prediction.", "background_label"], ["In this work, we propose a motion context modeling by summarizing the historical human motion with respect to the current prediction.", "objective_label"], ["A modified highway unit (MHU) is proposed for efficiently eliminating motionless joints and estimating next pose given the motion context.", "method_label"], ["Furthermore, we enhance the motion dynamic by minimizing the gram matrix loss for long-term motion prediction.", "method_label"], ["Experimental results show that the proposed model can promisingly forecast the human future movements, which yields superior performances over related state-of-the-art approaches.", "result_label"], ["Moreover, specifying the motion context with the activity labels enables our model to perform human motion transfer.", "result_label"]]]
[0, [["We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture.", "objective_label"], ["The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers.", "background_label"], ["We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos.", "method_label"], ["Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoid drifting for long periods of time.", "method_label"], ["For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions.", "method_label"], ["For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow.", "method_label"], ["ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics.", "method_label"], ["Our experiments show such representation learning is crucial for both labeling and prediction in space-time.", "result_label"], ["We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.", "result_label"]]]
[0, [["Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality.", "background_label"], ["Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis.", "objective_label"], ["We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state-of-the-art performance can be achieved by a simple baseline that does not attempt to model motion at all.", "method_label"], ["We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches.", "method_label"], ["We propose three changes to the standard RNN models typically used for human motion, which result in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.", "result_label"]]]
[0, [["Random data augmentation is a critical technique to avoid overfitting in training deep neural network models.", "background_label"], ["However, data augmentation and network training are usually treated as two isolated processes, limiting the effectiveness of network training.", "background_label"], ["Why not jointly optimize the two?", "background_label"], ["We propose adversarial data augmentation to address this limitation.", "objective_label"], ["The main idea is to design an augmentation network (generator) that competes against a target network (discriminator) by generating `hard' augmentation operations online.", "method_label"], ["The augmentation network explores the weaknesses of the target network, while the latter learns from `hard' augmentations to achieve better performance.", "method_label"], ["We also design a reward/penalty strategy for effective joint training.", "method_label"], ["We demonstrate our approach on the problem of human pose estimation and carry out a comprehensive experimental analysis, showing that our method can significantly improve state-of-the-art models without additional data efforts.", "result_label"]]]
[0, [["In this paper we consider the problem of human pose estimation from a single still image.", "background_label"], ["We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional neural net.", "method_label"], ["The voting scheme allows us to utilize information from the whole image, rather than rely on a sparse set of keypoint locations.", "method_label"], ["Using dense, multi-target votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint probabilities by looking at consensus voting.", "method_label"], ["This differs from most previous methods where joint probabilities are learned from relative keypoint locations and are independent of the image.", "method_label"], ["We finally combine the keypoints votes and joint probabilities in order to identify the optimal pose configuration.", "method_label"], ["We show our competitive performance on the MPII Human Pose and Leeds Sports Pose datasets.", "result_label"]]]
[0, [["Pose Machines provide a sequential prediction framework for learning rich implicit spatial models.", "background_label"], ["In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.", "background_label"], ["The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation.", "objective_label"], ["We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.", "method_label"], ["Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure.", "method_label"], ["We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.", "result_label"]]]
[0, [["This work introduces a novel convolutional network architecture for the task of human pose estimation.", "background_label"], ["Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body.", "background_label"], ["We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network.", "method_label"], ["We refer to the architecture as a \"stacked hourglass\"network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions.", "method_label"], ["State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.", "result_label"]]]
[0, [["This paper presents a deep learning based approach to the problem of human pose estimation.", "background_label"], ["We employ generative adversarial networks as our learning paradigm in which we set up two stacked hourglass networks with the same architecture, one as the generator and the other as the discriminator.", "method_label"], ["The generator is used as a human pose estimator after the training is done.", "method_label"], ["The discriminator distinguishes ground-truth heatmaps from generated ones, and back-propagates the adversarial loss to the generator.", "method_label"], ["This process enables the generator to learn plausible human body configurations and is shown to be useful for improving the prediction accuracy.", "result_label"]]]
[0, [["This paper is on human pose estimation using Convolutional Neural Networks.", "background_label"], ["Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions.", "background_label"], ["To this end, we propose a detection-followed-by-regression CNN cascade.", "method_label"], ["The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps.", "method_label"], ["The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context.", "method_label"], ["More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts.", "method_label"], ["Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning.", "method_label"], ["Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets.", "result_label"], ["Code can be downloaded from http://www.cs.nott.ac.uk/~psxab5/", "result_label"]]]
[0, [["How do we learn an object detector that is invariant to occlusions and deformations?", "background_label"], ["Our current solution is to use a data-driven strategy -- collect large-scale datasets which have object instances under different conditions.", "background_label"], ["The hope is that the final classifier can use these examples to learn invariances.", "background_label"], ["But is it really possible to see all the occlusions in a dataset?", "background_label"], ["We argue that like categories, occlusions and object deformations also follow a long-tail.", "result_label"], ["Some occlusions and deformations are so rare that they hardly happen; yet we want to learn a model invariant to such occurrences.", "result_label"], ["In this paper, we propose an alternative solution.", "objective_label"], ["We propose to learn an adversarial network that generates examples with occlusions and deformations.", "objective_label"], ["The goal of the adversary is to generate examples that are difficult for the object detector to classify.", "objective_label"], ["In our framework both the original detector and adversary are learned in a joint manner.", "method_label"], ["Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline.", "result_label"], ["We also release the code for this paper.", "result_label"]]]
[0, [["Articulated human pose estimation is a fundamental yet challenging task in computer vision.", "background_label"], ["The difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens.", "background_label"], ["Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored.", "background_label"], ["In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs.", "objective_label"], ["Given input features, the PRMs learn convolutional filters on various scales of input features, which are obtained with different subsampling ratios in a multi-branch network.", "method_label"], ["Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently.", "method_label"], ["Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures.", "method_label"], ["We investigate our method on two standard benchmarks for human pose estimation.", "method_label"], ["Our approach obtains state-of-the-art results on both benchmarks.", "result_label"], ["Code is available at https://github.com/bearpaw/PyraNet.", "result_label"]]]
[0, [["Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classification tasks using purely feedforward processing.", "background_label"], ["Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation.", "background_label"], ["Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback.", "method_label"], ["Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF).", "method_label"], ["IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.", "result_label"]]]
[0, [["Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets).", "background_label"], ["Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training.", "background_label"], ["These benefits of pooling come at the cost of reduced localization accuracy.", "background_label"], ["We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image.", "method_label"], ["This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation.", "method_label"], ["We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.", "result_label"]]]
[0, [["We propose a method for human pose estimation based on Deep Neural Networks (DNNs).", "method_label"], ["The pose estimation is formulated as a DNN-based regression problem towards body joints.", "method_label"], ["We present a cascade of such DNN regressors which results in high precision pose estimates.", "method_label"], ["The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning.", "method_label"], ["We present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images.", "result_label"]]]
[0, [["In this paper, we present an adaptation of the sequence-to-sequence model for structured output prediction in vision tasks.", "background_label"], ["In this model the output variables for a given input are predicted sequentially using neural networks.", "background_label"], ["The prediction for each output variable depends not only on the input but also on the previously predicted output variables.", "method_label"], ["The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each time step.", "method_label"], ["We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied.", "method_label"], ["Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted in different steps.", "method_label"], ["We show that chained predictions achieve top performing results on human pose estimation from single images and videos.", "result_label"]]]
[0, [["Unsupervised depth estimation from a single image is a very attractive technique with several implications in robotic, autonomous navigation, augmented reality and so on.", "background_label"], ["This topic represents a very challenging task and the advent of deep learning enabled to tackle this problem with excellent results.", "background_label"], ["However, these architectures are extremely deep and complex.", "background_label"], ["Thus, real-time performance can be achieved only by leveraging power-hungry GPUs that do not allow to infer depth maps in application fields characterized by low-power constraints.", "background_label"], ["To tackle this issue, in this paper we propose a novel architecture capable to quickly infer an accurate depth map on a CPU, even of an embedded system, using a pyramid of features extracted from a single input image.", "method_label"], ["Similarly to state-of-the-art, we train our network in an unsupervised manner casting depth estimation as an image reconstruction problem.", "method_label"], ["Extensive experimental results on the KITTI dataset show that compared to the top performing approach our network has similar accuracy but a much lower complexity (about 6% of parameters) enabling to infer a depth map for a KITTI image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard CPU.", "result_label"], ["Moreover, by trading accuracy for efficiency, our network allows to infer maps at about 2 Hz and 40 Hz respectively, still being more accurate than most state-of-the-art slower methods.", "result_label"], ["To the best of our knowledge, it is the first method enabling such performance on CPUs paving the way for effective deployment of unsupervised monocular depth estimation even on embedded systems.", "result_label"]]]
[0, [["This paper addresses the problem of estimating the depth map of a scene given a single RGB image.", "background_label"], ["We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps.", "objective_label"], ["In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network.", "method_label"], ["For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps.", "method_label"], ["Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps.", "method_label"], ["As a result, it runs in real-time on images or videos.", "method_label"], ["In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation.", "result_label"], ["Code and models are publicly available.", "result_label"]]]
[0, [["In this paper we formulate structure from motion as a learning problem.", "background_label"], ["We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs.", "background_label"], ["The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions.", "method_label"], ["The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching.", "method_label"], ["A crucial component of the approach is a training loss based on spatial relative differences.", "method_label"], ["Compared to traditional two-frame structure from motion methods, results are more accurate and more robust.", "result_label"], ["In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.", "result_label"]]]
[0, [["Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision, but their use in graphics problems has been limited.", "background_label"], ["In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets.", "background_label"], ["In contrast to traditional approaches which consist of multiple complex stages of processing, each of which require careful tuning and can fail in unexpected ways, our system is trained end-to-end.", "method_label"], ["The pixels from neighboring views of a scene are presented to the network which then directly produces the pixels of the unseen view.", "method_label"], ["The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes.", "method_label"], ["We believe this is due to the end-to-end nature of our system which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data.", "method_label"], ["To verify our method we show that it can convincingly reproduce known test views from nearby imagery.", "method_label"], ["Additionally we show images rendered from novel viewpoints.", "result_label"], ["To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.", "result_label"]]]
[0, [["In the past few years, convolutional neural nets (CNN) have shown incredible promise for learning visual representations.", "background_label"], ["In this paper, we use CNNs for the task of predicting surface normals from a single image.", "background_label"], ["But what is the right architecture we should use?", "background_label"], ["We propose to build upon the decades of hard work in 3D scene understanding, to design new CNN architecture for the task of surface normal estimation.", "objective_label"], ["We show by incorporating several constraints (man-made, manhattan world) and meaningful intermediate representations (room layout, edge labels) in the architecture leads to state of the art performance on surface normal estimation.", "method_label"], ["We also show that our network is quite robust and show state of the art results on other datasets as well without any fine-tuning.", "result_label"]]]
[0, [["We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning.", "background_label"], ["This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow.", "method_label"], ["Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update.", "method_label"], ["Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid.", "method_label"], ["This has several advantages.", "method_label"], ["First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters.", "method_label"], ["This makes it more efficient and appropriate for embedded applications.", "method_label"], ["Second, since the flow at each pyramid level is small (< 1 pixel), a convolutional approach applied to pairs of warped images is appropriate.", "method_label"], ["Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it.", "method_label"], ["Our results are more accurate than FlowNet on most standard benchmarks, suggesting a new direction of combining classical flow methods with deep learning.", "result_label"]]]
[0, [["There is large consent that successful training of deep networks requires many thousand annotated training samples.", "background_label"], ["In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.", "objective_label"], ["The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization.", "method_label"], ["We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks.", "method_label"], ["Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin.", "method_label"], ["Moreover, the network is fast.", "method_label"], ["Segmentation of a 512x512 image takes less than a second on a recent GPU.", "result_label"], ["The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .", "other_label"]]]
[0, [["Recent years have witnessed the breakthrough success of deep convolutional neural networks (DCNNs) in image classification and other vision applications.", "background_label"], ["Although freeing users from the troublesome handcrafted feature extraction by providing a uniform feature extraction-classification framework, DCNNs still require a handcrafted design of their architectures.", "background_label"], ["In this paper, we propose the genetic DCNN designer, an autonomous learning algorithm can generate a DCNN architecture automatically based on the data available for a specific image classification problem.", "objective_label"], ["We first partition a DCNN into multiple stacked meta convolutional blocks and fully connected blocks, each containing the operations of convolution, pooling, fully connection, batch normalization, activation and drop out, and thus convert the architecture into an integer vector.", "method_label"], ["Then, we use refined evolutionary operations, including selection, mutation and crossover to evolve a population of DCNN architectures.", "method_label"], ["Our results on the MNIST, Fashion-MNIST, EMNISTDigit, EMNIST-Letter, CIFAR10 and CIFAR100 datasets suggest that the proposed genetic DCNN designer is able to produce automatically DCNN architectures, whose performance is comparable to, if not better than, that of stateof- the-art DCNN models", "result_label"]]]
[0, [["The success of deep learning depends on finding an architecture to fit the task.", "background_label"], ["As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand.", "background_label"], ["This paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution.", "method_label"], ["By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling.", "method_label"], ["It also supports building a real-world application of automated image captioning on a magazine website.", "method_label"], ["Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.", "result_label"]]]
[0, [["For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting.", "background_label"], ["PathNet is a first step in this direction.", "background_label"], ["It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks.", "method_label"], ["Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm.", "method_label"], ["During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation.", "method_label"], ["Pathway fitness is the performance of that pathway measured according to a cost function.", "method_label"], ["We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning.", "method_label"], ["Paths evolved on task B re-use parts of the optimal path evolved on task A.", "result_label"], ["Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training.", "result_label"], ["Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).", "result_label"]]]
[0, [["This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network.", "background_label"], ["Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network.", "background_label"], ["Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster.", "background_label"], ["The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers.", "objective_label"], ["Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.", "result_label"], ["Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.", "result_label"]]]
[0, [["In a traditional convolutional layer, the learned filters stay fixed after training.", "background_label"], ["In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input.", "background_label"], ["We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters.", "background_label"], ["A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction.", "method_label"], ["Moreover, multiple such layers can be combined, e.g.", "method_label"], ["in a recurrent architecture.", "method_label"], ["We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model.", "result_label"], ["By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data.", "result_label"], ["This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.", "result_label"]]]
[0, [["Most machine learning methods are known to capture and exploit biases of the training data.", "background_label"], ["While some biases are beneficial for learning, others are harmful.", "background_label"], ["Specifically, image captioning models tend to exaggerate biases present in training data.", "background_label"], ["This can lead to incorrect captions in domains where unbiased captions are desired, or required, due to over reliance on the learned prior and image context.", "background_label"], ["We investigate generation of gender specific caption words (e.g.", "background_label"], ["man, woman) based on the person's appearance or the image context.", "method_label"], ["We introduce a new Equalizer model that ensures equal gender probability when gender evidence is occluded in a scene and confident predictions when gender evidence is present.", "method_label"], ["The resulting model is forced to look at a person rather than use contextual cues to make a gender specific prediction.", "method_label"], ["The losses that comprise our model, the Appearance Confusion Loss and the Confident Loss, are general, and can be added to any description model in order to mitigate impacts of unwanted bias in a description dataset.", "method_label"], ["Our proposed model has lower error than prior work when describing images with people and mentioning their gender and more closely matches the ground truth ratio of sentences including women to sentences including men.", "result_label"]]]
[0, [["We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features.", "objective_label"], ["Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition.", "method_label"], ["Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.", "method_label"], ["In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures.", "method_label"], ["We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.", "result_label"], ["We illustrate our notion using a case study of FICO credit scores.", "result_label"]]]
[0, [["The notions of concreteness and imageability, traditionally important in psycholinguistics, are gaining significance in semantic-oriented natural language processing tasks.", "background_label"], ["In this paper we investigate the predictability of these two concepts via supervised learning, using word embeddings as explanatory variables.", "objective_label"], ["We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space.", "method_label"], ["We show that the notions of concreteness and imageability are highly predictable both within and across languages, with a moderate loss of up to 20% in correlation when predicting across languages.", "result_label"], ["We further show that the cross-lingual transfer via word embeddings is more efficient than the simple transfer via bilingual dictionaries.", "result_label"]]]
[0, [["Embeddings are generic representations that are useful for many NLP tasks.", "background_label"], ["In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space.", "method_label"], ["We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information - sentiment, concreteness and frequency.", "method_label"], ["On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.", "result_label"]]]
[0, [["Estimating scene flow in RGB-D videos is attracting much interest of the computer vision researchers, due to its potential applications in robotics.", "background_label"], ["The state-of-the-art techniques for scene flow estimation, typically rely on the knowledge of scene structure of the frame and the correspondence between frames.", "background_label"], ["However, with the increasing amount of RGB-D data captured from sophisticated sensors like Microsoft Kinect, and the recent advances in the area of sophisticated deep learning techniques, introduction of an efficient deep learning technique for scene flow estimation, is becoming important.", "background_label"], ["This paper introduces a first effort to apply a deep learning method for direct estimation of scene flow by presenting a fully convolutional neural network with an encoder-decoder (ED) architecture.", "method_label"], ["The proposed network SceneEDNet involves estimation of three dimensional motion vectors of all the scene points from sequence of stereo images.", "method_label"], ["The training for direct estimation of scene flow is done using consecutive pairs of stereo images and corresponding scene flow ground truth.", "method_label"], ["The proposed architecture is applied on a huge dataset and provides meaningful results.", "method_label"]]]
[0, [["We introduce a novel multiframe scene flow approach that jointly optimizes the consistency of the patch appearances and their local rigid motions from RGB-D image sequences.", "method_label"], ["In contrast to the competing methods, we take advantage of an oversegmentation of the reference frame and robust optimization techniques.", "method_label"], ["We formulate scene flow recovery as a global non-linear least squares problem which is iteratively solved by a damped Gauss-Newton approach.", "method_label"], ["As a result, we obtain a qualitatively new level of accuracy in RGB-D based scene flow estimation which can potentially run in real-time.", "method_label"], ["Our method can handle challenging cases with rigid, piecewise rigid, articulated and moderate non-rigid motion, and does not rely on prior knowledge about the types of motions and deformations.", "method_label"], ["Extensive experiments on synthetic and real data show that our method outperforms state-of-the-art.", "result_label"]]]
[0, [["The FlowNet demonstrated that optical flow estimation can be cast as a learning problem.", "background_label"], ["However, the state of the art with regard to the quality of the flow has still been defined by traditional methods.", "background_label"], ["Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods.", "background_label"], ["In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well.", "objective_label"], ["The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important.", "method_label"], ["Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow.", "method_label"], ["Third, we elaborate on small displacements by introducing a sub-network specializing on small motions.", "method_label"], ["FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%.", "method_label"], ["It performs on par with state-of-the-art methods, while running at interactive frame rates.", "method_label"], ["Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.", "result_label"]]]
[0, [["The computational requirements for training deep neural networks (DNNs) have grown to the point that it is now standard practice to parallelize training.", "background_label"], ["Existing deep learning systems commonly use data or model parallelism, but unfortunately, these strategies often result in suboptimal parallelization performance.", "background_label"], ["In this paper, we define a more comprehensive search space of parallelization strategies for DNNs called SOAP, which includes strategies to parallelize a DNN in the Sample, Operation, Attribute, and Parameter dimensions.", "objective_label"], ["We also propose FlexFlow, a deep learning framework that uses guided randomized search of the SOAP space to find a fast parallelization strategy for a specific parallel machine.", "method_label"], ["To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy's performance and is three orders of magnitude faster than prior approaches that have to execute each strategy.", "method_label"], ["We evaluate FlexFlow with six real-world DNN benchmarks on two GPU clusters and show that FlexFlow can increase training throughput by up to 3.8x over state-of-the-art approaches, even when including its search time, and also improves scalability.", "result_label"]]]
[0, [["I present a new way to parallelize the training of convolutional neural networks across multiple GPUs.", "background_label"], ["The method scales significantly better than all alternatives when applied to modern convolutional neural networks.", "result_label"]]]
[0, [["The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks.", "background_label"], ["Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs.", "background_label"], ["Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions.", "background_label"], ["In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs.", "method_label"], ["Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices.", "method_label"], ["The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model.", "method_label"], ["Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.", "result_label"]]]
[0, [["Traditional multiple object tracking methods divide the task into two parts: affinity learning and data association.", "background_label"], ["The separation of the task requires to define a hand-crafted training goal in affinity learning stage and a hand-crafted cost function of data association stage, which prevents the tracking goals from learning directly from the feature.", "background_label"], ["In this paper, we present a new multiple object tracking (MOT) framework with data-driven association method, named as Tracklet Association Tracker (TAT).", "objective_label"], ["The framework aims at gluing feature learning and data association into a unity by a bi-level optimization formulation so that the association results can be directly learned from features.", "method_label"], ["To boost the performance, we also adopt the popular hierarchical association and perform the necessary alignment and selection of raw detection responses.", "method_label"], ["Our model trains over 20X faster than a similar approach, and achieves the state-of-the-art performance on both MOT2016 and MOT2017 benchmarks.", "result_label"]]]
[0, [["Multiple Object Tracking (MOT) is an important computer vision problem which has gained increasing attention due to its academic and commercial potential.", "background_label"], ["Although different kinds of approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions.", "background_label"], ["In this work, we contribute the first comprehensive and most recent review on this problem.", "objective_label"], ["We inspect the recent advances in various aspects and propose some interesting directions for future research.", "method_label"], ["To the best of our knowledge, there has not been any extensive review on this topic in the community.", "result_label"], ["We endeavor to provide a thorough review on the development of this problem in recent decades.", "background_label"], ["The main contributions of this review are fourfold: 1) Key aspects in a multiple object tracking system, including formulation, categorization, key principles, evaluation of an MOT are discussed.", "method_label"], ["2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks.", "method_label"], ["3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative comparisons.", "method_label"], ["We also point to some interesting discoveries by analyzing these results.", "method_label"], ["4) We provide a discussion about issues of MOT research, as well as some interesting directions which could possibly become potential research effort in the future.", "result_label"]]]
[0, [["Numerous powerful point process models have been developed to understand temporal patterns in sequential data from fields such as health-care, electronic commerce, social networks, and natural disaster forecasting.", "background_label"], ["In this paper, we develop novel models for learning the temporal distribution of human activities in streaming data (e.g., videos and person trajectories).", "objective_label"], ["We propose an integrated framework of neural networks and temporal point processes for predicting when the next activity will happen.", "objective_label"], ["Because point processes are limited to taking event frames as input, we propose a simple yet effective mechanism to extract features at frames of interest while also preserving the rich information in the remaining frames.", "method_label"], ["We evaluate our model on two challenging datasets.", "result_label"], ["The results show that our model outperforms traditional statistical point process approaches significantly, demonstrating its effectiveness in capturing the underlying temporal dynamics as well as the correlation within sequential activities.", "result_label"], ["Furthermore, we also extend our model to a joint estimation framework for predicting the timing, spatial location, and category of the activity simultaneously, to answer the when, where, and what of activity prediction.", "result_label"]]]
[0, [["Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications.", "background_label"], ["This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down.", "background_label"], ["We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video.", "background_label"], ["We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects.", "objective_label"], ["The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future.", "method_label"], ["Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute.", "method_label"], ["We then apply recognition algorithms on our predicted representation to anticipate objects and actions.", "method_label"], ["We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.", "result_label"]]]
[0, [["We propose a weakly-supervised framework for action labeling in video, where only the order of occurring actions is required during training time.", "background_label"], ["The key challenge is that the per-frame alignments between the input (video) and label (action) sequences are unknown during training.", "background_label"], ["We address this by introducing the Extended Connectionist Temporal Classification (ECTC) framework to efficiently evaluate all possible alignments via dynamic programming and explicitly enforce their consistency with frame-to-frame visual similarities.", "method_label"], ["This protects the model from distractions of visually inconsistent or degenerated alignments without the need of temporal supervision.", "method_label"], ["We further extend our framework to the semi-supervised case when a few frames are sparsely annotated in a video.", "method_label"], ["With less than 1% of labeled frames per video, our method is able to outperform existing semi-supervised approaches and achieve comparable performance to that of fully supervised approaches.", "result_label"]]]
[0, [["Automatic clinical diagnosis of retinal diseases has emerged as a promising approach to facilitate discovery in areas with limited access to specialists.", "background_label"], ["Based on the fact that fundus structure and vascular disorders are the main characteristics of retinal diseases, we propose a novel visual-assisted diagnosis hybrid model mixing the support vector machine (SVM) and deep neural networks (DNNs).", "method_label"], ["Furthermore, we present a new clinical retina dataset, called EyeNet2, for ophthalmology incorporating 52 retina diseases classes.", "method_label"], ["Using EyeNet2, our model achieves 90.43\\% diagnosis accuracy, and the model performance is comparable to the professional ophthalmologists.", "result_label"]]]
[0, [["Visual Question Answering (VQA) models should have both high robustness and accuracy.", "background_label"], ["Unfortunately, most of the current VQA research only focuses on accuracy because there is a lack of proper methods to measure the robustness of VQA models.", "background_label"], ["There are two main modules in our algorithm.", "background_label"], ["Given a natural language question about an image, the first module takes the question as input and then outputs the ranked basic questions, with similarity scores, of the main given question.", "method_label"], ["The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question about the given image.", "method_label"], ["We claim that a robust VQA model is one, whose performance is not changed much when related basic questions as also made available to it as input.", "method_label"], ["We formulate the basic questions generation problem as a LASSO optimization, and also propose a large scale Basic Question Dataset (BQD) and Rscore (novel robustness measure), for analyzing the robustness of VQA models.", "method_label"], ["We hope our BQD will be used as a benchmark for to evaluate the robustness of VQA models, so as to help the community build more robust and accurate VQA models.", "result_label"]]]
[0, [["Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA).", "background_label"], ["There are two main modules in our algorithm.", "background_label"], ["Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question.", "method_label"], ["The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question.", "method_label"], ["We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question.", "method_label"], ["Our method is evaluated on the challenging VQA dataset and yields state-of-the-art accuracy, 60.34% in open-ended task.", "result_label"]]]
[0, [["Dimensionality reduction is a topic of recent interest.", "background_label"], ["In this paper, we present the classification constrained dimensionality reduction (CCDR) algorithm to account for label information.", "objective_label"], ["The algorithm can account for multiple classes as well as the semi-supervised setting.", "background_label"], ["We present an out-of-sample expressions for both labeled and unlabeled data.", "method_label"], ["For unlabeled data, we introduce a method of embedding a new point as preprocessing to a classifier.", "method_label"], ["For labeled data, we introduce a method that improves the embedding during the training phase using the out-of-sample extension.", "method_label"], ["We investigate classification performance using the CCDR algorithm on hyper-spectral satellite imagery data.", "method_label"], ["We demonstrate the performance gain for both local and global classifiers and demonstrate a 10% improvement of the $k$-nearest neighbors algorithm performance.", "result_label"], ["We present a connection between intrinsic dimension estimation and the optimal embedding dimension obtained using the CCDR algorithm.", "result_label"]]]
[0, [["Deep neural networks have been playing an essential role in many computer vision tasks including Visual Question Answering (VQA).", "background_label"], ["Until recently, the study of their accuracy was the main focus of research but now there is a trend toward assessing the robustness of these models against adversarial attacks by evaluating their tolerance to varying noise levels.", "background_label"], ["In VQA, adversarial attacks can target the image and/or the proposed main question and yet there is a lack of proper analysis of the later.", "background_label"], ["In this work, we propose a flexible framework that focuses on the language part of VQA that uses semantically relevant questions, dubbed basic questions, acting as controllable noise to evaluate the robustness of VQA models.", "method_label"], ["We hypothesize that the level of noise is positively correlated to the similarity of a basic question to the main question.", "method_label"], ["Hence, to apply noise on any given main question, we rank a pool of basic questions based on their similarity by casting this ranking task as a LASSO optimization problem.", "method_label"], ["Then, we propose a novel robustness measure, R_score, and two large-scale basic question datasets (BQDs) in order to standardize robustness analysis for VQA models.", "result_label"]]]
[0, [["Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers.", "background_label"], ["Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation.", "background_label"], ["In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions.", "objective_label"], ["Ithemal uses a hierarchical LSTM--based approach to predict throughput based on the opcodes and operands of instructions in a basic block.", "method_label"], ["We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers.", "method_label"], ["In particular, our model has less than half the error of state-of-the-art analytical models (LLVM's llvm-mca and Intel's IACA).", "method_label"], ["Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.", "result_label"]]]
[0, [["Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks.", "background_label"], ["Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence.", "background_label"], ["In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism.", "method_label"], ["We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information.", "method_label"], ["We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.", "result_label"]]]
[0, [["Training recurrent neural networks to model long term dependencies is difficult.", "background_label"], ["Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize.", "objective_label"], ["Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs.", "method_label"], ["We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text.", "method_label"], ["We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA.", "method_label"], ["On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task.", "method_label"], ["Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.", "result_label"]]]
[0, [["The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy.", "background_label"], ["Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video.", "objective_label"], ["Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression.", "method_label"], ["The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE.", "method_label"], ["Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content.", "result_label"], ["Extreme compression performance is achieved when training the model on specialized content.", "result_label"]]]
[0, [["A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements.", "background_label"], ["Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness.", "background_label"], ["Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users.", "background_label"], ["Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks.", "objective_label"], ["Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits.", "method_label"], ["On a large-scale benchmark of 32$\\times$32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more.", "result_label"]]]
[0, [["This paper presents a set of full-resolution lossy image compression methods based on neural networks.", "background_label"], ["Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once.", "background_label"], ["All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding.", "method_label"], ["We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet.", "method_label"], ["We also study \"one-shot\"versus additive reconstruction architectures and introduce a new scaled-additive framework.", "method_label"], ["We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used.", "result_label"], ["As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.", "result_label"]]]
[0, [["We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio.", "background_label"], ["Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content).", "background_label"], ["This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features.", "method_label"], ["In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping.", "method_label"], ["For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics.", "method_label"], ["Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.", "result_label"]]]
[0, [["In this paper, we introduce a stochastic dynamics video infilling (SDVI) framework to generate frames between long intervals in a video.", "background_label"], ["Our task differs from video interpolation which aims to produce transitional frames for a short interval between every two frames and increase the temporal resolution.", "objective_label"], ["Our task, namely video infilling, however, aims to infill long intervals with plausible frame sequences.", "objective_label"], ["Our framework models the infilling as a constrained stochastic generation process and sequentially samples dynamics from the inferred distribution.", "method_label"], ["SDVI consists of two parts: (1) a bi-directional constraint propagation module to guarantee the spatial-temporal coherence among frames, (2) a stochastic sampling process to generate dynamics from the inferred distributions.", "method_label"], ["Experimental results show that SDVI can generate clear frame sequences with varying contents.", "result_label"], ["Moreover, motions in the generated sequence are realistic and able to transfer smoothly from the given start frame to the terminal frame.", "result_label"], ["Our project site is https://xharlie.github.io/projects/project_sites/SDVI/video_results.html", "result_label"]]]
[0, [["Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs).", "objective_label"], ["The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks.", "method_label"], ["We evaluate the method on four polyphonic musical data sets and motion capture data.", "result_label"]]]
[0, [["Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed.", "background_label"], ["Here we describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data.", "method_label"], ["We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g.", "result_label"], ["from noisy wireless channels) than traditional variable-length coding schemes.", "result_label"]]]
[0, [["Being able to predict what may happen in the future requires an in-depth understanding of the physical and causal rules that govern the world.", "background_label"], ["A model that is able to do so has a number of appealing applications, from robotic planning to representation learning.", "background_label"], ["However, learning to predict raw future observations, such as frames in a video, is exceedingly challenging -- the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single, blurry prediction.", "background_label"], ["Recently, this has been addressed by two distinct approaches: (a) latent variational variable models that explicitly model underlying stochasticity and (b) adversarially-trained models that aim to produce naturalistic images.", "method_label"], ["However, a standard latent variable model can struggle to produce realistic results, and a standard adversarially-trained model underutilizes latent variables and fails to produce diverse predictions.", "method_label"], ["We show that these distinct methods are in fact complementary.", "method_label"], ["Combining the two produces predictions that look more realistic to human raters and better cover the range of possible futures.", "method_label"], ["Our method outperforms prior and concurrent work in these aspects.", "result_label"]]]
[0, [["One key challenge to learning-based video compression is that motion predictive coding, a very effective tool for video compression, can hardly be trained into a neural network.", "background_label"], ["In this paper we propose the concept of PixelMotionCNN (PMCNN) which includes motion extension and hybrid prediction networks.", "objective_label"], ["PMCNN can model spatiotemporal coherence to effectively perform predictive coding inside the learning network.", "objective_label"], ["On the basis of PMCNN, we further explore a learning-based framework for video compression with additional components of iterative analysis/synthesis, binarization, etc.", "method_label"], ["Experimental results demonstrate the effectiveness of the proposed scheme.", "result_label"], ["Although entropy coding and complex configurations are not employed in this paper, we still demonstrate superior performance compared with MPEG-2 and achieve comparable results with H.264 codec.", "result_label"], ["The proposed learning-based scheme provides a possible new direction to further improve compression efficiency and functionalities of future video coding.", "result_label"]]]
[0, [["Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging.", "background_label"], ["Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world.", "background_label"], ["Many existing methods tackle this problem by making simplifying assumptions about the environment.", "background_label"], ["One common assumption is that the outcome is deterministic and there is only one plausible future.", "background_label"], ["This can lead to low-quality predictions in real-world settings with stochastic dynamics.", "background_label"], ["In this paper, we develop a stochastic variational video prediction (SV2P) method that predicts a different possible future for each sample of its latent variables.", "method_label"], ["To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video.", "method_label"], ["We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned.", "method_label"], ["We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods.", "result_label"], ["Our SV2P implementation will be open sourced upon publication.", "result_label"]]]
[0, [["Videos express highly structured spatio-temporal patterns of visual data.", "background_label"], ["A video can be thought of as being governed by two factors: (i) temporally invariant (e.g., person identity), or slowly varying (e.g., activity), attribute-induced appearance, encoding the persistent content of each frame, and (ii) an inter-frame motion or scene dynamics (e.g., encoding evolution of the person ex-ecuting the action).", "background_label"], ["Based on this intuition, we propose a generative framework for video generation and future prediction.", "objective_label"], ["The proposed framework generates a video (short clip) by decoding samples sequentially drawn from a latent space distribution into full video frames.", "method_label"], ["Variational Autoencoders (VAEs) are used as a means of encoding/decoding frames into/from the latent space and RNN as a wayto model the dynamics in the latent space.", "method_label"], ["We improve the video generation consistency through temporally-conditional sampling and quality by structuring the latent space with attribute controls; ensuring that attributes can be both inferred and conditioned on during learning/generation.", "method_label"], ["As a result, given attributes and/orthe first frame, our model is able to generate diverse but highly consistent sets ofvideo sequences, accounting for the inherent uncertainty in the prediction task.", "method_label"], ["Experimental results on Chair CAD, Weizmann Human Action, and MIT-Flickr datasets, along with detailed comparison to the state-of-the-art, verify effectiveness of the framework.", "result_label"]]]
[0, [["In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder.", "background_label"], ["We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech.", "method_label"], ["We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset.", "result_label"], ["Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.", "result_label"]]]
[0, [["We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g.", "background_label"], ["action classification) and video generation tasks (e.g.", "background_label"], ["future prediction).", "background_label"], ["We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background.", "method_label"], ["Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images.", "method_label"], ["Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning.", "result_label"], ["We believe generative video models can impact many applications in video understanding and simulation.", "result_label"]]]
[0, [["We describe an end-to-end trainable model for image compression based on variational autoencoders.", "background_label"], ["The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation.", "background_label"], ["This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs).", "background_label"], ["Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder.", "method_label"], ["We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate-distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR).", "result_label"], ["Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.", "result_label"]]]
[0, [["An ever increasing amount of our digital communication, media consumption, and content creation revolves around videos.", "background_label"], ["We share, watch, and archive many aspects of our lives through them, all of which are powered by strong video compression.", "background_label"], ["Traditional video compression is laboriously hand designed and hand optimized.", "background_label"], ["This paper presents an alternative in an end-to-end deep learning codec.", "objective_label"], ["Our codec builds on one simple idea: Video compression is repeated image interpolation.", "method_label"], ["It thus benefits from recent advances in deep image interpolation and generation.", "method_label"], ["Our deep video codec outperforms today's prevailing codecs, such as H.261, MPEG-4 Part 2, and performs on par with H.264.", "result_label"]]]
[0, [["Generating video frames that accurately predict future world states is challenging.", "background_label"], ["Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both.", "background_label"], ["In this paper we introduce an unsupervised video generation model that learns a prior model of uncertainty in a given environment.", "objective_label"], ["Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame.", "method_label"], ["The approach is simple and easily trained end-to-end on a variety of datasets.", "method_label"], ["Sample generations are both varied and sharp, even many frames into the future, and compare favorably to those from existing approaches.", "result_label"]]]
[0, [["The availability of affordable 3D full body reconstruction systems has given rise to free-viewpoint video (FVV) of human shapes.", "background_label"], ["Most existing solutions produce temporally uncorrelated point clouds or meshes with unknown point/vertex correspondences.", "background_label"], ["Individually compressing each frame is ineffective and still yields to ultra-large data sizes.", "background_label"], ["We present an end-to-end deep learning scheme to establish dense shape correspondences and subsequently compress the data.", "method_label"], ["Our approach uses sparse set of \"panoramic\"depth maps or PDMs, each emulating an inward-viewing concentric mosaics.", "method_label"], ["We then develop a learning-based technique to learn pixel-wise feature descriptors on PDMs.", "method_label"], ["The results are fed into an autoencoder-based network for compression.", "method_label"], ["Comprehensive experiments demonstrate our solution is robust and effective on both public and our newly captured datasets.", "result_label"]]]
[0, [["We propose a deep learning approach for finding dense correspondences between 3D scans of people.", "objective_label"], ["Our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the two people to be scanned from similar viewpoints, and runs in real time.", "method_label"], ["We use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body region classification problem, modified to increase the smoothness of the learned descriptors near region boundaries.", "method_label"], ["This approach ensures that nearby points on the human body are nearby in feature space, and vice versa, rendering the feature descriptor suitable for computing dense correspondences between the scans.", "method_label"], ["We validate our method on real and synthetic data for both clothed and unclothed humans, and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate than those found using methods that require full watertight 3D geometry.", "result_label"]]]
[0, [["Learning to rank is an important problem in machine learning and recommender systems.", "background_label"], ["In a recommender system, a user is typically recommended a list of items.", "background_label"], ["Since the user is unlikely to examine the entire recommended list, partial feedback arises naturally.", "background_label"], ["At the same time, diverse recommendations are important because it is challenging to model all tastes of the user in practice.", "background_label"], ["In this paper, we propose the first algorithm for online learning to rank diverse items from partial-click feedback.", "objective_label"], ["We assume that the user examines the list of recommended items until the user is attracted by an item, which is clicked, and does not examine the rest of the items.", "background_label"], ["This model of user behavior is known as the cascade model.", "background_label"], ["We propose an online learning algorithm, cascadelsb, for solving our problem.", "objective_label"], ["The algorithm actively explores the tastes of the user with the objective of learning to recommend the optimal diverse list.", "method_label"], ["We analyze the algorithm and prove a gap-free upper bound on its n-step regret.", "method_label"], ["We evaluate cascadelsb on both synthetic and real-world datasets, compare it to various baselines, and show that it learns even when our modeling assumptions do not hold exactly.", "result_label"]]]
[0, [["We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one.", "background_label"], ["The weights of the items are binary, stochastic, and drawn independently of each other.", "background_label"], ["The agent observes the index of the first chosen item whose weight is zero.", "background_label"], ["This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path.", "method_label"], ["We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its $n$-step regret.", "method_label"], ["Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability.", "method_label"], ["We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated.", "result_label"], ["We also demonstrate that our setting requires a new learning algorithm.", "result_label"]]]
[0, [["A search engine recommends to the user a list of web pages.", "background_label"], ["The user examines this list, from the first page to the last, and clicks on all attractive pages until the user is satisfied.", "background_label"], ["This behavior of the user can be described by the dependent click model (DCM).", "background_label"], ["We propose DCM bandits, an online learning variant of the DCM where the goal is to maximize the probability of recommending satisfactory items, such as web pages.", "objective_label"], ["The main challenge of our learning problem is that we do not observe which attractive item is satisfactory.", "method_label"], ["We propose a computationally-efficient learning algorithm for solving our problem, dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable assumptions; and also prove a matching lower bound up to logarithmic factors.", "method_label"], ["We evaluate our algorithm on synthetic and real-world problems, and show that it performs well even when our model is misspecified.", "result_label"], ["This work presents the first practical and regret-optimal online algorithm for learning to rank with multiple clicks in a cascade-like click model.", "result_label"]]]
[0, [["A search engine usually outputs a list of $K$ web pages.", "background_label"], ["The user examines this list, from the first web page to the last, and chooses the first attractive page.", "background_label"], ["This model of user behavior is known as the cascade model.", "background_label"], ["In this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify $K$ most attractive items.", "objective_label"], ["We formulate our problem as a stochastic combinatorial partial monitoring problem.", "method_label"], ["We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB.", "method_label"], ["We also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits.", "method_label"], ["The lower bound matches the upper bound of CascadeKL-UCB up to a logarithmic factor.", "method_label"], ["We experiment with our algorithms on several problems.", "method_label"], ["The algorithms perform surprisingly well even when our modeling assumptions are violated.", "result_label"]]]
[0, [["We predict credit applications with off-the-shelf, interchangeable black-box classifiers and we explain single predictions with counterfactual explanations.", "background_label"], ["Counterfactual explanations expose the minimal changes required on the input data to obtain a different result e.g., approved vs rejected application.", "background_label"], ["Despite their effectiveness, counterfactuals are mainly designed for changing an undesired outcome of a prediction i.e.", "background_label"], ["loan rejected.", "background_label"], ["Counterfactuals, however, can be difficult to interpret, especially when a high number of features are involved in the explanation.", "background_label"], ["Our contribution is two-fold: i) we propose positive counterfactuals, i.e.", "objective_label"], ["we adapt counterfactual explanations to also explain accepted loan applications, and ii) we propose two weighting strategies to generate more interpretable counterfactuals.", "method_label"], ["Experiments on the HELOC loan applications dataset show that our contribution outperforms the baseline counterfactual generation strategy, by leading to smaller and hence more interpretable counterfactuals.", "result_label"]]]
[0, [["In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user.", "background_label"], ["This lack of explanation constitutes both a practical and an ethical issue.", "background_label"], ["The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability.", "background_label"], ["The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation.", "method_label"], ["The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system.", "objective_label"], ["Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work.", "method_label"], ["The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.", "result_label"]]]
[0, [["Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications.", "background_label"], ["However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability.", "background_label"], ["In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another.", "background_label"], ["To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations).", "method_label"], ["SHAP assigns each feature an importance value for a particular prediction.", "method_label"], ["Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties.", "result_label"], ["The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties.", "result_label"], ["Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.", "result_label"]]]
[0, [["Most existing machine learning classifiers are highly vulnerable to adversarial examples.", "background_label"], ["An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it.", "background_label"], ["In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake.", "background_label"], ["Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model.", "background_label"], ["Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier.", "background_label"], ["This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input.", "background_label"], ["This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples.", "objective_label"], ["We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system.", "method_label"], ["We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.", "result_label"]]]
[0, [["Prototype methods seek a minimal subset of samples that can serve as a distillation or condensed view of a data set.", "background_label"], ["As the size of modern data sets grows, being able to present a domain specialist with a short list of \"representative\"samples chosen from the data set is of increasing interpretative value.", "background_label"], ["While much recent statistical research has been focused on producing sparse-in-the-variables methods, this paper aims at achieving sparsity in the samples.", "objective_label"], ["We discuss a method for selecting prototypes in the classification setting (in which the samples fall into known discrete categories).", "method_label"], ["Our method of focus is derived from three basic properties that we believe a good prototype set should satisfy.", "method_label"], ["This intuition is translated into a set cover optimization problem, which we solve approximately using standard approaches.", "method_label"], ["While prototype selection is usually viewed as purely a means toward building an efficient classifier, in this paper we emphasize the inherent value of having a set of prototypical elements.", "method_label"], ["That said, by using the nearest-neighbor rule on the set of prototypes, we can of course discuss our method as a classifier as well.", "result_label"]]]
[0, [["While learning visuomotor skills in an end-to-end manner is appealing, deep neural networks are often uninterpretable and fail in surprising ways.", "background_label"], ["For robotics tasks, such as autonomous driving, models that explicitly represent objects may be more robust to new scenes and provide intuitive visualizations.", "background_label"], ["We describe a taxonomy of \"object-centric\"models which leverage both object instances and end-to-end learning.", "method_label"], ["In the Grand Theft Auto V simulator, we show that object-centric models outperform object-agnostic methods in scenes with other vehicles and pedestrians, even with an imperfect detector.", "method_label"], ["We also demonstrate that our architectures perform well on real-world environments by evaluating on the Berkeley DeepDrive Video dataset, where an object-centric model outperforms object-agnostic models in the low-data regimes.", "result_label"]]]
[0, [["Most existing approaches to autonomous driving fall into one of two categories: modular pipelines, that build an extensive model of the environment, and imitation learning approaches, that map images directly to control outputs.", "background_label"], ["A recently proposed third paradigm, direct perception, aims to combine the advantages of both by using a neural network to learn appropriate low-dimensional intermediate representations.", "objective_label"], ["However, existing direct perception approaches are restricted to simple highway situations, lacking the ability to navigate intersections, stop at traffic lights or respect speed limits.", "background_label"], ["In this work, we propose a direct perception approach which maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs.", "method_label"], ["Compared to state-of-the-art reinforcement and conditional imitation learning approaches, we achieve an improvement of up to 68 % in goal-directed navigation on the challenging CARLA simulation benchmark.", "result_label"], ["In addition, our approach is the first to handle traffic lights and speed signs by using image-level labels only, as well as smooth car-following, resulting in a significant reduction of traffic accidents in simulation.", "result_label"]]]
[0, [["Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios.", "background_label"], ["In this paper, we presented a number of empirical evaluations of recent deep learning advances.", "background_label"], ["Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving.", "background_label"], ["To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios.", "background_label"], ["We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection.", "method_label"], ["We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system.", "method_label"], ["Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.", "result_label"]]]
[0, [["End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving.", "background_label"], ["Simulation can help end-to-end driving systems by providing a cheap, safe, and diverse training environment.", "background_label"], ["Yet training driving policies in simulation brings up the problem of transferring such policies to the real world.", "background_label"], ["We present an approach to transferring driving policies from simulation to reality via modularity and abstraction.", "objective_label"], ["Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches.", "method_label"], ["The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics.", "method_label"], ["We evaluate the presented approach in simulated urban environments and in the real world.", "result_label"], ["In particular, we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions, with no finetuning, on two continents.", "method_label"], ["The supplementary video can be viewed at https://youtu.be/BrMDJqI6H5U", "other_label"]]]
[0, [["We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis.", "objective_label"], ["The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours \\& sightseeing, the digital inspection of historical artifacts).", "objective_label"], ["A core component of our work is the handling of view-dependent effects.", "objective_label"], ["Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object.", "method_label"], ["As input data we are using an RGB video of the object.", "method_label"], ["This video is used to reconstruct a proxy geometry of the object via multi-view stereo.", "method_label"], ["Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering.", "result_label"], ["This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts.", "background_label"], ["To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects.", "objective_label"], ["Based on these estimations, we are able to convert observed images to diffuse images.", "method_label"], ["These diffuse images can be projected into other views.", "method_label"], ["In the target view, our pipeline reinserts the new view-dependent effects.", "method_label"], ["To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results.", "method_label"], ["Using this image-guided approach, the network does not have to allocate capacity on ``remembering'' object appearance, instead it learns how to combine the appearance of captured images.", "method_label"], ["We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.", "result_label"]]]
[0, [["We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video.", "background_label"], ["In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor.", "method_label"], ["The core of our approach is a generative neural network with a novel space-time architecture.", "method_label"], ["The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor.", "method_label"], ["The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input.", "method_label"], ["In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network -- thus taking full control of the target.", "method_label"], ["With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background.", "method_label"], ["For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing.", "result_label"], ["To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect.", "result_label"]]]
[0, [["This paper presents a simple method for \"do as I do\"motion transfer: given a source video of a person dancing, we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves.", "background_label"], ["We approach this problem as video-to-video translation using pose as an intermediate representation.", "method_label"], ["To transfer the motion, we extract poses from the source subject and apply the learned pose-to-appearance mapping to generate the target subject.", "method_label"], ["We predict two consecutive frames for temporally coherent video results and introduce a separate pipeline for realistic face synthesis.", "method_label"], ["Although our method is quite simple, it produces surprisingly compelling results (see video).", "method_label"], ["This motivates us to also provide a forensics tool for reliable synthetic content detection, which is able to distinguish videos synthesized by our system from real data.", "result_label"], ["In addition, we release a first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer.", "result_label"]]]
[0, [["We study how to synthesize novel views of human body from a single image.", "background_label"], ["Though recent deep learning based methods work well for rigid objects, they often fail on objects with large articulation, like human bodies.", "background_label"], ["The core step of existing methods is to fit a map from the observable views to novel views by CNNs; however, the rich articulation modes of human body make it rather challenging for CNNs to memorize and interpolate the data well.", "background_label"], ["To address the problem, we propose a novel deep learning based pipeline that explicitly estimates and leverages the geometry of the underlying human body.", "method_label"], ["Our new pipeline is a composition of a shape estimation network and an image generation network, and at the interface a perspective transformation is applied to generate a forward flow for pixel value transportation.", "method_label"], ["Our design is able to factor out the space of data variation and makes learning at each step much easier.", "method_label"], ["Empirically, we show that the performance for pose-varying objects can be improved dramatically.", "method_label"], ["Our method can also be applied on real data captured by 3D sensors, and the flow generated by our methods can be used for generating high quality results in higher resolution.", "result_label"]]]
[0, [["We address the problem of novel view synthesis: given an input image, synthesizing new images of the same object or scene observed from arbitrary viewpoints.", "background_label"], ["We approach this as a learning task but, critically, instead of learning to synthesize pixels from scratch, we learn to copy them from the input image.", "background_label"], ["Our approach exploits the observation that the visual appearance of different views of the same instance is highly correlated, and such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows -- 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view.", "method_label"], ["Furthermore, the proposed framework easily generalizes to multiple input views by learning how to optimally combine single-view predictions.", "method_label"], ["We show that for both objects and scenes, our approach is able to synthesize novel views of higher perceptual quality than previous CNN-based techniques.", "result_label"]]]
[0, [["We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility.", "background_label"], ["A stochastic routing framework as used by recent deep neural decision/regression forests is incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique.", "method_label"], ["We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse feature extraction process as in traditional decision tree with pixel-difference feature.", "method_label"], ["To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by using local binary convolution layer.", "method_label"], ["DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures.", "result_label"]]]
[0, [["Phenomenally successful in practical inference problems, convolutional neural networks (CNN) are widely deployed in mobile devices, data centers, and even supercomputers.", "background_label"], ["The number of parameters needed in CNNs, however, are often large and undesirable.", "background_label"], ["Consequently, various methods have been developed to prune a CNN once it is trained.", "background_label"], ["Nevertheless, the resulting CNNs offer limited benefits.", "background_label"], ["While pruning the fully connected layers reduces a CNN's size considerably, it does not improve inference speed noticeably as the compute heavy parts lie in convolutions.", "result_label"], ["Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures, thus limiting the achievable sparsity levels.", "background_label"], ["We present a method to realize simultaneously size economy and speed improvement while pruning CNNs.", "objective_label"], ["Paramount to our success is an efficient general sparse-with-dense matrix multiplication implementation that is applicable to convolution of feature maps with kernels of arbitrary sparsity patterns.", "method_label"], ["Complementing this, we developed a performance model that predicts sweet spots of sparsity levels for different layers and on different computer architectures.", "method_label"], ["Together, these two allow us to demonstrate 3.1--7.3$\\times$ convolution speedups over dense convolution in AlexNet, on Intel Atom, Xeon, and Xeon Phi processors, spanning the spectrum from mobile devices to supercomputers.", "method_label"], ["We also open source our project at https://github.com/IntelLabs/SkimCaffe.", "other_label"]]]
[0, [["We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions.", "objective_label"], ["First, a new image feature called Normalized Pixel Difference (NPD) is proposed.", "method_label"], ["NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology.", "method_label"], ["The new feature is scale invariant, bounded, and is able to reconstruct the original image.", "method_label"], ["Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules.", "method_label"], ["This way, only a single soft-cascade classifier is needed to handle unconstrained face detection.", "method_label"], ["Furthermore, we show that the NPD features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast.", "method_label"], ["Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes.", "result_label"]]]
[0, [["Age estimation from facial images is typically cast as a nonlinear regression problem.", "background_label"], ["The main challenge of this problem is the facial feature space w.r.t.", "background_label"], ["ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging patterns.", "background_label"], ["In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation.", "objective_label"], ["DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes.", "method_label"], ["This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free and fast-converging update rule derived from Variational Bounding.", "method_label"], ["We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.", "result_label"]]]
[0, [["We propose local binary convolution (LBC), an efficient alternative to convolutional layers in standard convolutional neural networks (CNN).", "objective_label"], ["The design principles of LBC are motivated by local binary patterns (LBP).", "background_label"], ["The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights.", "method_label"], ["The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer.", "method_label"], ["The LBC layer affords significant parameter savings, 9x to 169x in the number of learnable parameters compared to a standard convolutional layer.", "method_label"], ["Furthermore, the sparse and binary nature of the weights also results in up to 9x to 169x savings in model size compared to a standard convolutional layer.", "method_label"], ["We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer.", "result_label"], ["Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), achieves performance parity with regular CNNs on a range of visual datasets (MNIST, SVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings.", "result_label"]]]
[0, [["Recently, 3D input data based hand pose estimation methods have shown state-of-the-art performance, because 3D data capture more spatial information than the depth image.", "background_label"], ["Whereas 3D voxel-based methods need a large amount of memory, PointNet based methods need tedious preprocessing steps such as K-nearest neighbour search for each point.", "background_label"], ["In this paper, we present a novel deep learning hand pose estimation method for an unordered point cloud.", "objective_label"], ["Our method takes 1024 3D points as input and does not require additional information.", "method_label"], ["We use Permutation Equivariant Layer (PEL) as the basic element, where a residual network version of PEL is proposed for the hand pose estimation task.", "method_label"], ["Furthermore, we propose a voting based scheme to merge information from individual points to the final pose output.", "method_label"], ["In addition to the pose estimation task, the voting-based scheme can also provide point cloud segmentation result without ground-truth for segmentation.", "method_label"], ["We evaluate our method on both NYU dataset and the Hands2017Challenge dataset.", "result_label"], ["Our method outperforms recent state-of-the-art methods, where our pose accuracy is currently the best for the Hands2017Challenge dataset.", "result_label"]]]
[0, [["We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image.", "objective_label"], ["We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop.", "method_label"], ["The components of this feedback loop are also Deep Networks, optimized using training data.", "method_label"], ["They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm.", "method_label"], ["We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU.", "result_label"]]]
[0, [["Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices.", "background_label"], ["While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world.", "background_label"], ["Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds.", "background_label"], ["To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation.", "objective_label"], ["EdgeConv acts on graphs dynamically computed in each layer of the network.", "method_label"], ["It is differentiable and can be plugged into existing architectures.", "method_label"], ["Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding.", "method_label"], ["We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.", "result_label"]]]
[0, [["3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation.", "background_label"], ["With the recent availability of inexpensive 2.5D depth sensors (e.g.", "background_label"], ["Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop.", "background_label"], ["Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding.", "background_label"], ["To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network.", "method_label"], ["Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically.", "method_label"], ["It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning.", "method_label"], ["To train our 3D deep learning model, we construct ModelNet -- a large-scale 3D CAD model dataset.", "method_label"], ["Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.", "result_label"]]]
[0, [["We introduce and evaluate several architectures for Convolutional Neural Networks to predict the 3D joint locations of a hand given a depth map.", "background_label"], ["We first show that a prior on the 3D pose can be easily introduced and significantly improves the accuracy and reliability of the predictions.", "method_label"], ["We also show how to use context efficiently to deal with ambiguities between fingers.", "method_label"], ["These two contributions allow us to significantly outperform the state-of-the-art on several challenging benchmarks, both in terms of accuracy and computation times.", "result_label"]]]
[0, [["We propose a novel 3D neural network architecture for 3D hand pose estimation from a single depth image.", "objective_label"], ["Different from previous works that mostly run on 2D depth image domain and require intermediate or post process to bring in the supervision from 3D space, we convert the depth map to a 3D volumetric representation, and feed it into a 3D convolutional neural network(CNN) to directly produce the pose in 3D requiring no further process.", "method_label"], ["Our system does not require the ground truth reference point for initialization, and our network architecture naturally integrates both local feature and global context in 3D space.", "method_label"], ["To increase the coverage of the hand pose space of the training data, we render synthetic depth image by transferring hand pose from existing real image datasets.", "method_label"], ["We evaluation our algorithm on two public benchmarks and achieve the state-of-the-art performance.", "result_label"], ["The synthetic hand pose dataset will be available.", "result_label"]]]
[0, [["Deep learning with 3D data such as reconstructed point clouds and CAD models has received great research interests recently.", "background_label"], ["However, the capability of using point clouds with convolutional neural network has been so far not fully explored.", "background_label"], ["In this paper, we present a convolutional neural network for semantic segmentation and object recognition with 3D point clouds.", "objective_label"], ["At the core of our network is pointwise convolution, a new convolution operator that can be applied at each point of a point cloud.", "method_label"], ["Our fully convolutional network design, while being surprisingly simple to implement, can yield competitive accuracy in both semantic segmentation and object recognition task.", "result_label"]]]
[0, [["A dominant paradigm for learning-based approaches in computer vision is training generic models, such as ResNet for image recognition, or I3D for video understanding, on large datasets and allowing them to discover the optimal representation for the problem at hand.", "background_label"], ["While this is an obviously attractive approach, it is not applicable in all scenarios.", "background_label"], ["We claim that action detection is one such challenging problem - the models that need to be trained are large, and labeled data is expensive to obtain.", "background_label"], ["To address this limitation, we propose to incorporate domain knowledge into the structure of the model, simplifying optimization.", "method_label"], ["In particular, we augment a standard I3D network with a tracking module to aggregate long term motion patterns, and use a graph convolutional network to reason about interactions between actors and objects.", "method_label"], ["Evaluated on the challenging AVA dataset, the proposed approach improves over the I3D baseline by 5.5% mAP and over the state-of-the-art by 4.8% mAP.", "result_label"]]]
[0, [["Recent approaches for high accuracy detection and tracking of object categories in video consist of complex multistage solutions that become more cumbersome each year.", "background_label"], ["In this paper we propose a ConvNet architecture that jointly performs detection and tracking, solving the task in a simple and effective way.", "objective_label"], ["Our contributions are threefold: (i) we set up a ConvNet architecture for simultaneous detection and tracking, using a multi-task objective for frame-based object detection and across-frame track regression; (ii) we introduce correlation features that represent object co-occurrences across time to aid the ConvNet during tracking; and (iii) we link the frame level detections based on our across-frame tracklets to produce high accuracy detections at the video level.", "method_label"], ["Our ConvNet architecture for spatiotemporal object detection is evaluated on the large-scale ImageNet VID dataset where it achieves state-of-the-art results.", "result_label"], ["Our approach provides better single model performance than the winning method of the last ImageNet challenge while being conceptually much simpler.", "result_label"], ["Finally, we show that by increasing the temporal stride we can dramatically increase the tracker speed.", "result_label"]]]
[0, [["We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding.", "objective_label"], ["This is achieved by gathering images of complex everyday scenes containing common objects in their natural context.", "objective_label"], ["Objects are labeled using per-instance segmentations to aid in precise object localization.", "method_label"], ["Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old.", "method_label"], ["With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation.", "method_label"], ["We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN.", "method_label"], ["Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "result_label"]]]
[0, [["Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn.", "background_label"], ["In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning.", "objective_label"], ["We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems.", "method_label"], ["Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs.", "method_label"], ["Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.", "result_label"]]]
[0, [["The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks.", "background_label"], ["This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset.", "objective_label"], ["Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos.", "method_label"], ["We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.", "method_label"], ["We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters.", "method_label"], ["We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101.", "result_label"]]]
[0, [["We present a conceptually simple, flexible, and general framework for object instance segmentation.", "background_label"], ["Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.", "method_label"], ["The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition.", "method_label"], ["Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps.", "method_label"], ["Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework.", "method_label"], ["We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection.", "result_label"], ["Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners.", "result_label"], ["We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition.", "result_label"], ["Code has been made available at: https://github.com/facebookresearch/Detectron", "other_label"]]]
[0, [["In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos.", "background_label"], ["The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function.", "method_label"], ["The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking.", "method_label"], ["Once learned, the matching function is used as is, without any adapting, to track previously unseen targets.", "method_label"], ["It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance.", "method_label"], ["Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.", "result_label"]]]
[0, [["Human activity recognition is typically addressed by detecting key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context.", "background_label"], ["The next open challenges in activity recognition require a level of understanding that pushes beyond this and call for models with capabilities for fine distinction and detailed comprehension of interactions between actors and objects in a scene.", "background_label"], ["We propose a model capable of learning to reason about semantically meaningful spatiotemporal interactions in videos.", "objective_label"], ["The key to our approach is a choice of performing this reasoning at the object level through the integration of state of the art object detection networks.", "method_label"], ["This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level.", "method_label"], ["We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC Kitchens) and achieve state of the art results on all of them.", "result_label"], ["Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.", "result_label"]]]
[0, [["We present a method for detecting objects in images using a single deep neural network.", "background_label"], ["Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location.", "method_label"], ["At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape.", "method_label"], ["Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.", "method_label"], ["Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network.", "method_label"], ["This makes SSD easy to train and straightforward to integrate into systems that require a detection component.", "method_label"], ["Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference.", "result_label"], ["Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size.", "result_label"], ["For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model.", "result_label"], ["Code is available at https://github.com/weiliu89/caffe/tree/ssd .", "other_label"]]]
[0, [["We consider the problem of estimating human pose and trajectory by an aerial robot with a monocular camera in near real time.", "background_label"], ["We present a preliminary solution whose distinguishing feature is a dynamic classifier selection architecture.", "method_label"], ["In our solution, each video frame is corrected for perspective using projective transformation.", "method_label"], ["Then, two alternative feature sets are used: (i) Histogram of Oriented Gradients (HOG) of the silhouette, (ii) Convolutional Neural Network (CNN) features of the RGB image.", "method_label"], ["The features (HOG or CNN) are classified using a dynamic classifier.", "method_label"], ["A class is defined as a pose-viewpoint pair, and a total of 64 classes are defined to represent a forward walking and turning gait sequence.", "method_label"], ["Our solution provides three main advantages: (i) Classification is efficient due to dynamic selection (4-class vs. 64-class classification).", "background_label"], ["(ii) Classification errors are confined to neighbors of the true view-points.", "background_label"], ["(iii) The robust temporal relationship between poses is used to resolve the left-right ambiguities of human silhouettes.", "method_label"], ["Experiments conducted on both fronto-parallel videos and aerial videos confirm our solution can achieve accurate pose and trajectory estimation for both scenarios.", "result_label"], ["We found using HOG features provides higher accuracy than using CNN features.", "method_label"], ["For example, applying the HOG-based variant of our scheme to the 'walking on a figure 8-shaped path' dataset (1652 frames) achieved estimation accuracies of 99.6% for viewpoints and 96.2% for number of poses.", "result_label"]]]
[0, [["This work introduces a novel convolutional network architecture for the task of human pose estimation.", "background_label"], ["Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body.", "background_label"], ["We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network.", "method_label"], ["We refer to the architecture as a \"stacked hourglass\"network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions.", "method_label"], ["State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.", "result_label"]]]
[0, [["Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs.", "background_label"], ["Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks.", "background_label"], ["Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively.", "background_label"], ["In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results.", "method_label"], ["Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected.", "method_label"], ["In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network.", "method_label"], ["We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features.", "result_label"], ["A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", "result_label"]]]
[0, [["Recognizing human actions from unknown and unseen (novel) views is a challenging problem.", "background_label"], ["We propose a Robust Non-Linear Knowledge Transfer Model (R-NKTM) for human action recognition from novel views.", "objective_label"], ["The proposed R-NKTM is a deep fully-connected neural network that transfers knowledge of human actions from any unknown view to a shared high-level virtual view by finding a non-linear virtual path that connects the views.", "method_label"], ["The R-NKTM is learned from dense trajectories of synthetic 3D human models fitted to real motion capture data and generalizes to real videos of human actions.", "method_label"], ["The strength of our technique is that we learn a single R-NKTM for all actions and all viewpoints for knowledge transfer of any real human action video without the need for re-training or fine-tuning the model.", "method_label"], ["Thus, R-NKTM can efficiently scale to incorporate new action classes.", "method_label"], ["R-NKTM is learned with dummy labels and does not require knowledge of the camera viewpoint at any stage.", "method_label"], ["Experiments on three benchmark cross-view human action datasets show that our method outperforms existing state-of-the-art.", "result_label"]]]
[0, [["Pose Machines provide a sequential prediction framework for learning rich implicit spatial models.", "background_label"], ["In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.", "background_label"], ["The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation.", "objective_label"], ["We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.", "method_label"], ["Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure.", "method_label"], ["We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.", "result_label"]]]
[0, [["We study multiple rule-based and machine learning (ML) models for sepsis detection.", "background_label"], ["We report the first neural network detection and prediction results on three categories of sepsis.", "background_label"], ["We have used the retrospective Medical Information Mart for Intensive Care (MIMIC)-III dataset, restricted to intensive care unit (ICU) patients.", "method_label"], ["Features for prediction were created from only common vital sign measurements.", "method_label"], ["We show significant improvement of AUC score using neural network based ensemble model compared to single ML and rule-based models.", "method_label"], ["For the detection of sepsis, severe sepsis, and septic shock, our model achieves an AUC of 0.97, 0.96 and 0.91, respectively.", "method_label"], ["Four hours before the positive hours, it predicts the same three categories with an AUC of 0.90, 0.91 and 0.90 respectively.", "result_label"], ["Further, we ranked the features and found that using six vital signs consistently provides higher detection and prediction AUC for all the models tested.", "result_label"], ["Our novel ensemble model achieves highest AUC in detecting and predicting sepsis, severe sepsis, and septic shock in the MIMIC-III ICU patients, and is amenable to deployment in hospital settings.", "result_label"]]]
[0, [["Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications.", "background_label"], ["However, few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems on publicly available healthcare datasets.", "background_label"], ["In this paper, we present the benchmarking results for several clinical prediction tasks such as mortality prediction, length of stay prediction, and ICD-9 code group prediction using Deep Learning models, ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA scores.", "method_label"], ["We used the Medical Information Mart for Intensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes all patients admitted to an ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the benchmarking tasks.", "method_label"], ["Our results show that deep learning models consistently outperform all the other approaches especially when the `raw' clinical time series data is used as input features to the models.", "result_label"]]]
[0, [["Health care is one of the most exciting frontiers in data mining and machine learning.", "background_label"], ["Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets.", "background_label"], ["To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database.", "method_label"], ["These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification.", "method_label"], ["We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.", "result_label"]]]
[0, [["We present a method for improving segmentation tasks on images affected by adherent rain drops and streaks.", "background_label"], ["We introduce a novel stereo dataset recorded using a system that allows one lens to be affected by real water droplets while keeping the other lens clear.", "method_label"], ["We train a denoising generator using this dataset and show that it is effective at removing the effect of real water droplets, in the context of image reconstruction and road marking segmentation.", "method_label"], ["To further test our de-noising approach, we describe a method of adding computer-generated adherent water droplets and streaks to any images, and use this technique as a proxy to demonstrate the effectiveness of our model in the context of general semantic segmentation.", "method_label"], ["We benchmark our results using the CamVid road marking segmentation dataset, Cityscapes semantic segmentation datasets and our own real-rain dataset, and show significant improvement on all tasks.", "result_label"]]]
[0, [["This paper introduces depth estimation from water drops.", "background_label"], ["The key idea is that a single water drop adhered to window glass is totally transparent and convex, and thus optically acts like a fisheye lens.", "background_label"], ["If we have more than one water drop in a single image, then through each of them we can see the environment with different view points, similar to stereo.", "background_label"], ["To realize this idea, we need to rectify every water drop imagery to make radially distorted planar surfaces look flat.", "method_label"], ["For this rectification, we consider two physical properties of water drops: (1) A static water drop has constant volume, and its geometric convex shape is determined by the balance between the tension force and gravity.", "method_label"], ["This implies that the 3D geometric shape can be obtained by minimizing the overall potential energy, which is the sum of the tension energy and the gravitational potential energy.", "result_label"], ["(2) The imagery inside a water-drop is determined by the water-drop 3D shape and total reflection at the boundary.", "background_label"], ["This total reflection generates a dark band commonly observed in any adherent water drops.", "background_label"], ["Hence, once the 3D shape of water drops are recovered, we can rectify the water drop images through backward raytracing.", "method_label"], ["Subsequently, we can compute depth using stereo.", "method_label"], ["In addition to depth estimation, we can also apply image refocusing.", "method_label"], ["Experiments on real images and a quantitative evaluation show the effectiveness of our proposed method.", "result_label"], ["To our best knowledge, never before have adherent water drops been used to estimate depth.", "result_label"]]]
[0, [["Most computer vision systems and computational photography systems are visible light based which is a small fraction of the electromagnetic (EM) spectrum.", "background_label"], ["In recent years radio frequency (RF) hardware has become more widely available, for example, many cars are equipped with a RADAR, and almost every home has a WiFi device.", "background_label"], ["In the context of imaging, RF spectrum holds many advantages compared to visible light systems.", "background_label"], ["In particular, in this regime, EM energy effectively interacts in different ways with matter.", "background_label"], ["This property allows for many novel applications such as privacy preserving computer vision and imaging through absorbing and scattering materials in visible light such as walls.", "background_label"], ["Here, we expand many of the concepts in computational photography in visible light to RF cameras.", "method_label"], ["The main limitation of imaging with RF is the large wavelength that limits the imaging resolution when compared to visible light.", "method_label"], ["However, the output of RF cameras is usually processed by computer vision and perception algorithms which would benefit from multi-modal sensing of the environment, and from sensing in situations in which visible light systems fail.", "method_label"], ["To bridge the gap between computational photography and RF imaging, we expand the concept of light-field to RF.", "method_label"], ["This work paves the way to novel computational sensing systems with RF.", "result_label"]]]
[0, [["Lensless imaging is an important and challenging problem.", "background_label"], ["One notable solution to lensless imaging is a single pixel camera which benefits from ideas central to compressive sampling.", "background_label"], ["However, traditional single pixel cameras require many illumination patterns which result in a long acquisition process.", "background_label"], ["Here we present a method for lensless imaging based on compressive ultrafast sensing.", "method_label"], ["Each sensor acquisition is encoded with a different illumination pattern and produces a time series where time is a function of the photon's origin in the scene.", "method_label"], ["Currently available hardware with picosecond time resolution enables time tagging photons as they arrive to an omnidirectional sensor.", "background_label"], ["This allows lensless imaging with significantly fewer patterns compared to regular single pixel imaging.", "background_label"], ["To that end, we develop a framework for designing lensless imaging systems that use ultrafast detectors.", "objective_label"], ["We provide an algorithm for ideal sensor placement and an algorithm for optimized active illumination patterns.", "method_label"], ["We show that efficient lensless imaging is possible with ultrafast measurement and compressive sensing.", "method_label"], ["This paves the way for novel imaging architectures and remote sensing in extreme situations where imaging with a lens is not possible.", "result_label"]]]
[0, [["Today, experiencing virtual reality (VR) is a cumbersome experience which either requires dedicated infrastructure like infrared cameras to track the headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides only 3-DoF (Degrees of Freedom) tracking which severely limits the user experience (e.g., Samsung Gear).", "background_label"], ["To truly enable VR everywhere, we need position tracking to be available as a ubiquitous service.", "background_label"], ["This paper presents WiCapture, a novel approach which leverages commodity WiFi infrastructure, which is ubiquitous today, for tracking purposes.", "objective_label"], ["We prototype WiCapture using off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm compared to sophisticated infrared based tracking systems like the Oculus, while providing much higher range, resistance to occlusion, ubiquity and ease of deployment.", "result_label"]]]
[0, [["Graph-based methods have been demonstrated as one of the most effective approaches for semi-supervised learning, as they can exploit the connectivity patterns between labeled and unlabeled data samples to improve learning performance.", "background_label"], ["However, existing graph-based methods either are limited in their ability to jointly model graph structures and data features, such as the classical label propagation methods, or require a considerable amount of labeled data for training and validation due to high model complexity, such as the recent neural-network-based methods.", "background_label"], ["In this paper, we address label efficient semi-supervised learning from a graph filtering perspective.", "objective_label"], ["Specifically, we propose a graph filtering framework that injects graph similarity into data features by taking them as signals on the graph and applying a low-pass graph filter to extract useful data representations for classification, where label efficiency can be achieved by conveniently adjusting the strength of the graph filter.", "method_label"], ["Interestingly, this framework unifies two seemingly very different methods -- label propagation and graph convolutional networks.", "method_label"], ["Revisiting them under the graph filtering framework leads to new insights that improve their modeling capabilities and reduce model complexity.", "method_label"], ["Experiments on various semi-supervised classification tasks on four citation networks and one knowledge graph and one semi-supervised regression task for zero-shot image recognition validate our findings and proposals.", "result_label"]]]
[0, [["Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing.", "background_label"], ["However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs.", "background_label"], ["Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques.", "background_label"], ["In this survey, we comprehensively review the different types of deep learning methods on graphs.", "method_label"], ["We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods.", "method_label"], ["We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history.", "method_label"], ["We also analyze the differences and compositions of different methods.", "method_label"], ["Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.", "result_label"]]]
[0, [["Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions.", "background_label"], ["However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes.", "background_label"], ["Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data.", "method_label"], ["Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood.", "method_label"], ["Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.", "result_label"]]]
[0, [["We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.", "background_label"], ["By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.", "background_label"], ["In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.", "method_label"], ["Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).", "result_label"]]]
[0, [["Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain.", "background_label"], ["In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group.", "background_label"], ["In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian.", "method_label"], ["We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.", "result_label"]]]
[0, [["Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision.", "background_label"], ["In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition.", "background_label"], ["Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos.", "background_label"], ["Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics.", "background_label"], ["In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features.", "objective_label"], ["We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework.", "method_label"], ["We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.", "result_label"]]]
[0, [["We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner.", "background_label"], ["DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures.", "background_label"], ["The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks.", "method_label"], ["In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups.", "method_label"], ["We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.", "result_label"]]]
[0, [["We introduce a convolutional neural network that operates directly on graphs.", "background_label"], ["These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.", "background_label"], ["The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.", "method_label"], ["We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.", "result_label"]]]
[0, [["We present DeepWalk, a novel approach for learning latent representations of vertices in a network.", "background_label"], ["These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models.", "background_label"], ["DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.", "method_label"], ["DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences.", "method_label"], ["We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube.", "result_label"], ["Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information.", "background_label"], ["DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse.", "background_label"], ["In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.", "method_label"], ["DeepWalk is also scalable.", "method_label"], ["It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable.", "method_label"], ["These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.", "result_label"]]]
[0, [["Lots of learning tasks require dealing with graph data which contains rich relation information among elements.", "background_label"], ["Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs.", "background_label"], ["In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models.", "background_label"], ["Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs.", "background_label"], ["In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks.", "method_label"], ["In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.", "result_label"]]]
[0, [["As cellular networks become denser, a scalable and dynamic tuning of wireless base station parameters can only be achieved through automated optimization.", "background_label"], ["Although the contextual bandit framework arises as a natural candidate for such a task, its extension to a parallel setting is not straightforward: one needs to carefully adapt existing methods to fully leverage the multi-agent structure of this problem.", "background_label"], ["We propose two approaches: one derived from a deterministic UCB-like method and the other relying on Thompson sampling.", "method_label"], ["Thanks to its bayesian nature, the latter is intuited to better preserve the exploration-exploitation balance in the bandit batch.", "method_label"], ["This is verified on toy experiments, where Thompson sampling shows robustness to the variability of the contexts.", "method_label"], ["Finally, we apply both methods on a real base station network dataset and evidence that Thompson sampling outperforms both manual tuning and contextual UCB.", "result_label"]]]
[0, [["Most models in machine learning contain at least one hyperparameter to control for model complexity.", "background_label"], ["Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging.", "background_label"], ["In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information.", "objective_label"], ["An advantage of this method is that hyperparameters can be updated before model parameters have fully converged.", "method_label"], ["We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors.", "method_label"], ["Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression.", "result_label"], ["Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.", "result_label"]]]
[0, [["In this paper we initiate the study of optimization of bandit type problems in scenarios where the feedback of a play is not immediately known.", "background_label"], ["This arises naturally in allocation problems which have been studied extensively in the literature, albeit in the absence of delays in the feedback.", "background_label"], ["We study this problem in the Bayesian setting.", "method_label"], ["In presence of delays, no solution with provable guarantees is known to exist with sub-exponential running time.", "method_label"], ["We show that bandit problems with delayed feedback that arise in allocation settings can be forced to have significant structure, with a slight loss in optimality.", "method_label"], ["This structure gives us the ability to reason about the relationship of single arm policies to the entangled optimum policy, and eventually leads to a O(1) approximation for a significantly general class of priors.", "method_label"], ["The structural insights we develop are of key interest and carry over to the setting where the feedback of an action is available instantaneously, and we improve all previous results in this setting as well.", "result_label"]]]
[0, [["The popularity of Bayesian optimization methods for efficient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions.", "background_label"], ["However, most proposed approaches only allow the exploration of the parameter space to occur sequentially.", "background_label"], ["Often, it is desirable to simultaneously propose batches of parameter values to explore.", "background_label"], ["This is particularly the case when large parallel processing facilities are available.", "background_label"], ["These facilities could be computational or physical facets of the process being optimized.", "background_label"], ["E.g.", "background_label"], ["in biological experiments many experimental set ups allow several samples to be simultaneously processed.", "result_label"], ["Batch methods, however, require modeling of the interaction between the evaluations in the batch, which can be expensive in complex scenarios.", "background_label"], ["We investigate a simple heuristic based on an estimate of the Lipschitz constant that captures the most important aspect of this interaction (i.e.", "background_label"], ["local repulsion) at negligible computational overhead.", "method_label"], ["The resulting algorithm compares well, in running time, with much more elaborate alternatives.", "method_label"], ["The approach assumes that the function of interest, $f$, is a Lipschitz continuous function.", "method_label"], ["A wrap-loop around the acquisition function is used to collect batches of points of certain size minimizing the non-parallelizable computational effort.", "method_label"], ["The speed-up of our method with respect to previous approaches is significant in a set of computationally expensive experiments.", "result_label"]]]
[0, [["Online advertising and product recommendation are important domains of applications for multi-armed bandit methods.", "background_label"], ["In these fields, the reward that is immediately available is most often only a proxy for the actual outcome of interest, which we refer to as a conversion.", "background_label"], ["For instance, in web advertising, clicks can be observed within a few seconds after an ad display but the corresponding sale --if any-- will take hours, if not days to happen.", "background_label"], ["This paper proposes and investigates a new stochas-tic multi-armed bandit model in the framework proposed by Chapelle (2014) --based on empirical studies in the field of web advertising-- in which each action may trigger a future reward that will then happen with a stochas-tic delay.", "method_label"], ["We assume that the probability of conversion associated with each action is unknown while the distribution of the conversion delay is known, distinguishing between the (idealized) case where the conversion events may be observed whatever their delay and the more realistic setting in which late conversions are censored.", "method_label"], ["We provide performance lower bounds as well as two simple but efficient algorithms based on the UCB and KLUCB frameworks.", "method_label"], ["The latter algorithm, which is preferable when conversion rates are low, is based on a Poissonization argument, of independent interest in other settings where aggregation of Bernoulli observations with different success probabilities is required.", "result_label"]]]
[0, [["In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g.", "background_label"], ["when evaluating the performances of several different neural network architectures in a parallel computing environment.", "background_label"], ["In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method.", "method_label"], ["By construction, this method provides the one-step Bayes-optimal batch of points to sample.", "method_label"], ["We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.", "result_label"]]]
[0, [["Motivated by practical applications, chiefly clinical trials, we study the regret achievable for stochastic bandits under the constraint that the employed policy must split trials into a small number of batches.", "background_label"], ["We propose a simple policy, and show that a very small number of batches gives close to minimax optimal regret bounds.", "method_label"], ["As a byproduct, we derive optimal policies with low switching cost for stochastic bandits.", "result_label"]]]
[0, [["Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems.", "background_label"], ["In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms.", "background_label"], ["Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems.", "method_label"], ["We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop.", "method_label"], ["Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.", "result_label"]]]
[0, [["In the domain of algorithmic music composition, machine learning-driven systems eliminate the need for carefully hand-crafting rules for composition.", "background_label"], ["In particular, the capability of recurrent neural networks to learn complex temporal patterns lends itself well to the musical domain.", "background_label"], ["Promising results have been observed across a number of recent attempts at music composition using deep RNNs.", "background_label"], ["These approaches generally aim at first training neural networks to reproduce subsequences drawn from existing songs.", "objective_label"], ["Subsequently, they are used to compose music either at the audio sample-level or at the note-level.", "method_label"], ["We designed a representation that divides polyphonic music into a small number of monophonic streams.", "method_label"], ["This representation greatly reduces the complexity of the problem and eliminates an exponential number of probably poor compositions.", "method_label"], ["On top of our LSTM neural network that learnt musical sequences in this representation, we built an RL agent that learnt to find combinations of songs whose joint dominance produced pleasant compositions.", "method_label"], ["We present Amadeus, an algorithmic music composition system that composes music that consists of intricate melodies, basic chords, and even occasional contrapuntal sequences.", "result_label"]]]
[0, [["Algorithmic composition is the partial or total automation of the process of music composition by using computers.", "background_label"], ["Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms.", "background_label"], ["This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.", "objective_label"]]]
[0, [["This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces.", "background_label"], ["We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach.", "background_label"], ["DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data.", "background_label"], ["This is in contrast with many automatic music composition approaches which tend to compose music sequentially.", "method_label"], ["Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score.", "method_label"], ["We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.", "result_label"]]]
[0, [["A big challenge in algorithmic composition is to devise a model that is both easily trainable and able to reproduce the long-range temporal dependencies typical of music.", "background_label"], ["Here we investigate how artificial neural networks can be trained on a large corpus of melodies and turned into automated music composers able to generate new melodies coherent with the style they have been trained on.", "objective_label"], ["We employ gated recurrent unit networks that have been shown to be particularly efficient in learning complex sequential activations with arbitrary long time lags.", "method_label"], ["Our model processes rhythm and melody in parallel while modeling the relation between these two features.", "method_label"], ["Using such an approach, we were able to generate interesting complete melodies or suggest possible continuations of a melody fragment that is coherent with the characteristics of the fragment itself.", "result_label"]]]
[0, [["Graph Convolutional Networks (GCNs) have proved to be a most powerful architecture in aggregating local neighborhood information for individual graph nodes.", "background_label"], ["Low-rank proximities and node features are successfully leveraged in existing GCNs, however, attributes that graph links may carry are commonly ignored, as almost all of these models simplify graph links into binary or scalar values describing node connectedness.", "background_label"], ["In our paper instead, links are reverted to hypostatic relationships between entities with descriptional attributes.", "background_label"], ["We propose GCN-LASE (GCN with Link Attributes and Sampling Estimation), a novel GCN model taking both node and link attributes as inputs.", "method_label"], ["To adequately captures the interactions between link and node attributes, their tensor product is used as neighbor features, based on which we define several graph kernels and further develop according architectures for LASE.", "method_label"], ["Besides, to accelerate the training process, the sum of features in entire neighborhoods are estimated through Monte Carlo method, with novel sampling strategies designed for LASE to minimize the estimation variance.", "method_label"], ["Our experiments show that LASE outperforms strong baselines over various graph datasets, and further experiments corroborate the informativeness of link attributes and our model's ability of adequately leveraging them.", "result_label"]]]
[0, [["We present a unified framework to study graph kernels, special cases of which include the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05}, marginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04}, and geometric kernel on graphs \\citep{Gaertner02}.", "background_label"], ["Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from $O(n^6)$ to $O(n^3)$.", "method_label"], ["When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain.", "method_label"], ["Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches.", "method_label"], ["We then explore connections between diffusion kernels \\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels, and use these connections to propose new graph kernels.", "method_label"], ["Finally, we show that rational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized to graphs reduce to the random walk graph kernel.", "result_label"]]]
[0, [["Numerous important problems can be framed as learning from graph data.", "background_label"], ["We propose a framework for learning convolutional neural networks for arbitrary graphs.", "background_label"], ["These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes.", "background_label"], ["Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs.", "method_label"], ["Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.", "result_label"]]]
[0, [["Many successful methods have been proposed for learning low dimensional representations on large-scale networks, while almost all existing methods are designed in inseparable processes, learning embeddings for entire networks even when only a small proportion of nodes are of interest.", "background_label"], ["This leads to great inconvenience, especially on super-large or dynamic networks, where these methods become almost impossible to implement.", "background_label"], ["In this paper, we formalize the problem of separated matrix factorization, based on which we elaborate a novel objective function that preserves both local and global information.", "method_label"], ["We further propose SepNE, a simple and flexible network embedding algorithm which independently learns representations for different subsets of nodes in separated processes.", "method_label"], ["By implementing separability, our algorithm reduces the redundant efforts to embed irrelevant nodes, yielding scalability to super-large networks, automatic implementation in distributed learning and further adaptations.", "method_label"], ["We demonstrate the effectiveness of this approach on several real-world networks with different scales and subjects.", "result_label"], ["With comparable accuracy, our approach significantly outperforms state-of-the-art baselines in running times on large networks.", "result_label"]]]
[0, [["We present DeepWalk, a novel approach for learning latent representations of vertices in a network.", "background_label"], ["These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models.", "background_label"], ["DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.", "method_label"], ["DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences.", "method_label"], ["We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube.", "result_label"], ["Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information.", "background_label"], ["DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse.", "background_label"], ["In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.", "method_label"], ["DeepWalk is also scalable.", "method_label"], ["It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable.", "method_label"], ["These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.", "result_label"]]]
[0, [["Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms.", "background_label"], ["Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves.", "background_label"], ["However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks.", "background_label"], ["Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks.", "objective_label"], ["In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes.", "method_label"], ["We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods.", "method_label"], ["Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.", "method_label"], ["We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains.", "result_label"], ["Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.", "result_label"]]]
[0, [["Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices.", "background_label"], ["The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers.", "background_label"], ["In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method.", "objective_label"], ["By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling.", "method_label"], ["More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method.", "method_label"], ["Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections.", "method_label"], ["Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.", "result_label"]]]
[0, [["The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process.", "background_label"], ["In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations.", "background_label"], ["We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces.", "method_label"], ["Our recurrent modules compare the input to virtual reference objects (cf.", "method_label"], ["filters in CNN) via the kernels.", "method_label"], ["Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training.", "method_label"], ["We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.", "result_label"]]]
[0, [["Structural identity is a concept of symmetry in which network nodes are identified according to the network structure and their relationship to other nodes.", "background_label"], ["Structural identity has been studied in theory and practice over the past decades, but only recently has it been addressed with representational learning techniques.", "background_label"], ["This work presents struc2vec, a novel and flexible framework for learning latent representations for the structural identity of nodes.", "objective_label"], ["struc2vec uses a hierarchy to measure node similarity at different scales, and constructs a multilayer graph to encode structural similarities and generate structural context for nodes.", "method_label"], ["Numerical experiments indicate that state-of-the-art techniques for learning node representations fail in capturing stronger notions of structural identity, while struc2vec exhibits much superior performance in this task, as it overcomes limitations of prior approaches.", "result_label"], ["As a consequence, numerical experiments indicate that struc2vec improves performance on classification tasks that depend more on structural identity.", "result_label"]]]
[0, [["Ground robots which are able to navigate a variety of terrains are needed in many domains.", "background_label"], ["One of the key aspects is the capability to adapt to the ground structure, which can be realized through movable body parts coming along with additional degrees of freedom (DoF).", "background_label"], ["However, planning respective locomotion is challenging since suitable representations result in large state spaces.", "background_label"], ["Employing an additional abstract representation---which is coarser, lower-dimensional, and semantically enriched---can support the planning.", "method_label"], ["While a desired robot representation and action set of such an abstract representation can be easily defined, the cost function requires large tuning efforts.", "method_label"], ["We propose a method to represent the cost function as a CNN.", "method_label"], ["Training of the network is done on generated artificial data, while it generalizes well to the abstraction of real world scenes.", "method_label"], ["We further apply our method to the problem of search-based planning of hybrid driving-stepping locomotion.", "method_label"], ["The abstract representation is used as a powerful informed heuristic which accelerates planning by multiple orders of magnitude.", "method_label"]]]
[0, [["A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization.", "background_label"], ["To this end, we introduce universal planning networks (UPN).", "method_label"], ["UPNs embed differentiable planning within a goal-directed policy.", "method_label"], ["This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization.", "method_label"], ["The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective.", "method_label"], ["We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images.", "method_label"], ["The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals.", "method_label"], ["We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.", "result_label"]]]
[0, [["We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within.", "background_label"], ["VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning.", "background_label"], ["Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation.", "method_label"], ["We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task.", "result_label"], ["We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.", "result_label"]]]
[0, [["This paper introduces the QMDP-net, a neural network architecture for planning under partial observability.", "background_label"], ["The QMDP-net combines the strengths of model-free learning and model-based planning.", "background_label"], ["It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture.", "method_label"], ["The QMDP-net is fully differentiable and allows for end-to-end training.", "method_label"], ["We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and \"transfer\"to other similar tasks beyond the set.", "method_label"], ["In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation.", "result_label"], ["Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.", "result_label"]]]
[0, [["Value function estimation is an important task in reinforcement learning, i.e., prediction.", "background_label"], ["The Boltzmann softmax operator is a natural value estimator and can provide several benefits.", "background_label"], ["However, it does not satisfy the non-expansion property, and its direct use may fail to converge even in value iteration.", "background_label"], ["In this paper, we propose to update the value function with dynamic Boltzmann softmax (DBS) operator, which has good convergence property in the setting of planning and learning.", "objective_label"], ["Experimental results on GridWorld show that the DBS operator enables better estimation of the value function, which rectifies the convergence issue of the softmax operator.", "result_label"], ["Finally, we propose the DBS-DQN algorithm by applying dynamic Boltzmann softmax updates in deep Q-network, which outperforms DQN substantially in 40 out of 49 Atari games.", "result_label"]]]
[0, [["This paper surveys the field of reinforcement learning from a computer-science perspective.", "background_label"], ["It is written to be accessible to researchers familiar with machine learning.", "background_label"], ["Both the historical basis of the field and a broad selection of current work are summarized.", "background_label"], ["Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment.", "background_label"], ["The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.''", "method_label"], ["The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state.", "method_label"], ["It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.", "result_label"]]]
[0, [["In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes.", "objective_label"], ["We prove the finite-iteration and asymptotic l\\infty-norm performance-loss bounds for DPP in the presence of approximation/estimation error.", "method_label"], ["The bounds are expressed in terms of the l\\infty-norm of the average accumulated error as opposed to the l\\infty-norm of the error in the case of the standard approximate value iteration (AVI) and the approximate policy iteration (API).", "method_label"], ["This suggests that DPP can achieve a better performance than AVI and API since it averages out the simulation noise caused by Monte-Carlo sampling throughout the learning process.", "result_label"], ["We examine this theoretical results numerically by com- paring the performance of the approximate variants of DPP with existing reinforcement learning (RL) methods on different problem domains.", "result_label"], ["Our results show that, in all cases, DPP-based algorithms outperform other RL methods by a wide margin.", "result_label"]]]
[0, [["This paper introduces new optimality-preserving operators on Q-functions.", "background_label"], ["We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency.", "method_label"], ["We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies.", "method_label"], ["This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context.", "method_label"], ["Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator.", "method_label"], ["As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties.", "method_label"], ["We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.", "result_label"]]]
[0, [["We propose a system for surface completion and inpainting of 3D shapes using generative models, learnt on local patches.", "objective_label"], ["Our method uses a novel encoding of height map based local patches parameterized using 3D mesh quadrangulation of the low resolution input shape.", "method_label"], ["This provides us sufficient amount of local 3D patches to learn a generative model for the task of repairing moderate sized holes.", "method_label"], ["Following the ideas from the recent progress in 2D inpainting, we investigated both linear dictionary based model and convolutional denoising autoencoders based model for the task for inpainting, and show our results to be better than the previous geometry based method of surface inpainting.", "result_label"], ["We validate our method on both synthetic shapes and real world scans.", "result_label"]]]
[0, [["We present OctNet, a representation for deep learning with sparse 3D data.", "background_label"], ["In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution.", "background_label"], ["Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation.", "method_label"], ["This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution.", "method_label"], ["We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.", "result_label"]]]
[0, [["In this paper, we present a learning based approach to depth fusion, i.e., dense 3D reconstruction from multiple depth images.", "background_label"], ["The most common approach to depth fusion is based on averaging truncated signed distance functions, which was originally proposed by Curless and Levoy in 1996.", "method_label"], ["While this method is simple and provides great results, it is not able to reconstruct (partially) occluded surfaces and requires a large number frames to filter out sensor noise and outliers.", "method_label"], ["Motivated by the availability of large 3D model repositories and recent advances in deep learning, we present a novel 3D CNN architecture that learns to predict an implicit surface representation from the input depth maps.", "method_label"], ["Our learning based method significantly outperforms the traditional volumetric fusion approach in terms of noise reduction and outlier suppression.", "method_label"], ["By learning the structure of real world 3D objects and scenes, our approach is further able to reconstruct occluded regions and to fill in gaps in the reconstruction.", "method_label"], ["We demonstrate that our learning based approach outperforms both vanilla TSDF fusion as well as TV-L1 fusion on the task of volumetric fusion.", "result_label"], ["Further, we demonstrate state-of-the-art 3D shape completion results.", "result_label"]]]
[0, [["We explore deep Reinforcement Learning(RL) algorithms for scalping trading and knew that there is no appropriate trading gym and agent examples.", "background_label"], ["Thus we propose gym and agent like Open AI gym in finance.", "objective_label"], ["Not only that, we introduce new RL framework based on our hybrid algorithm which leverages between supervised learning and RL algorithm and uses meaningful observations such order book and settlement data from experience watching scalpers trading.", "method_label"], ["That is very crucial information for traders behavior to be decided.", "method_label"], ["To feed these data into our model, we use spatio-temporal convolution layer, called Conv3D for order book data and temporal CNN, called Conv1D for settlement data.", "method_label"], ["Those are preprocessed by episode filter we developed.", "method_label"], ["Agent consists of four sub agents divided to clarify their own goal to make best decision.", "background_label"], ["Also, we adopted value and policy based algorithm to our framework.", "method_label"], ["With these features, we could make agent mimic scalpers as much as possible.", "background_label"], ["In many fields, RL algorithm has already begun to transcend human capabilities in many domains.", "method_label"], ["This approach could be a starting point to beat human in the financial stock market, too and be a good reference for anyone who wants to design RL algorithm in real world domain.", "method_label"], ["Finally, weexperiment our framework and gave you experiment progress.", "result_label"]]]
[0, [["Deep Reinforcement Learning has yielded proficient controllers for complex tasks.", "background_label"], ["However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point.", "background_label"], ["To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM.", "objective_label"], ["The resulting \\textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens.", "method_label"], ["Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability.", "method_label"], ["Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's.", "result_label"], ["Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.", "result_label"]]]
[0, [["We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.", "background_label"], ["The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.", "method_label"], ["We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm.", "method_label"], ["We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.", "result_label"]]]
[0, [["Portfolio management is the decision-making process of allocating an amount of fund into different financial investment products.", "background_label"], ["Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency.", "background_label"], ["This paper presents a model-less convolutional neural network with historic prices of a set of financial assets as its input, outputting portfolio weights of the set.", "objective_label"], ["The network is trained with 0.7 years' price data from a cryptocurrency exchange.", "method_label"], ["The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network.", "method_label"], ["Backtest trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10-fold returns in 1.8 months' periods.", "method_label"], ["Some recently published portfolio selection strategies are also used to perform the same back-tests, whose results are compared with the neural network.", "result_label"], ["The network is not limited to cryptocurrency, but can be applied to any other financial markets.", "result_label"]]]
[0, [["Recent progresses in visual tracking have greatly improved the tracking performance.", "background_label"], ["However, challenges such as occlusion and view change remain obstacles in real world deployment.", "background_label"], ["A natural solution to these challenges is to use multiple cameras with multiview inputs, though existing systems are mostly limited to specific targets (e.g.", "background_label"], ["human), static cameras, and/or camera calibration.", "background_label"], ["To break through these limitations, we propose a generic multiview tracking (GMT) framework that allows camera movement, while requiring neither specific object model nor camera calibration.", "method_label"], ["A key innovation in our framework is a cross-camera trajectory prediction network (TPN), which implicitly and dynamically encodes camera geometric relations, and hence addresses missing target issues such as occlusion.", "method_label"], ["Moreover, during tracking, we assemble information across different cameras to dynamically update a novel collaborative correlation filter (CCF), which is shared among cameras to achieve robustness against view change.", "method_label"], ["The two components are integrated into a correlation filter tracking framework, where the features are trained offline using existing single view tracking datasets.", "method_label"], ["For evaluation, we first contribute a new generic multiview tracking dataset (GMTD) with careful annotations, and then run experiments on GMTD and the PETS2009 datasets.", "result_label"], ["On both datasets, the proposed GMT algorithm shows clear advantages over state-of-the-art ones.", "result_label"]]]
[0, [["In the field of generic object tracking numerous attempts have been made to exploit deep features.", "background_label"], ["Despite all expectations, deep trackers are yet to reach an outstanding level of performance compared to methods solely based on handcrafted features.", "background_label"], ["In this paper, we investigate this key issue and propose an approach to unlock the true potential of deep features for tracking.", "objective_label"], ["We systematically study the characteristics of both deep and shallow features, and their relation to tracking accuracy and robustness.", "method_label"], ["We identify the limited data and low spatial resolution as the main challenges, and propose strategies to counter these issues when integrating deep features for tracking.", "method_label"], ["Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy.", "method_label"], ["Extensive experiments are performed on four challenging datasets.", "method_label"], ["On VOT2017, our approach significantly outperforms the top performing tracker from the challenge with a relative gain of 17% in EAO.", "result_label"]]]
[0, [["In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking.", "background_label"], ["However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded.", "background_label"], ["Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting.", "background_label"], ["In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance.", "objective_label"], ["We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity.", "method_label"], ["We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor.", "method_label"], ["When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method in the VOT2016 challenge.", "result_label"], ["Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.", "result_label"]]]
[0, [["Discriminant Correlation Filters (DCF) based methods now become a kind of dominant approach to online object tracking.", "background_label"], ["The features used in these methods, however, are either based on hand-crafted features like HoGs, or convolutional features trained independently from other tasks like image classification.", "background_label"], ["In this work, we present an end-to-end lightweight network architecture, namely DCFNet, to learn the convolutional features and perform the correlation tracking process simultaneously.", "objective_label"], ["Specifically, we treat DCF as a special correlation filter layer added in a Siamese network, and carefully derive the backpropagation through it by defining the network output as the probability heatmap of object location.", "method_label"], ["Since the derivation is still carried out in Fourier frequency domain, the efficiency property of DCF is preserved.", "method_label"], ["This enables our tracker to run at more than 60 FPS during test time, while achieving a significant accuracy gain compared with KCF using HoGs.", "method_label"], ["Extensive evaluations on OTB-2013, OTB-2015, and VOT2015 benchmarks demonstrate that the proposed DCFNet tracker is competitive with several state-of-the-art trackers, while being more compact and much faster.", "result_label"]]]
[0, [["Accurate scale estimation of a target is a challenging research problem in visual object tracking.", "background_label"], ["Most state-of-the-art methods employ an exhaustive scale search to estimate the target size.", "background_label"], ["The exhaustive search strategy is computationally expensive and struggles when encountered with large scale variations.", "background_label"], ["This paper investigates the problem of accurate and robust scale estimation in a tracking-by-detection framework.", "objective_label"], ["We propose a novel scale adaptive tracking approach by learning separate discriminative correlation filters for translation and scale estimation.", "method_label"], ["The explicit scale filter is learned online using the target appearance sampled at a set of different scales.", "method_label"], ["Contrary to standard approaches, our method directly learns the appearance change induced by variations in the target scale.", "background_label"], ["Additionally, we investigate strategies to reduce the computational cost of our approach.", "method_label"], ["Extensive experiments are performed on the OTB and the VOT2014 datasets.", "method_label"], ["Compared to the standard exhaustive scale search, our approach achieves a gain of 2.5% in average overlap precision on the OTB dataset.", "method_label"], ["Additionally, our method is computationally efficient, operating at a 50% higher frame rate compared to the exhaustive scale search.", "method_label"], ["Our method obtains the top rank in performance by outperforming 19 state-of-the-art trackers on OTB and 37 state-of-the-art trackers on VOT2014.", "result_label"]]]
[0, [["A typical conversation comprises of multiple turns between participants where they go back-and-forth between different topics.", "background_label"], ["At each user turn, dialogue state tracking (DST) aims to estimate user's goal by processing the current utterance.", "background_label"], ["However, in many turns, users implicitly refer to the previous goal, necessitating the use of relevant dialogue history.", "background_label"], ["Nonetheless, distinguishing relevant history is challenging and a popular method of using dialogue recency for that is inefficient.", "background_label"], ["We, therefore, propose a novel framework for DST that identifies relevant historical context by referring to the past utterances where a particular slot-value changes and uses that together with weighted system utterance to identify the relevant context.", "method_label"], ["Specifically, we use the current user utterance and the most recent system utterance to determine the relevance of a system utterance.", "method_label"], ["Empirical analyses show that our method improves joint goal accuracy by 2.75% and 2.36% on WoZ 2.0 and MultiWoZ 2.0 restaurant domain datasets respectively over the previous state-of-the-art GLAD model.", "result_label"]]]
[0, [["The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the previous three editions as follows: the number of slot-value pairs present in the ontology is much larger, no spoken language understanding output is given, and utterances are labeled at the subdialog level.", "background_label"], ["This paper describes a novel dialog state tracking method designed to work robustly under these conditions, using elaborate string matching, coreference resolution tailored for dialogs and a few other improvements.", "method_label"], ["The method can correctly identify many values that are not explicitly present in the utterance.", "method_label"], ["On the final evaluation, our method came in first among 7 competing teams and 24 entries.", "method_label"], ["The F1-score achieved by our method was 9 and 7 percentage points higher than that of the runner-up for the utterance-level evaluation and for the subdialog-level evaluation, respectively.", "result_label"]]]
[0, [["Modern audio source separation techniques rely on optimizing sequence model architectures such as, 1D-CNNs, on mixture recordings to generalize well to unseen mixtures.", "background_label"], ["Specifically, recent focus is on time-domain based architectures such as Wave-U-Net which exploit temporal context by extracting multi-scale features.", "background_label"], ["However, the optimality of the feature extraction process in these architectures has not been well investigated.", "background_label"], ["In this paper, we examine and recommend critical architectural changes that forge an optimal multi-scale feature extraction process.", "objective_label"], ["To this end, we replace regular $1-$D convolutions with adaptive dilated convolutions that have innate capability of capturing increased context by using large temporal receptive fields.", "method_label"], ["We also investigate the impact of dense connections on the extraction process that encourage feature reuse and better gradient flow.", "method_label"], ["The dense connections between the downsampling and upsampling paths of a U-Net architecture capture multi-resolution information leading to improved temporal modelling.", "method_label"], ["We evaluate the proposed approaches on the MUSDB test dataset.", "result_label"], ["In addition to providing an improved performance over the state-of-the-art, we also provide insights on the impact of different architectural choices on complex data-driven solutions for source separation.", "result_label"]]]
[0, [["Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output.", "background_label"], ["In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion.", "background_label"], ["Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections.", "method_label"], ["For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers.", "method_label"], ["DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.", "method_label"], ["We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).", "result_label"], ["DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance.", "result_label"], ["Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .", "other_label"]]]
[0, [["Estimating 3D hand meshes from single RGB images is challenging, due to intrinsic 2D-3D mapping ambiguities and limited training data.", "background_label"], ["We adopt a compact parametric 3D hand model that represents deformable and articulated hand meshes.", "background_label"], ["To achieve the model fitting to RGB images, we investigate and contribute in three ways: 1) Neural rendering: inspired by recent work on human body, our hand mesh estimator (HME) is implemented by a neural network and a differentiable renderer, supervised by 2D segmentation masks and 3D skeletons.", "method_label"], ["HME demonstrates good performance for estimating diverse hand shapes and improves pose estimation accuracies.", "result_label"], ["2) Iterative testing refinement: Our fitting function is differentiable.", "result_label"], ["We iteratively refine the initial estimate using the gradients, in the spirit of iterative model fitting methods like ICP.", "result_label"], ["The idea is supported by the latest research on human body.", "background_label"], ["3) Self-data augmentation: collecting sized RGB-mesh (or segmentation mask)-skeleton triplets for training is a big hurdle.", "background_label"], ["Once the model is successfully fitted to input RGB images, its meshes i.e.", "background_label"], ["shapes and articulations, are realistic, and we augment view-points on top of estimated dense hand poses.", "method_label"], ["Experiments using three RGB-based benchmarks show that our framework offers beyond state-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D hand shapes.", "result_label"], ["Each technical component above meaningfully improves the accuracy in the ablation study.", "result_label"]]]
[0, [["For modeling the 3D world behind 2D images, which 3D representation is most appropriate?", "background_label"], ["A polygon mesh is a promising candidate for its compactness and geometric properties.", "background_label"], ["However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation.", "background_label"], ["Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks.", "objective_label"], ["Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach.", "method_label"], ["Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time.", "method_label"], ["These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.", "result_label"]]]
[0, [["Low-cost consumer depth cameras and deep learning have enabled reasonable 3D hand pose estimation from single depth images.", "background_label"], ["In this paper, we present an approach that estimates 3D hand pose from regular RGB images.", "background_label"], ["This task has far more ambiguities due to the missing depth information.", "background_label"], ["To this end, we propose a deep network that learns a network-implicit 3D articulation prior.", "method_label"], ["Together with detected keypoints in the images, this network yields good estimates of the 3D pose.", "method_label"], ["We introduce a large scale 3D hand pose dataset based on synthetic hand models for training the involved networks.", "method_label"], ["Experiments on a variety of test sets, including one on sign language recognition, demonstrate the feasibility of 3D hand pose estimation on single color images.", "result_label"]]]
[0, [["In this work, we establish dense correspondences between RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation.", "background_label"], ["We first gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efficient annotation pipeline.", "method_label"], ["We then use our dataset to train CNN-based systems that deliver dense correspondence 'in the wild', namely in the presence of background, occlusions and scale variations.", "method_label"], ["We improve our training set's effectiveness by training an 'inpainting' network that can fill in missing groundtruth values and report clear improvements with respect to the best results that would be achievable in the past.", "method_label"], ["We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter; we further improve accuracy through cascading, obtaining a system that delivers highly0accurate results in real time.", "result_label"], ["Supplementary materials and videos are provided on the project page http://densepose.org", "other_label"]]]
[0, [["Estimating the 3D pose of a hand is an essential part of human-computer interaction.", "background_label"], ["Estimating 3D pose using depth or multi-view sensors has become easier with recent advances in computer vision, however, regressing pose from a single RGB image is much less straightforward.", "background_label"], ["The main difficulty arises from the fact that 3D pose requires some form of depth estimates, which are ambiguous given only an RGB image.", "background_label"], ["In this paper we propose a new method for 3D hand pose estimation from a monocular image through a novel 2.5D pose representation.", "method_label"], ["Our new representation estimates pose up to a scaling factor, which can be estimated additionally if a prior of the hand size is given.", "method_label"], ["We implicitly learn depth maps and heatmap distributions with a novel CNN architecture.", "method_label"], ["Our system achieves the state-of-the-art estimation of 2D and 3D hand pose on several challenging datasets in presence of severe occlusions.", "result_label"]]]
[0, [["This work addresses the problem of estimating the full body 3D human pose and shape from a single color image.", "background_label"], ["This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions.", "background_label"], ["Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets.", "objective_label"], ["Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework.", "method_label"], ["This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction.", "method_label"], ["Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks.", "method_label"], ["These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training.", "method_label"], ["Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss.", "method_label"], ["Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks).", "method_label"], ["The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.", "result_label"]]]
[0, [["We present OctNet, a representation for deep learning with sparse 3D data.", "background_label"], ["In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution.", "background_label"], ["Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation.", "method_label"], ["This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution.", "method_label"], ["We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.", "result_label"]]]
[0, [["We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand.", "background_label"], ["We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand.", "method_label"], ["The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers.", "method_label"], ["Finally, the reprojected triangulations are used as new labeled training data to improve the detector.", "method_label"], ["We repeat this process, generating more labeled data in each iteration.", "method_label"], ["We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector.", "method_label"], ["The method is used to train a hand keypoint detector for single images.", "method_label"], ["The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors.", "method_label"], ["The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.", "result_label"]]]
[0, [["In collaborative learning, multiple parties contribute their datasets to jointly deduce global machine learning models for numerous predictive tasks.", "background_label"], ["Despite its efficacy, this learning paradigm fails to encompass critical application domains that involve highly sensitive data, such as healthcare and security analytics, where privacy risks limit entities to individually train models using only their own datasets.", "background_label"], ["In this work, we target privacy-preserving collaborative hierarchical clustering.", "objective_label"], ["We introduce a formal security definition that aims to achieve the balance between utility and privacy and present a two-party protocol that provably satisfies it.", "method_label"], ["We then extend our protocol with: (i) an optimized version for the single-linkage clustering, and (ii) scalable approximation variants.", "method_label"], ["We implement all our schemes and experimentally evaluate their performance and accuracy on synthetic and real datasets, obtaining very encouraging results.", "method_label"], ["For example, end-to-end execution of our secure approximate protocol for over 1M 10-dimensional data samples requires 35sec of computation and achieves 97.09% accuracy.", "result_label"]]]
[0, [["Machine learning algorithms based on deep neural networks have achieved remarkable results and are being extensively used in different domains.", "background_label"], ["However, the machine learning algorithms requires access to raw data which is often privacy sensitive.", "background_label"], ["To address this issue, we develop new techniques to provide solutions for running deep neural networks over encrypted data.", "background_label"], ["In this paper, we develop new techniques to adopt deep neural networks within the practical limitation of current homomorphic encryption schemes.", "objective_label"], ["More specifically, we focus on classification of the well-known convolutional neural networks (CNN).", "objective_label"], ["First, we design methods for approximation of the activation functions commonly used in CNNs (i.e.", "method_label"], ["ReLU, Sigmoid, and Tanh) with low degree polynomials which is essential for efficient homomorphic encryption schemes.", "method_label"], ["Then, we train convolutional neural networks with the approximation polynomials instead of original activation functions and analyze the performance of the models.", "background_label"], ["Finally, we implement convolutional neural networks over encrypted data and measure performance of the models.", "method_label"], ["Our experimental results validate the soundness of our approach with several convolutional neural networks with varying number of layers and structures.", "method_label"], ["When applied to the MNIST optical character recognition tasks, our approach achieves 99.52\\% accuracy which significantly outperforms the state-of-the-art solutions and is very close to the accuracy of the best non-private version, 99.77\\%.", "result_label"], ["Also, it can make close to 164000 predictions per hour.", "result_label"], ["We also applied our approach to CIFAR-10, which is much more complex compared to MNIST, and were able to achieve 91.5\\% accuracy with approximation polynomials used as activation functions.", "method_label"], ["These results show that CryptoDL provides efficient, accurate and scalable privacy-preserving predictions.", "result_label"]]]
[0, [["We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained.", "background_label"], ["We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset.", "background_label"], ["To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.", "method_label"], ["We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\"providers such as Google and Amazon.", "result_label"], ["Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks.", "result_label"], ["We then investigate the factors that influence this leakage and evaluate mitigation strategies.", "result_label"]]]
[0, [["Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains.", "background_label"], ["Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information.", "background_label"], ["The models should not expose private information in these datasets.", "background_label"], ["Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy.", "objective_label"], ["Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.", "result_label"]]]
[0, [["The growing popularity of cloud-based machine learning raises a natural question about the privacy guarantees that can be provided in such a setting.", "background_label"], ["Our work tackles this problem in the context where a client wishes to classify private images using a convolutional neural network (CNN) trained by a server.", "background_label"], ["Our goal is to build efficient protocols whereby the client can acquire the classification result without revealing their input to the server, while guaranteeing the privacy of the server's neural network.", "objective_label"], ["To this end, we design Gazelle, a scalable and low-latency system for secure neural network inference, using an intricate combination of homomorphic encryption and traditional two-party computation techniques (such as garbled circuits).", "method_label"], ["Gazelle makes three contributions.", "method_label"], ["First, we design the Gazelle homomorphic encryption library which provides fast algorithms for basic homomorphic operations such as SIMD (single instruction multiple data) addition, SIMD multiplication and ciphertext permutation.", "method_label"], ["Second, we implement the Gazelle homomorphic linear algebra kernels which map neural network layers to optimized homomorphic matrix-vector multiplication and convolution routines.", "method_label"], ["Third, we design optimized encryption switching protocols which seamlessly convert between homomorphic and garbled circuit encodings to enable implementation of complete neural network inference.", "method_label"], ["We evaluate our protocols on benchmark neural networks trained on the MNIST and CIFAR-10 datasets and show that Gazelle outperforms the best existing systems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint 2017/1164) by 30 times in online runtime.", "result_label"], ["Similarly when compared with fully homomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders of magnitude faster online run-time.", "result_label"]]]
[0, [["We approach video object segmentation (VOS) by splitting the task into two sub-tasks: bounding box level tracking, followed by bounding box segmentation.", "background_label"], ["Following this paradigm, we present BoLTVOS (Box-Level Tracking for VOS), which consists of an R-CNN detector conditioned on the first-frame bounding box to detect the object of interest, a temporal consistency rescoring algorithm, and a Box2Seg network that converts bounding boxes to segmentation masks.", "method_label"], ["BoLTVOS performs VOS using only the firstframe bounding box without the mask.", "method_label"], ["We evaluate our approach on DAVIS 2017 and YouTube-VOS, and show that it outperforms all methods that do not perform first-frame fine-tuning.", "method_label"], ["We further present BoLTVOS-ft, which learns to segment the object in question using the first-frame mask while it is being tracked, without increasing the runtime.", "method_label"], ["BoLTVOS-ft outperforms PReMVOS, the previously best performing VOS method on DAVIS 2016 and YouTube-VOS, while running up to 45 times faster.", "result_label"], ["Our bounding box tracker also outperforms all previous short-term and longterm trackers on the bounding box level tracking datasets OTB 2015 and LTB35.", "result_label"], ["A newer version of this work can be found at arXiv:1911.12836.", "other_label"]]]
[0, [["Video object segmentation targets at segmenting a specific object throughout a video sequence, given only an annotated first frame.", "background_label"], ["Recent deep learning based approaches find it effective by fine-tuning a general-purpose segmentation model on the annotated frame using hundreds of iterations of gradient descent.", "background_label"], ["Despite the high accuracy these methods achieve, the fine-tuning process is inefficient and fail to meet the requirements of real world applications.", "background_label"], ["We propose a novel approach that uses a single forward pass to adapt the segmentation model to the appearance of a specific object.", "method_label"], ["Specifically, a second meta neural network named modulator is learned to manipulate the intermediate layers of the segmentation network given limited visual and spatial information of the target object.", "method_label"], ["The experiments show that our approach is 70times faster than fine-tuning approaches while achieving similar accuracy.", "result_label"]]]
[0, [["In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach.", "objective_label"], ["Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task.", "method_label"], ["Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second.", "method_label"], ["Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state of the art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017.", "result_label"], ["The project website is http://www.robots.ox.ac.uk/~qwang/SiamMask.", "other_label"]]]
[0, [["In this paper we present a large-scale visual object detection and tracking benchmark, named VisDrone2018, aiming at advancing visual understanding tasks on the drone platform.", "objective_label"], ["The images and video sequences in the benchmark were captured over various urban/suburban areas of 14 different cities across China from north to south.", "background_label"], ["Specifically, VisDrone2018 consists of 263 video clips and 10,209 images (no overlap with video clips) with rich annotations, including object bounding boxes, object categories, occlusion, truncation ratios, etc.", "background_label"], ["With intensive amount of effort, our benchmark has more than 2.5 million annotated instances in 179,264 images/video frames.", "method_label"], ["Being the largest such dataset ever published, the benchmark enables extensive evaluation and investigation of visual analysis algorithms on the drone platform.", "method_label"], ["In particular, we design four popular tasks with the benchmark, including object detection in images, object detection in videos, single object tracking, and multi-object tracking.", "method_label"], ["All these tasks are extremely challenging in the proposed dataset due to factors such as occlusion, large scale and pose variation, and fast motion.", "result_label"], ["We hope the benchmark largely boost the research and development in visual analysis on drone platforms.", "result_label"]]]
[0, [["Video Object Segmentation, and video processing in general, has been historically dominated by methods that rely on the temporal consistency and redundancy in consecutive video frames.", "background_label"], ["When the temporal smoothness is suddenly broken, such as when an object is occluded, or some frames are missing in a sequence, the result of these methods can deteriorate significantly or they may not even produce any result at all.", "background_label"], ["This paper explores the orthogonal approach of processing each frame independently, i.e disregarding the temporal information.", "objective_label"], ["In particular, it tackles the task of semi-supervised video object segmentation: the separation of an object from the background in a video, given its mask in the first frame.", "objective_label"], ["We present Semantic One-Shot Video Object Segmentation (OSVOS-S), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one shot).", "method_label"], ["We show that instance level semantic information, when combined effectively, can dramatically improve the results of our previous method, OSVOS.", "method_label"], ["We perform experiments on two recent video segmentation databases, which show that OSVOS-S is both the fastest and most accurate method in the state of the art.", "result_label"]]]
[0, [["Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately.", "background_label"], ["To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications.", "background_label"], ["In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images.", "objective_label"], ["We first utilize a part-based tracking method to deal with challenging factors such as large deformation, occlusion, and cluttered background.", "method_label"], ["Based on the tracked bounding boxes of parts, we construct a region-of-interest segmentation network to generate part masks.", "method_label"], ["Finally, a similarity-based scoring function is adopted to refine these object parts by comparing them to the visual information in the first frame.", "method_label"], ["Our method performs favorably against state-of-the-art algorithms in accuracy on the DAVIS benchmark dataset, while achieving much faster runtime performance.", "result_label"]]]
[0, [["Video object segmentation is challenging yet important in a wide variety of applications for video analysis.", "background_label"], ["Recent works formulate video object segmentation as a prediction task using deep nets to achieve appealing state-of-the-art performance.", "background_label"], ["Due to the formulation as a prediction task, most of these methods require fine-tuning during test time, such that the deep nets memorize the appearance of the objects of interest in the given video.", "background_label"], ["However, fine-tuning is time-consuming and computationally expensive, hence the algorithms are far from real time.", "background_label"], ["To address this issue, we develop a novel matching based algorithm for video object segmentation.", "method_label"], ["In contrast to memorization based classification techniques, the proposed approach learns to match extracted features to a provided template without memorizing the appearance of the objects.", "method_label"], ["We validate the effectiveness and the robustness of the proposed method on the challenging DAVIS-16, DAVIS-17, Youtube-Objects and JumpCut datasets.", "result_label"], ["Extensive results show that our method achieves comparable performance without fine-tuning and is much more favorable in terms of computational time.", "result_label"]]]
[0, [["Convolutional layers are one of the basic building blocks of modern deep neural networks.", "background_label"], ["One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset.", "background_label"], ["We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example.", "objective_label"], ["Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference.", "method_label"], ["We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks.", "method_label"], ["On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3% accuracy with only 413M multiply-adds.", "method_label"], ["Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.", "other_label"]]]
[0, [["Self-attention is a useful mechanism to build generative models for language and images.", "background_label"], ["It determines the importance of context elements by comparing each element to the current time step.", "background_label"], ["In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results.", "objective_label"], ["Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention.", "method_label"], ["We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements.", "method_label"], ["The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic.", "method_label"], ["Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models.", "result_label"], ["On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.", "result_label"]]]
[0, [["We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation.", "background_label"], ["Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network).", "method_label"], ["This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively.", "method_label"], ["The rest of the model remains unchanged and is shared across all languages.", "method_label"], ["We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation.", "method_label"], ["We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.", "result_label"]]]
[0, [["We present a simple, highly modularized network architecture for image classification.", "background_label"], ["Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology.", "background_label"], ["Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set.", "background_label"], ["This strategy exposes a new dimension, which we call \"cardinality\"(the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.", "method_label"], ["On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy.", "method_label"], ["Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity.", "result_label"], ["Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place.", "method_label"], ["We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart.", "result_label"], ["The code and models are publicly available online.", "result_label"]]]
[0, [["Training convolutional networks (CNN's) that fit on a single GPU with minibatch stochastic gradient descent has become effective in practice.", "background_label"], ["However, there is still no effective method for training large CNN's that do not fit in the memory of a few GPU cards, or for parallelizing CNN training.", "background_label"], ["In this work we show that a simple hard mixture of experts model can be efficiently trained to good effect on large scale hashtag (multilabel) prediction tasks.", "method_label"], ["Mixture of experts models are not new (Jacobs et.", "other_label"], ["al.", "other_label"], ["1991, Collobert et.", "other_label"], ["al.", "other_label"], ["2003), but in the past, researchers have had to devise sophisticated methods to deal with data fragmentation.", "background_label"], ["We show empirically that modern weakly supervised data sets are large enough to support naive partitioning schemes where each data point is assigned to a single expert.", "background_label"], ["Because the experts are independent, training them in parallel is easy, and evaluation is cheap for the size of the model.", "method_label"], ["Furthermore, we show that we can use a single decoding layer for all the experts, allowing a unified feature embedding space.", "method_label"], ["We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.", "result_label"]]]
[0, [["We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs).", "background_label"], ["SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy.", "background_label"], ["Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e., conditioned on the computational path).", "background_label"], ["SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples.", "method_label"], ["In particular, we embed infinitely many function weights (e.g.", "method_label"], ["filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter.", "method_label"], ["Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight.", "method_label"], ["We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks.", "method_label"], ["Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet.", "result_label"]]]
[0, [["Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules.", "background_label"], ["In this work, we introduce two new modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling.", "objective_label"], ["Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision.", "method_label"], ["The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks.", "method_label"], ["Extensive experiments validate the effectiveness of our approach on sophisticated vision tasks of object detection and semantic segmentation.", "result_label"], ["The code would be released.", "result_label"]]]
[0, [["Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive.", "background_label"], ["The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013).", "background_label"], ["It operates by selectively activating only parts of the network at a time.", "background_label"], ["In this paper, we use reinforcement learning as a tool to optimize conditional computation policies.", "objective_label"], ["More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem.", "method_label"], ["We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy.", "method_label"], ["We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy.", "method_label"], ["We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.", "result_label"]]]
[0, [["Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications.", "background_label"], ["We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy.", "objective_label"], ["Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image.", "method_label"], ["In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy.", "method_label"], ["We conduct extensive experiments on CIFAR and ImageNet.", "method_label"], ["The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information.", "result_label"], ["Built upon a ResNet-101 model, our method achieves a speedup of 20\\% on average, going as high as 36\\% for some images, while maintaining the same 76.4\\% top-1 accuracy on ImageNet.", "result_label"]]]
[0, [["We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths.", "background_label"], ["Though some approaches have advantages over others, the resulting networks are often qualitatively similar.", "background_label"], ["We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images.", "method_label"], ["Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.", "result_label"]]]
[0, [["The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration.", "background_label"], ["The best performing models also connect the encoder and decoder through an attention mechanism.", "background_label"], ["We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.", "objective_label"], ["Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.", "method_label"], ["Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU.", "result_label"], ["On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.", "result_label"], ["We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "result_label"]]]
[0, [["While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient.", "background_label"], ["We exploit this observation by learning to skip convolutional layers on a per-input basis.", "background_label"], ["We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer.", "method_label"], ["We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions.", "method_label"], ["We show SkipNet reduces computation by 30-90% while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods.", "result_label"], ["We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.", "result_label"]]]
[0, [["Recognizing human actions is a core challenge for autonomous systems as they directly share the same space with humans.", "background_label"], ["Systems must be able to recognize and assess human actions in real-time.", "background_label"], ["In order to train corresponding data-driven algorithms, a significant amount of annotated training data is required.", "background_label"], ["We demonstrated a pipeline to detect humans, estimate their pose, track them over time and recognize their actions in real-time with standard monocular camera sensors.", "method_label"], ["For action recognition, we encode the human pose into a new data format called Encoded Human Pose Image (EHPI) that can then be classified using standard methods from the computer vision community.", "method_label"], ["With this simple procedure we achieve competitive state-of-the-art performance in pose-based action detection and can ensure real-time performance.", "method_label"], ["In addition, we show a use case in the context of autonomous driving to demonstrate how such a system can be trained to recognize human actions using simulation data.", "result_label"]]]
[0, [["Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains.", "background_label"], ["They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance.", "background_label"], ["Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts.", "background_label"], ["Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss.", "method_label"], ["We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches.", "method_label"], ["We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA).", "method_label"], ["We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.", "result_label"]]]
[0, [["We introduce CARLA, an open-source simulator for autonomous driving research.", "background_label"], ["CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems.", "background_label"], ["In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely.", "background_label"], ["The simulation platform supports flexible specification of sensor suites and environmental conditions.", "method_label"], ["We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning.", "method_label"], ["The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research.", "result_label"], ["The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E", "other_label"]]]
[0, [["Recent progress in multiple object tracking (MOT) has shown that a robust similarity score is key to the success of trackers.", "background_label"], ["A good similarity score is expected to reflect multiple cues, e.g.", "background_label"], ["appearance, location, and topology, over a long period of time.", "background_label"], ["However, these cues are heterogeneous, making them hard to be combined in a unified network.", "background_label"], ["As a result, existing methods usually encode them in separate networks or require a complex training approach.", "background_label"], ["In this paper, we present a unified framework for similarity measurement which could simultaneously encode various cues and perform reasoning across both spatial and temporal domains.", "method_label"], ["We also study the feature representation of a tracklet-object pair in depth, showing a proper design of the pair features can well empower the trackers.", "method_label"], ["The resulting approach is named spatial-temporal relation networks (STRN).", "method_label"], ["It runs in a feed-forward way and can be trained in an end-to-end manner.", "method_label"], ["The state-of-the-art accuracy was achieved on all of the MOT15-17 benchmarks using public detection and online settings.", "result_label"]]]
[0, [["Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence.", "background_label"], ["Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system.", "objective_label"], ["Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks.", "method_label"], ["We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics.", "method_label"], ["Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations.", "result_label"], ["Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.", "result_label"]]]
[0, [["This paper introduces a novel approach to the task of data association within the context of pedestrian tracking, by introducing a two-stage learning scheme to match pairs of detections.", "objective_label"], ["First, a Siamese convolutional neural network (CNN) is trained to learn descriptors encoding local spatio-temporal structures between the two input image patches, aggregating pixel values and optical flow information.", "method_label"], ["Second, a set of contextual features derived from the position and size of the compared input patches are combined with the CNN output by means of a gradient boosting classifier to generate the final matching probability.", "method_label"], ["This learning approach is validated by using a linear programming based multi-person tracker showing that even a simple and efficient tracker may outperform much more complex models when fed with our learned matching probabilities.", "method_label"], ["Results on publicly available sequences show that our method meets state-of-the-art standards in multiple people tracking.", "result_label"]]]
[0, [["Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era.", "background_label"], ["All state-of-the-art object detection systems still rely on recognizing object instances individually, without exploiting their relations during learning.", "background_label"], ["This work proposes an object relation module.", "objective_label"], ["It processes a set of objects simultaneously through interaction between their appearance feature and geometry, thus allowing modeling of their relations.", "objective_label"], ["It is lightweight and in-place.", "method_label"], ["It does not require additional supervision and is easy to embed in existing networks.", "method_label"], ["It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline.", "method_label"], ["It verifies the efficacy of modeling object relations in CNN based detection.", "result_label"], ["It gives rise to the first fully end-to-end object detector.", "result_label"]]]
[0, [["In this work, we define a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers.", "objective_label"], ["We focus on consensus super teaching.", "objective_label"], ["It aims at organizing distributed teachers to jointly select a compact while informative training subset from data hosted by the teachers to make a learner learn better.", "objective_label"], ["The challenges arise from three perspectives.", "objective_label"], ["First, the state-of-the-art pool-based super teaching method applies mixed-integer non-linear programming (MINLP) which does not scale well to very large data sets.", "method_label"], ["Second, it is desirable to restrict data access of the teachers to only their own data during the collaboration stage to mitigate privacy leaks.", "background_label"], ["Finally, the teaching collaboration should be communication-efficient since large communication overheads can cause synchronization delays between teachers.", "background_label"], ["To address these challenges, we formulate collaborative teaching as a consensus and privacy-preserving optimization process to minimize teaching risk.", "method_label"], ["We theoretically demonstrate the necessity of collaboration between teachers for improving the learner's learning.", "method_label"], ["Furthermore, we show that the proposed method enjoys a similar property as the Oracle property of adaptive Lasso.", "method_label"], ["The empirical study illustrates that our teaching method can deliver significantly more accurate teaching results with high speed, while the non-collaborative MINLP-based super teaching becomes prohibitively expensive to compute.", "result_label"]]]
[0, [["Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner.", "background_label"], ["Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization.", "background_label"], ["This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression.", "objective_label"], ["We also exhibit optimal training sets that match these teaching dimensions.", "method_label"], ["Our approach generalizes to other linear learners.", "method_label"]]]
[0, [["In this paper, we consider the problem of machine teaching, the inverse problem of machine learning.", "background_label"], ["Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.", "method_label"], ["We show that the teaching complexity in the iterative case is very different from that in the batch case.", "method_label"], ["Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model.", "method_label"], ["Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.", "method_label"], ["We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.", "result_label"]]]
[0, [["Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations.", "background_label"], ["A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students.", "background_label"], ["In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \\emph{learning}.", "background_label"], ["In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies.", "method_label"], ["We call this approach `learning to teach'.", "method_label"], ["In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model).", "method_label"], ["The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution.", "method_label"], ["To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).", "result_label"]]]
[0, [["Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.", "objective_label"], ["We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model.", "method_label"], ["The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance.", "background_label"], ["In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g.", "method_label"], ["either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server.", "method_label"], ["Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.", "result_label"]]]
[0, [["We study offline data poisoning attacks in contextual bandits, a class of reinforcement learning problems with important applications in online recommendation and adaptive medical treatment, among others.", "background_label"], ["We provide a general attack framework based on convex optimization and show that by slightly manipulating rewards in the data, an attacker can force the bandit algorithm to pull a target arm for a target contextual vector.", "method_label"], ["The target arm and target contextual vector are both chosen by the attacker.", "method_label"], ["That is, the attacker can hijack the behavior of a contextual bandit.", "method_label"], ["We also investigate the feasibility and the side effects of such attacks, and identify future directions for defense.", "method_label"], ["Experiments on both synthetic and real-world data demonstrate the efficiency of the attack algorithm.", "result_label"]]]
[0, [["We call a learner super-teachable if a teacher can trim down an iid training set while making the learner learn even better.", "background_label"], ["We provide sharp super-teaching guarantees on two learners: the maximum likelihood estimator for the mean of a Gaussian, and the large margin classifier in 1D.", "background_label"], ["For general learners, we provide a mixed-integer nonlinear programming-based algorithm to find a super teaching set.", "method_label"], ["Empirical experiments show that our algorithm is able to find good super-teaching sets for both regression and classification problems.", "result_label"]]]
[0, [["What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner?", "background_label"], ["We propose an optimal teaching framework aimed at learners who employ Bayesian models.", "objective_label"], ["Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher.", "method_label"], ["This optimization problem is in general hard.", "method_label"], ["In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for finding the optimal teaching set.", "method_label"], ["Our algorithm optimizes the aggregate sufficient statistics, then unpacks them into actual teaching examples.", "method_label"], ["We give several examples to illustrate our framework.", "result_label"]]]
[0, [["Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server.", "background_label"], ["In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence.", "objective_label"], ["We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates.", "method_label"], ["To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective.", "method_label"], ["We follow up by using parameter estimation for the benign agents' updates to improve on attack success.", "method_label"], ["Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable.", "method_label"], ["Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.", "result_label"]]]
[0, [["Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on.", "background_label"], ["In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert.", "objective_label"], ["We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting.", "method_label"], ["We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning.", "method_label"], ["Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.", "result_label"]]]
[0, [["High sensitivity of neural architecture search (NAS) methods against their input such as step-size (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process.", "objective_label"], ["Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS.", "objective_label"], ["We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation.", "method_label"], ["It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast).", "method_label"], ["We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust).", "method_label"], ["Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-the-art performances with low computational budgets both on image classification and inpainting tasks.", "result_label"]]]
[0, [["We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline.", "background_label"], ["In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell.", "objective_label"], ["To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed.", "method_label"], ["We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently.", "method_label"], ["This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint.", "method_label"], ["In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet.", "method_label"], ["It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "result_label"], ["We have released our implementation at https://github.com/SNAS-Series/SNAS-Series.", "result_label"]]]
[0, [["We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design.", "objective_label"], ["In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph.", "background_label"], ["The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set.", "method_label"], ["Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss.", "method_label"], ["Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search.", "method_label"], ["On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing.", "method_label"], ["On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.", "result_label"]]]
[0, [["This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner.", "background_label"], ["Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent.", "method_label"], ["Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.", "method_label"], ["Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.", "result_label"]]]
[0, [["Many real-world applications involve multivariate, geo-tagged time series data: at each location, multiple sensors record corresponding measurements.", "background_label"], ["For example, air quality monitoring system records PM2.5, CO, etc.", "background_label"], ["The resulting time-series data often has missing values due to device outages or communication errors.", "background_label"], ["In order to impute the missing values, state-of-the-art methods are built on Recurrent Neural Networks (RNN), which process each time stamp sequentially, prohibiting the direct modeling of the relationship between distant time stamps.", "background_label"], ["Recently, the self-attention mechanism has been proposed for sequence modeling tasks such as machine translation, significantly outperforming RNN because the relationship between each two time stamps can be modeled explicitly.", "method_label"], ["In this paper, we are the first to adapt the self-attention mechanism for multivariate, geo-tagged time series data.", "objective_label"], ["In order to jointly capture the self-attention across multiple dimensions, including time, location and the sensor measurements, while maintain low computational complexity, we propose a novel approach called Cross-Dimensional Self-Attention (CDSA) to process each dimension sequentially, yet in an order-independent manner.", "method_label"], ["Our extensive experiments on four real-world datasets, including three standard benchmarks and our newly collected NYC-traffic dataset, demonstrate that our approach outperforms the state-of-the-art imputation and forecasting methods.", "result_label"], ["A detailed systematic analysis confirms the effectiveness of our design choices.", "result_label"]]]
[0, [["The linearly constrained matrix rank minimization problem is widely applicable in many fields such as control, signal processing and system identification.", "background_label"], ["The tightest convex relaxation of this problem is the linearly constrained nuclear norm minimization.", "background_label"], ["Although the latter can be cast as a semidefinite programming problem, such an approach is computationally expensive to solve when the matrices are large.", "background_label"], ["In this paper, we propose fixed point and Bregman iterative algorithms for solving the nuclear norm minimization problem and prove convergence of the first of these algorithms.", "objective_label"], ["By using a homotopy approach together with an approximate singular value decomposition procedure, we get a very fast, robust and powerful algorithm, which we call FPCA (Fixed Point Continuation with Approximate SVD), that can solve very large matrix rank minimization problems.", "method_label"], ["Our numerical results on randomly generated and real matrix completion problems demonstrate that this algorithm is much faster and provides much better recoverability than semidefinite programming solvers such as SDPT3.", "result_label"], ["For example, our algorithm can recover 1000 x 1000 matrices of rank 50 with a relative error of 1e-5 in about 3 minutes by sampling only 20 percent of the elements.", "result_label"], ["We know of no other method that achieves as good recoverability.", "result_label"], ["Numerical experiments on online recommendation, DNA microarray data set and image inpainting problems demonstrate the effectiveness of our algorithms.", "result_label"]]]
[0, [["Time series are widely used as signals in many classification/regression tasks.", "background_label"], ["It is ubiquitous that time series contains many missing values.", "background_label"], ["Given multiple correlated time series data, how to fill in missing values and to predict their class labels?", "background_label"], ["Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space.", "background_label"], ["In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data.", "objective_label"], ["Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption.", "method_label"], ["The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation.BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data.We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity.", "result_label"], ["Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.", "result_label"]]]
[0, [["This paper introduces a novel algorithm to approximate the matrix with minimum nuclear norm among all matrices obeying a set of convex constraints.", "background_label"], ["This problem may be understood as the convex relaxation of a rank minimization problem, and arises in many important applications as in the task of recovering a large matrix from a small subset of its entries (the famous Netflix problem).", "background_label"], ["Off-the-shelf algorithms such as interior point methods are not directly amenable to large problems of this kind with over a million unknown entries.", "background_label"], ["This paper develops a simple first-order and easy-to-implement algorithm that is extremely efficient at addressing problems in which the optimal solution has low rank.", "method_label"], ["The algorithm is iterative and produces a sequence of matrices (X^k, Y^k) and at each step, mainly performs a soft-thresholding operation on the singular values of the matrix Y^k.", "method_label"], ["There are two remarkable features making this attractive for low-rank matrix completion problems.", "background_label"], ["The first is that the soft-thresholding operation is applied to a sparse matrix; the second is that the rank of the iterates X^k is empirically nondecreasing.", "background_label"], ["Both these facts allow the algorithm to make use of very minimal storage space and keep the computational cost of each iteration low.", "background_label"], ["We provide numerical examples in which 1,000 by 1,000 matrices are recovered in less than a minute on a modest desktop computer.", "method_label"], ["We also demonstrate that our approach is amenable to very large scale problems by recovering matrices of rank about 10 with nearly a billion unknowns from just about 0.4% of their sampled entries.", "result_label"], ["Our methods are connected with linearized Bregman iterations for l1 minimization, and we develop a framework in which one can understand these algorithms in terms of well-known Lagrange multiplier algorithms.", "result_label"]]]
[0, [["Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings.", "background_label"], ["Recent work has primarily focused on explaining models for tasks like image classification or visual question answering.", "background_label"], ["In this paper, we introduce Salient Attributes for Network Explanation (SANE) to explain image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classification score.", "method_label"], ["In this task, an explanation depends on both of the input images, so standard methods do not apply.", "method_label"], ["Our SANE explanations pairs a saliency map identifying important image regions with an attribute that best explains the match.", "method_label"], ["We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition.", "method_label"], ["Our approach's ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2.", "result_label"], ["Code available at: https://github.com/VisionLearningGroup/SANE", "other_label"]]]
[0, [["As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions.", "background_label"], ["In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks \"look\"in an image for evidence for their predictions.", "background_label"], ["However, these techniques are limited by their heuristic nature and architectural constraints.", "background_label"], ["In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm.", "method_label"], ["Second, we specialise the framework to find the part of an image most responsible for a classifier decision.", "method_label"], ["Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.", "result_label"]]]
[0, [["Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images.", "background_label"], ["However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind.", "background_label"], ["Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets.", "background_label"], ["We introduce two such tools here.", "method_label"], ["The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g.", "method_label"], ["a live webcam stream).", "method_label"], ["We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work.", "method_label"], ["The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space.", "method_label"], ["Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations.", "method_label"], ["Both tools are open source and work on a pre-trained convnet with minimal setup.", "result_label"]]]
[0, [["Existing visual explanation generating agents learn to fluently justify a class prediction.", "background_label"], ["However, they may mention visual attributes which reflect a strong class prior, although the evidence may not actually be in the image.", "background_label"], ["This is particularly concerning as ultimately such agents fail in building trust with human users.", "background_label"], ["To overcome this limitation, we propose a phrase-critic model to refine generated candidate explanations augmented with flipped phrases which we use as negative examples while training.", "method_label"], ["At inference time, our phrase-critic model takes an image and a candidate explanation as input and outputs a score indicating how well the candidate explanation is grounded in the image.", "method_label"], ["Our explainable AI agent is capable of providing counter arguments for an alternative prediction, i.e.", "method_label"], ["counterfactuals, along with explanations that justify the correct classification decisions.", "method_label"], ["Our model improves the textual explanation quality of fine-grained classification decisions on the CUB dataset by mentioning phrases that are grounded in the image.", "method_label"], ["Moreover, on the FOIL tasks, our agent detects when there is a mistake in the sentence, grounds the incorrect phrase and corrects it significantly better than other models.", "result_label"]]]
[0, [["Despite widespread adoption, machine learning models remain mostly black boxes.", "background_label"], ["Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.", "background_label"], ["Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.", "background_label"], ["In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.", "method_label"], ["We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.", "method_label"], ["We demonstrate the flexibility of these methods by explaining different models for text (e.g.", "method_label"], ["random forests) and image classification (e.g.", "method_label"], ["neural networks).", "method_label"], ["We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.", "result_label"]]]
[0, [["In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels.", "background_label"], ["While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks.", "background_label"], ["Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach.", "result_label"], ["We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them", "result_label"]]]
[0, [["Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark.", "background_label"], ["However there is no clear understanding of why they perform so well, or how they might be improved.", "background_label"], ["In this paper we address both issues.", "background_label"], ["We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier.", "method_label"], ["We also perform an ablation study to discover the performance contribution from different model layers.", "method_label"], ["This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark.", "method_label"], ["We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "result_label"]]]
[0, [["Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself.", "background_label"], ["Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions.", "background_label"], ["We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image.", "objective_label"], ["We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity.", "method_label"], ["Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.", "result_label"]]]
[0, [["When an image classifier makes a prediction, which parts of the image are relevant and why?", "background_label"], ["We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision?", "objective_label"], ["Producing an answer requires marginalizing over images that could have been seen but weren't.", "method_label"], ["We can sample plausible image in-fills by conditioning a generative model on the rest of the image.", "method_label"], ["We then optimize to find the image regions that most change the classifier's decision after in-fill.", "method_label"], ["Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image.", "method_label"], ["Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.", "result_label"]]]
[0, [["Most existing super-resolution methods do not perform well in real scenarios due to lack of realistic training data and information loss of the model input.", "background_label"], ["To solve the first problem, we propose a new pipeline to generate realistic training data by simulating the imaging process of digital cameras.", "method_label"], ["And to remedy the information loss of the input, we develop a dual convolutional neural network to exploit the originally captured radiance information in raw images.", "method_label"], ["In addition, we propose to learn a spatially-variant color transformation which helps more effective color corrections.", "method_label"], ["Extensive experiments demonstrate that super-resolution with raw data helps recover fine details and clear structures, and more importantly, the proposed network and data generation pipeline achieve superior results for single image super-resolution in real scenarios.", "result_label"]]]
[0, [["Many creative ideas are being proposed for image sensor designs, and these may be useful in applications ranging from consumer photography to computer vision.", "background_label"], ["To understand and evaluate each new design, we must create a corresponding image processing pipeline that transforms the sensor data into a form that is appropriate for the application.", "background_label"], ["The need to design and optimize these pipelines is time-consuming and costly.", "objective_label"], ["We explain a method that combines machine learning and image systems simulation that automates the pipeline design.", "method_label"], ["The approach is based on a new way of thinking of the image processing pipeline as a large collection of local linear filters.", "method_label"], ["We illustrate how the method has been used to design pipelines for novel sensor architectures in consumer photography applications.", "result_label"]]]
[0, [["Imaging in low light is challenging due to low photon count and low SNR.", "background_label"], ["Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical.", "background_label"], ["A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night.", "background_label"], ["To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images.", "method_label"], ["Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network.", "method_label"], ["The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data.", "method_label"], ["We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work.", "result_label"], ["The results are shown in the supplementary video at https://youtu.be/qWKUFK7MWvg", "result_label"]]]
[0, [["To combine explicit and implicit generative models, we introduce semi-implicit generator (SIG) as a flexible hierarchical model that can be trained in the maximum likelihood framework.", "method_label"], ["Both theoretically and experimentally, we demonstrate that SIG can generate high quality samples especially when dealing with multi-modality.", "method_label"], ["By introducing SIG as an unbiased regularizer to the generative adversarial network (GAN), we show the interplay between maximum likelihood and adversarial learning can stabilize the adversarial training, resist the notorious mode collapsing problem of GANs, and improve the diversity of generated random samples.", "result_label"]]]
[0, [["Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes.", "background_label"], ["We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution.", "objective_label"], ["We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models.", "method_label"], ["We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.", "result_label"]]]
[0, [["Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classification.", "background_label"], ["Noise-contrastive estimation (NCE) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples.", "background_label"], ["Generative adversarial networks (GANs) are pairs of generator and discriminator networks, with the generator network learning to generate samples by attempting to fool the discriminator network into believing its samples are real data.", "method_label"], ["Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum likelihood estimation (MLE).", "method_label"], ["NCE corresponds to training an internal data model belonging to the {\\em discriminator} network but using a fixed generator network.", "method_label"], ["We show that a variant of NCE, with a dynamic generator network, is equivalent to maximum likelihood estimation.", "method_label"], ["Since pairing a learned discriminator with an appropriate dynamically selected generator recovers MLE, one might expect the reverse to hold for pairing a learned generator with a certain discriminator.", "method_label"], ["However, we show that recovering MLE for a learned generator requires departing from the distinguishability game.", "method_label"], ["Specifically:  (i) The expected gradient of the NCE discriminator can be made to match the expected gradient of  MLE, if one is allowed to use a non-stationary noise distribution for NCE,  (ii) No choice of discriminator network can make the expected gradient for the GAN generator match that of MLE, and  (iii) The existing theory does not guarantee that GANs will converge in the non-convex case.", "result_label"], ["This suggests that the key next step in GAN research is to determine whether GANs converge, and if not, to modify their training algorithm to force convergence.", "result_label"]]]
[0, [["We present a framework to understand GAN training as alternating density ratio estimation and approximate divergence minimization.", "background_label"], ["This provides an interpretation for the mismatched GAN generator and discriminator objectives often used in practice, and explains the problem of poor sample diversity.", "background_label"], ["We also derive a family of generator objectives that target arbitrary $f$-divergences without minimizing a lower bound, and use them to train generative image models that target either improved sample quality or greater sample diversity.", "method_label"]]]
[0, [["Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images.", "background_label"], ["However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space.", "background_label"], ["We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample.", "method_label"], ["This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor.", "method_label"], ["We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise.", "method_label"], ["We also illustrate experimentally that this procedure addresses the problem of missing modes.", "result_label"]]]
[0, [["We show that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics.", "background_label"], ["However, generalization does occur for a weaker metric called neural net distance.", "background_label"], ["It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a special class of generators with natural training objectives when generator capacity and training set sizes are moderate.", "method_label"], ["This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.", "result_label"]]]
[0, [["We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator.", "method_label"], ["This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions.", "method_label"], ["We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.", "result_label"]]]
[0, [["Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification.", "background_label"], ["This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive.", "background_label"], ["Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models.", "objective_label"], ["Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors.", "method_label"], ["On standard datasets like CIFAR-10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7% over using 4K supervised examples alone, and captures over 95% of the improvement from the same number of labeled examples.", "method_label"], ["Finally, we report an improvement of 4% over the previous state-of-the-art on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset.", "result_label"], ["This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.", "result_label"]]]
[0, [["What is the role of unlabeled data in an inference problem, when the presumed underlying distribution is adversarially perturbed?", "background_label"], ["To provide a concrete answer to this question, this paper unifies two major learning frameworks: Semi-Supervised Learning (SSL) and Distributionally Robust Learning (DRL).", "objective_label"], ["We develop a generalization theory for our framework based on a number of novel complexity measures, such as an adversarial extension of Rademacher complexity and its semi-supervised analogue.", "method_label"], ["Moreover, our analysis is able to quantify the role of unlabeled data in the generalization under a more general condition compared to the existing theoretical works in SSL.", "method_label"], ["Based on our framework, we also present a hybrid of DRL and EM algorithms that has a guaranteed convergence rate.", "method_label"], ["When implemented with deep neural networks, our method shows a comparable performance to those of the state-of-the-art on a number of real-world benchmark datasets.", "result_label"]]]
[0, [["This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness.", "background_label"], ["We motivate 'adversarial risk' as an objective for achieving models robust to worst-case inputs.", "objective_label"], ["We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk.", "method_label"], ["This suggests that models may optimize this surrogate rather than the true adversarial risk.", "method_label"], ["We formalize this notion as 'obscurity to an adversary,' and develop tools and heuristics for identifying obscured models and designing transparent models.", "method_label"], ["We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero.", "result_label"], ["Our hope is that our formulations and results will help researchers to develop more powerful defenses.", "result_label"]]]
[0, [["We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning.", "background_label"], ["Theoretically, we revisit the simple Gaussian model of Schmidt et al.", "background_label"], ["that shows a sample complexity gap between standard and robust classification.", "background_label"], ["We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy.", "method_label"], ["Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\\ell_\\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\\ell_2$ and $\\ell_\\infty$ robustness via randomized smoothing.", "result_label"], ["On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.", "result_label"]]]
[0, [["Neural network robustness has recently been highlighted by the existence of adversarial examples.", "background_label"], ["Many previous works show that the learned networks do not perform well on perturbed test data, and significantly more labeled data is required to achieve adversarially robust generalization.", "background_label"], ["In this paper, we theoretically and empirically show that with just more unlabeled data, we can learn a model with better adversarially robust generalization.", "objective_label"], ["The key insight of our results is based on a risk decomposition theorem, in which the expected robust risk is separated into two parts: the stability part which measures the prediction stability in the presence of perturbations, and the accuracy part which evaluates the standard classification accuracy.", "method_label"], ["As the stability part does not depend on any label information, we can optimize this part using unlabeled data.", "method_label"], ["We further prove that for a specific Gaussian mixture problem, adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabeled data is provided.", "method_label"], ["Inspired by the theoretical findings, we further show that a practical adversarial training algorithm that leverages unlabeled data can improve adversarial robust generalization on MNIST and Cifar-10.", "result_label"]]]
[0, [["Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance.", "background_label"], ["However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train.", "background_label"], ["To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks.", "objective_label"], ["We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts.", "method_label"], ["For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet.", "result_label"], ["Our code and models are available at https://github.com/szagoruyko/wide-residual-networks", "other_label"]]]
[0, [["Machine learning models are often susceptible to adversarial perturbations of their inputs.", "background_label"], ["Even small perturbations can cause state-of-the-art classifiers with high \"standard\"accuracy to produce an incorrect prediction with high confidence.", "background_label"], ["To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization.", "method_label"], ["We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of \"standard\"learning.", "method_label"], ["This gap is information theoretic and holds irrespective of the training algorithm or the model family.", "method_label"], ["We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well.", "result_label"], ["We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.", "result_label"]]]
[0, [["Adversarial examples are malicious inputs designed to fool machine learning models.", "background_label"], ["They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters.", "background_label"], ["Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs.", "background_label"], ["So far, adversarial training has primarily been applied to small problems.", "background_label"], ["In this research, we apply adversarial training to ImageNet.", "objective_label"], ["Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\"effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.", "result_label"]]]
[0, [["We consider a model of robust learning in an adversarial environment.", "background_label"], ["The learner gets uncorrupted training data with access to possible corruptions that may be effected by the adversary during testing.", "background_label"], ["The learner's goal is to build a robust classifier, which will be tested on future adversarial examples.", "objective_label"], ["The adversary is limited to $k$ possible corruptions for each input.", "method_label"], ["We model the learner-adversary interaction as a zero-sum game.", "method_label"], ["This model is closely related to the adversarial examples model of Schmidt et al.", "result_label"], ["(2018); Madry et al.", "result_label"], ["(2017).", "result_label"], ["Our main results consist of generalization bounds for the binary and multiclass classification, as well as the real-valued case (regression).", "background_label"], ["For the binary classification setting, we both tighten the generalization bound of Feige, Mansour, and Schapire (2015), and are also able to handle infinite hypothesis classes.", "method_label"], ["The sample complexity is improved from $O(\\frac{1}{\\epsilon^4}\\log(\\frac{|\\mathcal{H}|}{\\delta}))$ to $O\\big(\\frac{1}{\\epsilon^2}(\\sqrt{k \\mathrm{VC}(\\mathcal{H})}\\log^{\\frac{3}{2}+\\alpha}(k\\mathrm{VC}(\\mathcal{H}))+\\log(\\frac{1}{\\delta})\\big)$ for any $\\alpha > 0$.", "method_label"], ["Additionally, we extend the algorithm and generalization bound from the binary to the multiclass and real-valued cases.", "method_label"], ["Along the way, we obtain results on fat-shattering dimension and Rademacher complexity of $k$-fold maxima over function classes; these may be of independent interest.", "result_label"], ["For binary classification, the algorithm of Feige et al.", "method_label"], ["(2015) uses a regret minimization algorithm and an ERM oracle as a black box; we adapt it for the multiclass and regression settings.", "method_label"], ["The algorithm provides us with near-optimal policies for the players on a given training sample.", "method_label"]]]
[0, [["Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks.", "background_label"], ["While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties.", "background_label"], ["In this paper we report two such properties.", "objective_label"], ["First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis.", "method_label"], ["It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.", "method_label"], ["Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend.", "method_label"], ["We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error.", "method_label"], ["In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.", "result_label"]]]
[0, [["Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples.", "background_label"], ["Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN.", "background_label"], ["We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs.", "objective_label"], ["We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs).", "method_label"], ["We find that DAEs can remove substantial amounts of the adversarial noise.", "method_label"], ["How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion.", "result_label"], ["As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE).", "method_label"], ["This increases the network robustness to adversarial examples, without a significant performance penalty.", "result_label"]]]
[0, [["Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence.", "background_label"], ["Moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data.", "background_label"], ["In this paper, we focus on $\\ell_\\infty$ attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity.", "objective_label"], ["For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded $\\ell_1$ norm.", "method_label"], ["The results also extend to multi-class linear classifiers.", "method_label"], ["For (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists.", "result_label"], ["We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting.", "method_label"], ["Our results indicate that having $\\ell_1$ norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting.", "result_label"], ["We demonstrate experimental results that validate our theoretical findings.", "result_label"]]]
[0, [["Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence.", "background_label"], ["Early attempts at explaining this phenomenon focused on nonlinearity and overfitting.", "background_label"], ["We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature.", "background_label"], ["This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets.", "method_label"], ["Moreover, this view yields a simple and fast method of generating adversarial examples.", "method_label"], ["Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.", "result_label"]]]
[0, [["In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data.", "background_label"], ["In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples.", "background_label"], ["In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks.", "objective_label"], ["Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples.", "method_label"], ["This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting).", "method_label"], ["We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded.", "result_label"], ["We also sketch some countermeasures suggested by our analysis.", "result_label"]]]
[0, [["Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network.", "background_label"], ["In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models.", "background_label"], ["To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization.", "objective_label"], ["This approach provides us with a broad and unifying view on much of the prior work on this topic.", "method_label"], ["Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal.", "method_label"], ["In particular, they specify a concrete security guarantee that would protect against any adversary.", "method_label"], ["These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks.", "method_label"], ["They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee.", "method_label"], ["We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.", "result_label"], ["Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.", "other_label"]]]
[0, [["In this paper, a novel Generation-Evaluation framework is developed for multi-turn conversations with the objective of letting both participants know more about each other.", "background_label"], ["For the sake of rational knowledge utilization and coherent conversation flow, a dialogue strategy which controls knowledge selection is instantiated and continuously adapted via reinforcement learning.", "method_label"], ["Under the deployed strategy, knowledge grounded conversations are conducted with two dialogue agents.", "method_label"], ["The generated dialogues are comprehensively evaluated on aspects like informativeness and coherence, which are aligned with our objective and human instinct.", "method_label"], ["These assessments are integrated as a compound reward to guide the evolution of dialogue strategy via policy gradient.", "method_label"], ["Comprehensive experiments have been carried out on the publicly available dataset, demonstrating that the proposed method outperforms the other state-of-the-art approaches significantly.", "result_label"]]]
[0, [["Conversational modeling is an important task in natural language understanding and machine intelligence.", "background_label"], ["Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules.", "background_label"], ["In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework.", "method_label"], ["Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation.", "method_label"], ["The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules.", "result_label"], ["We find that this straightforward model can generate simple conversations given a large conversational training dataset.", "background_label"], ["Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well.", "result_label"], ["It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles.", "method_label"], ["On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations.", "result_label"], ["On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning.", "method_label"], ["As expected, we also find that the lack of consistency is a common failure mode of our model.", "result_label"]]]
[0, [["This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures.", "background_label"], ["By doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces.", "background_label"], ["The work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions.", "background_label"], ["The result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer.", "method_label"], ["This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.", "result_label"]]]
[0, [["This paper presents a kernel-based discriminative learning framework on probability measures.", "background_label"], ["Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data.", "background_label"], ["By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion.", "method_label"], ["To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM).", "method_label"], ["Our analyses of SMMs provides several insights into their relationship to traditional SVMs.", "method_label"], ["Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example.", "method_label"], ["Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.", "result_label"]]]
[0, [["Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks.", "background_label"], ["Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences.", "background_label"], ["In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences.", "background_label"], ["For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability.", "background_label"], ["In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model.", "method_label"], ["We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way.", "method_label"], ["In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets.", "method_label"], ["We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.", "result_label"]]]
[0, [["Covariance pooling is a feature pooling method with good classification accuracy.", "background_label"], ["Because covariance features consist of second-order statistics, the scale of the feature elements are varied.", "background_label"], ["Therefore, normalizing covariance features using a matrix square root affects the performance improvement.", "background_label"], ["When pooling methods are applied to local features extracted from CNN models, the accuracy increases when the pooling function is back-propagatable and the feature-extraction model is learned in an end-to-end manner.", "method_label"], ["Recently, the iterative polynomial approximation method for the matrix square root of a covariance feature was proposed, and resulted in a faster and more stable training than the methods based on singular-value decomposition.", "method_label"], ["In this paper, we propose an extension of compact bilinear pooling, which is a compact approximation of the standard covariance feature, to the polynomials of the covariance feature.", "method_label"], ["Subsequently, we apply the proposed approximation to the polynomial corresponding to the matrix square root to obtain a compact approximation for the square root of the covariance feature.", "method_label"], ["Our method approximates a higher-dimensional polynomial of a covariance by the weighted sum of the approximate features corresponding to a pair of local features based on the similarity of the local features.", "method_label"], ["We apply our method for standard fine-grained image recognition datasets and demonstrate that the proposed method shows comparable accuracy with fewer dimensions than the original feature.", "result_label"]]]
[0, [["Bilinear pooling of Convolutional Neural Network (CNN) features [22, 23], and their compact variants [10], have been shown to be effective at fine-grained recognition, scene categorization, texture recognition, and visual question-answering tasks among others.", "background_label"], ["The resulting representation captures second-order statistics of convolutional features in a translationally invariant manner.", "background_label"], ["In this paper we investigate various ways of normalizing these statistics to improve their representation power.", "objective_label"], ["In particular we find that the matrix square-root normalization offers significant improvements and outperforms alternative schemes such as the matrix logarithm normalization when combined with elementwise square-root and l2 normalization.", "method_label"], ["This improves the accuracy by 2-3% on a range of fine-grained recognition datasets leading to a new state of the art.", "method_label"], ["We also investigate how the accuracy of matrix function computations effect network training and evaluation.", "method_label"], ["In particular we compare against a technique for estimating matrix square-root gradients via solving a Lyapunov equation that is more numerically accurate than computing gradients via a Singular Value Decomposition (SVD).", "method_label"], ["We find that while SVD gradients are numerically inaccurate the overall effect on the final accuracy is negligible once boundary cases are handled carefully.", "result_label"], ["We present an alternative scheme for computing gradients that is faster and yet it offers improvements over the baseline model.", "method_label"], ["Finally we show that the matrix square-root computed approximately using a few Newton iterations is just as accurate for the classification task but allows an order-of-magnitude faster GPU implementation compared to SVD decomposition.", "result_label"]]]
[0, [["Approximating non-linear kernels using feature maps has gained a lot of interest in recent years due to applications in reducing training and testing times of SVM classifiers and other kernel based learning algorithms.", "background_label"], ["We extend this line of work and present low distortion embeddings for dot product kernels into linear Euclidean spaces.", "objective_label"], ["We base our results on a classical result in harmonic analysis characterizing all dot product kernels and use it to define randomized feature maps into explicit low dimensional Euclidean spaces in which the native dot product provides an approximation to the dot product kernel with high confidence.", "method_label"]]]
[0, [["Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition.", "background_label"], ["However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis.", "background_label"], ["We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions.", "objective_label"], ["Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system.", "method_label"], ["The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods.", "method_label"], ["Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.", "result_label"]]]
[0, [["Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve performance in multiple vision tasks.", "background_label"], ["Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion.", "background_label"], ["However, a serious drawback of this family of pooling layers is their dimensionality explosion.", "background_label"], ["Approximate pooling methods with compact properties have been explored towards resolving this weakness.", "background_label"], ["Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize unstable higher order information.", "method_label"], ["However, combining compact pooling with matrix normalization and other order information has not been explored until now.", "result_label"], ["In this paper, we unify bilinear pooling and the global Gaussian embedding layers through the empirical moment matrix.", "method_label"], ["In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods.", "method_label"], ["Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet.", "result_label"], ["Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with encoded features with 96% less dimensions.", "result_label"]]]
[0, [["Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts.", "background_label"], ["We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth.", "objective_label"], ["We propose robust growing and stopping policies to generalize to different network architectures and datasets.", "method_label"], ["Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet.", "method_label"], ["For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts.", "result_label"], ["Our AutoGrow is efficient.", "result_label"], ["It discovers depth within similar time of training a single DNN.", "result_label"], ["Our code is available at https://github.com/wenwei202/autogrow.", "other_label"]]]
[0, [["Deep neural networks are increasingly used on mobile devices, where computational resources are limited.", "background_label"], ["In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency.", "objective_label"], ["It combines dense connectivity with a novel module called learned group convolution.", "method_label"], ["The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous.", "method_label"], ["At test time, our model can be implemented using standard group convolutions, allowing for efficient computation in practice.", "method_label"], ["Our experiments show that CondenseNets are far more efficient than state-of-the-art compact convolutional networks such as MobileNets and ShuffleNets.", "result_label"]]]
[0, [["We present new algorithms for adaptively learning artificial neural networks.", "background_label"], ["Our algorithms (AdaNet) adaptively learn both the structure of the network and its weights.", "background_label"], ["They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail.", "method_label"], ["We report the results of large-scale experiments with one of our algorithms on several binary classification tasks extracted from the CIFAR-10 dataset.", "method_label"], ["The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved for neural networks found by standard approaches.", "result_label"]]]
[0, [["In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction.", "method_label"], ["We further generalize this algorithm to multi-layer and multi-branch cases.", "method_label"], ["Our method reduces the accumulated error and enhance the compatibility with various architectures.", "method_label"], ["Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error.", "result_label"], ["More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.", "result_label"], ["Code has been made publicly available.", "result_label"]]]
[0, [["High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices.", "background_label"], ["In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.", "objective_label"], ["SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation.", "method_label"], ["Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries.", "result_label"], ["These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy.", "method_label"], ["The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers.", "result_label"], ["For AlexNet, structure regularization by SSL also reduces the error by around ~1%.", "result_label"], ["Open source code is in https://github.com/wenwei202/caffe/tree/scnn", "result_label"]]]
[0, [["Today a canonical approach to reduce the computation cost of Deep Neural Networks (DNNs) is to pre-define an over-parameterized model before training to guarantee the learning capacity, and then prune unimportant learning units (filters and neurons) during training to improve model compactness.", "background_label"], ["We argue it is unnecessary to introduce redundancy at the beginning of the training but then reduce redundancy for the ultimate inference model.", "background_label"], ["In this paper, we propose a Continuous Growth and Pruning (CGaP) scheme to minimize the redundancy from the beginning.", "objective_label"], ["CGaP starts the training from a small network seed, then expands the model continuously by reinforcing important learning units, and finally prunes the network to obtain a compact and accurate model.", "method_label"], ["As the growth phase favors important learning units, CGaP provides a clear learning purpose to the pruning phase.", "method_label"], ["Experimental results on representative datasets and DNN architectures demonstrate that CGaP outperforms previous pruning-only approaches that deal with pre-defined structures.", "result_label"], ["For VGG-19 on CIFAR-100 and SVHN datasets, CGaP reduces the number of parameters by 78.9% and 85.8%, FLOPs by 53.2% and 74.2%, respectively; For ResNet-110 On CIFAR-10, CGaP reduces 64.0% number of parameters and 63.3% FLOPs.", "result_label"]]]
[0, [["Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch.", "background_label"], ["In this paper, we address the problem of automatically finding a good network size during a single training cycle.", "objective_label"], ["We introduce *nonparametric neural networks*, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty.", "method_label"], ["We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty.", "method_label"], ["We employ a novel optimization algorithm, which we term *adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising results.", "method_label"]]]
[0, [["We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance.", "background_label"], ["Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies.", "method_label"], ["Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches.", "method_label"], ["We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.", "result_label"]]]
[0, [["This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner.", "background_label"], ["Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent.", "method_label"], ["Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.", "method_label"], ["Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.", "result_label"]]]
[0, [["We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms.", "method_label"], ["Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space.", "method_label"], ["Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al.", "result_label"], ["(2018) in terms of number of models evaluated, and 8 times faster in terms of total compute.", "result_label"], ["The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.", "result_label"]]]
[0, [["Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem.", "background_label"], ["Instead of aiming to select a single optimal architecture, we propose a \"fabric\"that embeds an exponentially large number of architectures.", "objective_label"], ["The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern.", "method_label"], ["The only hyper-parameters of a fabric are the number of channels and layers.", "method_label"], ["While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap.", "method_label"], ["Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size.", "method_label"], ["We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.", "result_label"]]]
[0, [["The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs.", "background_label"], ["Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.", "background_label"], ["However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks.", "background_label"], ["We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy.", "method_label"], ["By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly.", "method_label"], ["In contrast to pruning weights, this approach does not result in sparse connectivity patterns.", "method_label"], ["Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications.", "method_label"], ["We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.", "result_label"]]]
[0, [["Developing neural network image classification models often requires significant architecture engineering.", "background_label"], ["In this paper, we study a method to learn the model architectures directly on the dataset of interest.", "objective_label"], ["As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset.", "method_label"], ["The key contribution of this work is the design of a new search space (the \"NASNet search space\") which enables transferability.", "objective_label"], ["In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named \"NASNet architecture\".", "method_label"], ["We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models.", "result_label"], ["On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art.", "background_label"], ["On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet.", "background_label"], ["Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model.", "method_label"], ["When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models.", "result_label"], ["For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms.", "result_label"], ["Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.", "result_label"]]]
[0, [["The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost.", "background_label"], ["In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy.", "objective_label"], ["This is achieved by enforcing channel-level sparsity in the network in a simple but effective way.", "method_label"], ["Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models.", "method_label"], ["We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy.", "method_label"], ["We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets.", "result_label"], ["For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.", "result_label"]]]
[0, [["We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design.", "objective_label"], ["In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph.", "background_label"], ["The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set.", "method_label"], ["Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss.", "method_label"], ["Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search.", "method_label"], ["On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing.", "method_label"], ["On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.", "result_label"]]]
[0, [["Rendering an accurate image of an isosurface in a volumetric field typically requires large numbers of data samples.", "background_label"], ["Reducing the number of required samples lies at the core of research in volume rendering.", "background_label"], ["With the advent of deep learning networks, a number of architectures have been proposed recently to infer missing samples in multi-dimensional fields, for applications such as image super-resolution and scan completion.", "background_label"], ["In this paper, we investigate the use of such architectures for learning the upscaling of a low-resolution sampling of an isosurface to a higher resolution, with high fidelity reconstruction of spatial detail and shading.", "objective_label"], ["We introduce a fully convolutional neural network, to learn a latent representation generating a smooth, edge-aware normal field and ambient occlusions from a low-resolution normal and depth field.", "method_label"], ["By adding a frame-to-frame motion loss into the learning stage, the upscaling can consider temporal variations and achieves improved frame-to-frame coherence.", "method_label"], ["We demonstrate the quality of the network for isosurfaces which were never seen during training, and discuss remote and in-situ visualization as well as focus+context visualization as potential applications", "result_label"]]]
[0, [["Proliferation of cryptocurrencies (e.g., Bitcoin) that allow pseudo-anonymous transactions, has made it easier for ransomware developers to demand ransom by encrypting sensitive user data.", "background_label"], ["The recently revealed strikes of ransomware attacks have already resulted in significant economic losses and societal harm across different sectors, ranging from local governments to health care.", "background_label"], ["Most modern ransomware use Bitcoin for payments.", "background_label"], ["However, although Bitcoin transactions are permanently recorded and publicly available, current approaches for detecting ransomware depend only on a couple of heuristics and/or tedious information gathering steps (e.g., running ransomware to collect ransomware related Bitcoin addresses).", "background_label"], ["To our knowledge, none of the previous approaches have employed advanced data analytics techniques to automatically detect ransomware related transactions and malicious Bitcoin addresses.", "background_label"], ["By capitalizing on the recent advances in topological data analysis, we propose an efficient and tractable data analytics framework to automatically detect new malicious addresses in a ransomware family, given only a limited records of previous transactions.", "method_label"], ["Furthermore, our proposed techniques exhibit high utility to detect the emergence of new ransomware families, that is, ransomware with no previous records of transactions.", "method_label"], ["Using the existing known ransomware data sets, we show that our proposed methodology provides significant improvements in precision and recall for ransomware transaction detection, compared to existing heuristic based approaches, and can be utilized to automate ransomware detection.", "result_label"]]]
[0, [["In the cryptographic currency Bitcoin, all transactions are recorded in the blockchain - a public, global, and immutable ledger.", "background_label"], ["Because transactions are public, Bitcoin and its users employ obfuscation to maintain a degree of financial privacy.", "background_label"], ["Critically, and in contrast to typical uses of obfuscation, in Bitcoin obfuscation is not aimed against the system designer but is instead enabled by design.", "background_label"], ["We map sixteen proposed privacy-preserving techniques for Bitcoin on an obfuscation-vs.-cryptography axis, and find that those that are used in practice tend toward obfuscation.", "method_label"], ["We argue that this has led to a balance between privacy and regulatory acceptance.", "result_label"]]]
[0, [["Ransomware can prevent a user from accessing a device and its files until a ransom is paid to the attacker, most frequently in Bitcoin.", "background_label"], ["With over 500 known ransomware families, it has become one of the dominant cybercrime threats for law enforcement, security professionals and the public.", "background_label"], ["However, a more comprehensive, evidence-based picture on the global direct financial impact of ransomware attacks is still missing.", "background_label"], ["In this paper, we present a data-driven method for identifying and gathering information on Bitcoin transactions related to illicit activity based on footprints left on the public Bitcoin blockchain.", "method_label"], ["We implement this method on-top-of the GraphSense open-source platform and apply it to empirically analyze transactions related to 35 ransomware families.", "method_label"], ["We estimate the lower bound direct financial impact of each ransomware family and find that, from 2013 to mid-2017, the market for ransomware payments has a minimum worth of USD 12,768,536 (22,967.54 BTC).", "method_label"], ["We also find that the market is highly skewed with only a few number of players responsible for the majority of the payments.", "result_label"], ["Based on these research findings, policy-makers and law enforcement agencies can use the statistics provided to understand the size of the illicit market and make informed decisions on how best to address the threat.", "result_label"]]]
[0, [["The ever increasing computational cost of Deep Neural Networks (DNN) and the demand for energy efficient hardware for DNN acceleration has made accuracy and hardware cost co-optimization for DNNs tremendously important, especially for edge devices.", "background_label"], ["Owing to the large parameter space and cost of evaluating each parameter in the search space, manually tuning of DNN hyperparameters is impractical.", "background_label"], ["Automatic joint DNN and hardware hyperparameter optimization is indispensable for such problems.", "background_label"], ["Bayesian optimization-based approaches have shown promising results for hyperparameter optimization of DNNs.", "result_label"], ["However, most of these techniques have been developed without considering the underlying hardware, thereby leading to inefficient designs.", "method_label"], ["Further, the few works that perform joint optimization are not generalizable and mainly focus on CMOS-based architectures.", "result_label"], ["In this work, we present a novel pseudo agent-based multi-objective hyperparameter optimization (PABO) for maximizing the DNN performance while obtaining low hardware cost.", "background_label"], ["Compared to the existing methods, our work poses a theoretically different approach for joint optimization of accuracy and hardware cost and focuses on memristive crossbar-based accelerators.", "method_label"], ["PABO uses a supervisor agent to establish connections between the posterior Gaussian distribution models of network accuracy and hardware cost requirements.", "method_label"], ["The agent reduces the mathematical complexity of the co-optimization problem by removing unnecessary computations and updates of acquisition functions, thereby achieving significant speed-ups for the optimization procedure.", "method_label"], ["PABO outputs a Pareto frontier that underscores the trade-offs between designing high-accuracy and hardware efficiency.", "method_label"], ["Our results demonstrate a superior performance compared to the state-of-the-art methods both in terms of accuracy and computational speed (~100x speed up).", "result_label"]]]
[0, [["Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters.", "background_label"], ["Unfortunately, this tuning is often a \"black art\"that requires expert experience, unwritten rules of thumb, or sometimes brute-force search.", "background_label"], ["Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand.", "background_label"], ["In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP).", "method_label"], ["The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next.", "method_label"], ["Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization.", "method_label"], ["We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms.", "method_label"], ["We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation.", "method_label"], ["We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.", "result_label"]]]
[0, [["To achieve fully autonomous navigation, vehicles need to compute an accurate model of their direct surrounding.", "background_label"], ["In this paper, a 3D surface reconstruction algorithm from heterogeneous density 3D data is presented.", "objective_label"], ["The proposed method is based on a TSDF voxel-based representation, where an adaptive neighborhood kernel sourced on a Gaussian confidence evaluation is introduced.", "method_label"], ["This enables to keep a good trade-off between the density of the reconstructed mesh and its accuracy.", "method_label"], ["Experimental evaluations carried on both synthetic (CARLA) and real (KITTI) 3D data show a good performance compared to a state of the art method used for surface reconstruction.", "result_label"]]]
[0, [["Computational fluid dynamics (CFD) in many cases requires designing 3D models manually, which is a tedious task that requires specific skills.", "background_label"], ["In this paper, we present a novel method for performing CFD directly on scanned 3D point clouds.", "objective_label"], ["The proposed method builds an anisotropic volumetric tetrahedral mesh adapted around a point-sampled surface, without an explicit surface reconstruction step.", "method_label"], ["The surface is represented by a new extended implicit moving least squares (EIMLS) scalar representation that extends the definition of the function to the entire computational domain, which makes it possible for use in immersed boundary flow simulations.", "method_label"], ["The workflow we present allows us to compute flows around point-sampled geometries automatically.", "method_label"], ["It also gives a better control of the precision around the surface with a limited number of computational nodes, which is a critical issue in CFD.", "result_label"]]]
[0, [["Reusing recorded sounds (sampling) is a key component in Electronic Music Production (EMP), which has been present since its early days and is at the core of genres like hip-hop or jungle.", "background_label"], ["Commercial and non-commercial services allow users to obtain collections of sounds (sample packs) to reuse in their compositions.", "background_label"], ["Automatic classification of one-shot instrumental sounds allows automatically categorising the sounds contained in these collections, allowing easier navigation and better characterisation.", "background_label"], ["Automatic instrument classification has mostly targeted the classification of unprocessed isolated instrumental sounds or detecting predominant instruments in mixed music tracks.", "method_label"], ["For this classification to be useful in audio databases for EMP, it has to be robust to the audio effects applied to unprocessed sounds.", "method_label"], ["In this paper we evaluate how a state of the art model trained with a large dataset of one-shot instrumental sounds performs when classifying instruments processed with audio effects.", "result_label"], ["In order to evaluate the robustness of the model, we use data augmentation with audio effects and evaluate how each effect influences the classification accuracy.", "result_label"]]]
[0, [["Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models.", "background_label"], ["DL is a form of machine learning that uses hierarchical layers of latent features.", "background_label"], ["In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective.", "objective_label"], ["We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation.", "method_label"], ["Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "result_label"]]]
[0, [["Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets.", "background_label"], ["In this paper, we offer contributions in both these areas to enable similar progress in audio modeling.", "objective_label"], ["First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform.", "method_label"], ["Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets.", "method_label"], ["Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline.", "result_label"], ["Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.", "result_label"]]]
[0, [["Neural generation methods for task-oriented dialogue typically generate from a meaning representation that is populated using a database of domain information, such as a table of data describing a restaurant.", "background_label"], ["While earlier work focused solely on the semantic fidelity of outputs, recent work has started to explore methods for controlling the style of the generated text while simultaneously achieving semantic accuracy.", "background_label"], ["Here we experiment with two stylistic benchmark tasks, generating language that exhibits variation in personality, and generating discourse contrast.", "method_label"], ["We report a huge performance improvement in both stylistic control and semantic accuracy over the state of the art on both of these benchmarks.", "method_label"], ["We test several different models and show that putting stylistic conditioning in the decoder and eliminating the semantic re-ranker used in earlier models results in more than 15 points higher BLEU for Personality, with a reduction of semantic error to near zero.", "result_label"], ["We also report an improvement from .75 to .81 in controlling contrast and a reduction in semantic error from 16% to 2%.", "result_label"]]]
[0, [["Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain.", "background_label"], ["This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics.", "objective_label"], ["We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures.", "method_label"], ["With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes.", "method_label"], ["Quantitative evaluation validates the accuracy of sentence and attribute generation.", "result_label"]]]
[0, [["Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text.", "background_label"], ["We experiment with controlling several stylistic aspects of the generated text, in addition to its content.", "method_label"], ["The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts.", "method_label"], ["We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content.", "result_label"]]]
[0, [["We introduce LEAF-QA, a comprehensive dataset of $250,000$ densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts.", "background_label"], ["LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community.", "background_label"], ["Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz.", "background_label"], ["FigureQA and DVQA, which present only limited variations in chart data.", "background_label"], ["LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering.", "method_label"], ["To this end, LEAF-Net, a deep architecture involving chart element localization, question and answer encoding in terms of chart elements, and an attention network is proposed.", "method_label"], ["Different experiments are conducted to demonstrate the challenges of QA on LEAF-QA.", "method_label"], ["The proposed architecture, LEAF-Net also considerably advances the current state-of-the-art on FigureQA and DVQA.", "result_label"]]]
[0, [["Charts are an excellent way to convey patterns and trends in data, but they do not facilitate further modeling of the data or close inspection of individual data points.", "background_label"], ["We present a fully automated system for extracting the numerical values of data points from images of scatter plots.", "objective_label"], ["We use deep learning techniques to identify the key components of the chart, and optical character recognition together with robust regression to map from pixels to the coordinate system of the chart.", "method_label"], ["We focus on scatter plots with linear scales, which already have several interesting challenges.", "method_label"], ["Previous work has done fully automatic extraction for other types of charts, but to our knowledge this is the first approach that is fully automatic for scatter plots.", "method_label"], ["Our method performs well, achieving successful data extraction on 89% of the plots in our test set.", "result_label"]]]
[0, [["Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes.", "background_label"], ["As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning.", "background_label"], ["Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer.", "method_label"], ["Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE.", "method_label"], ["Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.", "result_label"]]]
[0, [["This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection.", "objective_label"], ["Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks.", "objective_label"], ["Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy.", "method_label"], ["Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012.", "method_label"], ["Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate.", "method_label"], ["Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.", "other_label"]]]
[0, [["This work aims to address the problem of image-based question-answering (QA) with new models and datasets.", "objective_label"], ["In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images.", "objective_label"], ["Our model performs 1.8 times better than the only published results on an existing image QA dataset.", "method_label"], ["We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form.", "method_label"], ["We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers.", "method_label"], ["A suite of baseline results on this new dataset are also presented.", "result_label"]]]
[0, [["Siamese approaches address the visual tracking problem by extracting an appearance template from the current frame, which is used to localize the target in the next frame.", "background_label"], ["In general, this template is linearly combined with the accumulated template from the previous frame, resulting in an exponential decay of information over time.", "background_label"], ["While such an approach to updating has led to improved results, its simplicity limits the potential gain likely to be obtained by learning to update.", "background_label"], ["Therefore, we propose to replace the handcrafted update function with a method which learns to update.", "method_label"], ["We use a convolutional neural network, called UpdateNet, which given the initial template, the accumulated template and the template of the current frame aims to estimate the optimal template for the next frame.", "method_label"], ["The UpdateNet is compact and can easily be integrated into existing Siamese trackers.", "method_label"], ["We demonstrate the generality of the proposed approach by applying it to two Siamese trackers, SiamFC and DaSiamRPN.", "method_label"], ["Extensive experiments on VOT2016, VOT2018, LaSOT, and TrackingNet datasets demonstrate that our UpdateNet effectively predicts the new target template, outperforming the standard linear update.", "result_label"], ["On the large-scale TrackingNet dataset, our UpdateNet improves the results of DaSiamRPN with an absolute gain of 3.9% in terms of success score.", "result_label"]]]
[0, [["Correlation filters take advantage of specific properties in the Fourier domain allowing them to be estimated efficiently: O(NDlogD) in the frequency domain, versus O(D^3 + ND^2) spatially where D is signal length, and N is the number of signals.", "background_label"], ["Recent extensions to correlation filters, such as MOSSE, have reignited interest of their use in the vision community due to their robustness and attractive computational properties.", "background_label"], ["In this paper we demonstrate, however, that this computational efficiency comes at a cost.", "objective_label"], ["Specifically, we demonstrate that only 1/D proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance.", "method_label"], ["In this paper, we propose a novel approach to correlation filter estimation that: (i) takes advantage of inherent computational redundancies in the frequency domain, and (ii) dramatically reduces boundary effects.", "method_label"], ["Impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency.", "result_label"]]]
[0, [["In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking.", "background_label"], ["However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded.", "background_label"], ["Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting.", "background_label"], ["In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance.", "objective_label"], ["We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity.", "method_label"], ["We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor.", "method_label"], ["When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method in the VOT2016 challenge.", "result_label"], ["Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.", "result_label"]]]
[0, [["Template-matching methods for visual tracking have gained popularity recently due to their comparable performance and fast speed.", "background_label"], ["However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art.", "background_label"], ["In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking.", "objective_label"], ["An LSTM is used as a memory controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the memory block.", "method_label"], ["As the location of the target is at first unknown in the search feature map, an attention mechanism is applied to concentrate the LSTM input on the potential target.", "method_label"], ["To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template.", "method_label"], ["Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory.", "method_label"], ["Moreover, unlike other tracking methods where the model capacity is fixed after offline training --- the capacity of our tracker can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information.", "method_label"], ["Extensive experiments on OTB and VOT demonstrates that our tracker MemTrack performs favorably against state-of-the-art tracking methods while retaining real-time speed of 50 fps.", "result_label"]]]
[0, [["Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance.", "background_label"], ["Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training.", "background_label"], ["We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps.", "method_label"], ["Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications.", "method_label"], ["Our tracker uses a simple feed-forward network with no online training required.", "method_label"], ["The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set.", "method_label"], ["We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance.", "method_label"], ["Further, our performance improves as we add more videos to our offline training set.", "result_label"], ["To the best of our knowledge, our tracker is the first neural-network tracker that learns to track generic objects at 100 fps.", "result_label"]]]
[0, [["Robust and accurate visual tracking is one of the most challenging computer vision problems.", "background_label"], ["Due to the inherent lack of training data, a robust approach for constructing a target appearance model is crucial.", "background_label"], ["Recently, discriminatively learned correlation filters (DCF) have been successfully applied to address this problem for tracking.", "background_label"], ["These methods utilize a periodic assumption of the training samples to efficiently learn a classifier on all patches in the target neighborhood.", "method_label"], ["However, the periodic assumption also introduces unwanted boundary effects, which severely degrade the quality of the tracking model.", "method_label"], ["We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking.", "method_label"], ["A spatial regularization component is introduced in the learning to penalize correlation filter coefficients depending on their spatial location.", "background_label"], ["Our SRDCF formulation allows the correlation filters to be learned on a significantly larger set of negative training samples, without corrupting the positive samples.", "background_label"], ["We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efficient online learning of our SRDCF.", "method_label"], ["Experiments are performed on four benchmark datasets: OTB-2013, ALOV++, OTB-2015, and VOT2014.", "method_label"], ["Our approach achieves state-of-the-art results on all four datasets.", "result_label"], ["On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and 8.2% respectively, in mean overlap precision, compared to the best existing trackers.", "result_label"]]]
[0, [["Learning continuous-time dynamics on complex networks is crucial for understanding, predicting and controlling complex systems in science and engineering.", "background_label"], ["However, this task is very challenging due to the combinatorial complexities in the structures of high dimensional systems, their elusive continuous-time nonlinear dynamics, and their structural-dynamic dependencies.", "background_label"], ["To address these challenges, we propose to combine Ordinary Differential Equation Systems (ODEs) and Graph Neural Networks (GNNs) to learn continuous-time dynamics on complex networks in a data-driven manner.", "objective_label"], ["We model differential equation systems by GNNs.", "method_label"], ["Instead of mapping through a discrete number of neural layers in the forward process, we integrate GNN layers over continuous time numerically, leading to capturing continuous-time dynamics on graphs.", "method_label"], ["Our model can be interpreted as a Continuous-time GNN model or a Graph Neural ODEs model.", "method_label"], ["Our model can be utilized for continuous-time network dynamics prediction, structured sequence prediction (a regularly-sampled case), and node semi-supervised classification tasks (a one-snapshot case) in a unified framework.", "method_label"], ["We validate our model by extensive experiments in the above three scenarios.", "result_label"], ["The promising experimental results demonstrate our model's capability of jointly capturing the structure and dynamics of complex systems in a unified framework.", "result_label"]]]
[0, [["We present a numerical framework for approximating unknown governing equations using observation data and deep neural networks (DNN).", "background_label"], ["In particular, we propose to use residual network (ResNet) as the basic building block for equation approximation.", "objective_label"], ["We demonstrate that the ResNet block can be considered as a one-step method that is exact in temporal integration.", "method_label"], ["We then present two multi-step methods, recurrent ResNet (RT-ResNet) method and recursive ReNet (RS-ResNet) method.", "method_label"], ["The RT-ResNet is a multi-step method on uniform time steps, whereas the RS-ResNet is an adaptive multi-step method using variable time steps.", "method_label"], ["All three methods presented here are based on integral form of the underlying dynamical system.", "method_label"], ["As a result, they do not require time derivative data for equation recovery and can cope with relatively coarsely distributed trajectory data.", "result_label"], ["Several numerical examples are presented to demonstrate the performance of the methods.", "result_label"]]]
[0, [["Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use ad hoc gating mechanisms.", "background_label"], ["Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues.", "background_label"], ["We prove that learnable gates in a recurrent model formally provide quasi- invariance to general time transformations in the input data.", "method_label"], ["We recover part of the LSTM architecture from a simple axiomatic approach.", "method_label"], ["This result leads to a new way of initializing gate biases in LSTMs and GRUs.", "result_label"], ["Ex- perimentally, this new chrono initialization is shown to greatly improve learning of long term dependencies, with minimal implementation effort.", "result_label"]]]
[0, [["Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks.", "background_label"], ["In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs.", "background_label"], ["Posing image processing problems in the infinite dimensional setting provides powerful tools for their analysis and solution.", "background_label"], ["Over the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction.", "background_label"], ["In this paper, we establish a new PDE-interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data.", "objective_label"], ["Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges.", "method_label"], ["Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning.", "result_label"], ["Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs.", "method_label"], ["We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments.", "result_label"]]]
[0, [["We introduce a new family of deep neural network models.", "background_label"], ["Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network.", "method_label"], ["The output of the network is computed using a black-box differential equation solver.", "method_label"], ["These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed.", "method_label"], ["We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models.", "method_label"], ["We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions.", "method_label"], ["For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations.", "result_label"], ["This allows end-to-end training of ODEs within larger models.", "result_label"]]]
[0, [["We consider recent work of Haber and Ruthotto 2017 and Chang et al.", "background_label"], ["2018, where deep learning neural networks have been interpreted as discretisations of an optimal control problem subject to an ordinary differential equation constraint.", "background_label"], ["We review the first order conditions for optimality, and the conditions ensuring optimality after discretisation.", "method_label"], ["This leads to a class of algorithms for solving the discrete optimal control problem which guarantee that the corresponding discrete necessary conditions for optimality are fulfilled.", "method_label"], ["The differential equation setting lends itself to learning additional parameters such as the time discretisation.", "method_label"], ["We explore this extension alongside natural constraints (e.g.", "method_label"], ["time steps lie in a simplex).", "method_label"], ["We compare these deep learning algorithms numerically in terms of induced flow and generalisation ability.", "result_label"]]]
[0, [["Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding.", "background_label"], ["The data in these tasks are typically represented in the Euclidean space.", "background_label"], ["However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects.", "background_label"], ["The complexity of graph data has imposed significant challenges on existing machine learning algorithms.", "background_label"], ["Recently, many studies on extending deep learning approaches for graph data have emerged.", "background_label"], ["In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields.", "objective_label"], ["We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks.", "method_label"], ["We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks.", "method_label"], ["Finally, we propose potential research directions in this rapidly growing field.", "result_label"]]]
[0, [["We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent.", "background_label"], ["To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.", "method_label"]]]
[0, [["Most deep latent factor models choose simple priors for simplicity, tractability or not knowing what prior to use.", "background_label"], ["Recent studies show that the choice of the prior may have a profound effect on the expressiveness of the model,especially when its generative network has limited capacity.", "background_label"], ["In this paper, we propose to learn a proper prior from data for adversarial autoencoders(AAEs).", "objective_label"], ["We introduce the notion of code generators to transform manually selected simple priors into ones that can better characterize the data distribution.", "method_label"], ["Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than AAEs in both supervised and unsupervised settings.", "result_label"], ["Lastly, we present its ability to do cross-domain translation in a text-to-image synthesis task.", "result_label"]]]
[0, [["Many different methods to train deep generative models have been introduced in the past.", "background_label"], ["In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call \"Variational Mixture of Posteriors\"prior, or VampPrior for short.", "objective_label"], ["The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs.", "method_label"], ["We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models.", "method_label"], ["The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs.", "method_label"], ["We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.", "result_label"]]]
[0, [["In this paper, we propose the \"adversarial autoencoder\"(AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution.", "method_label"], ["Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples.", "method_label"], ["As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution.", "method_label"], ["We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization.", "method_label"], ["We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.", "result_label"]]]
[0, [["We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed.", "background_label"], ["We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage.", "method_label"], ["Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans.", "method_label"], ["Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.", "result_label"]]]
[0, [["To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences.", "background_label"], ["One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge.", "background_label"], ["To help address this concern, we propose training agents via self play on a zero sum debate game.", "method_label"], ["Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information.", "method_label"], ["In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions).", "method_label"], ["In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment.", "method_label"], ["We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels.", "result_label"], ["Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.", "result_label"]]]
[0, [["While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability.", "background_label"], ["In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words.", "objective_label"], ["We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision.", "method_label"], ["In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.", "result_label"]]]
[0, [["Prediction without justification has limited applicability.", "background_label"], ["As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction.", "background_label"], ["Our approach combines two modular components, generator and encoder, which are trained to operate well together.", "method_label"], ["The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction.", "method_label"], ["Rationales are never given during training.", "method_label"], ["Instead, the model is regularized by desiderata for rationales.", "method_label"], ["We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases.", "result_label"], ["Our approach outperforms attention-based baseline by a significant margin.", "result_label"], ["We also successfully illustrate the method on the question retrieval task.", "result_label"]]]
[0, [["Real-time scene parsing is a fundamental feature for autonomous driving vehicles with multiple cameras.", "background_label"], ["In this letter we demonstrate that sharing semantics between cameras with different perspectives and overlapped views can boost the parsing performance when compared with traditional methods, which individually process the frames from each camera.", "objective_label"], ["Our framework is based on a deep neural network for semantic segmentation but with two kinds of additional modules for sharing and fusing semantics.", "method_label"], ["On the one hand, a semantics sharing module is designed to establish the pixel-wise mapping between the input images.", "method_label"], ["Features as well as semantics are shared by the map to reduce duplicated workload which leads to more efficient computation.", "method_label"], ["On the other hand, feature fusion modules are designed to combine different modal of semantic features, which leverage the information from both inputs for better accuracy.", "method_label"], ["To evaluate the effectiveness of the proposed framework, we have applied our network to a dual-camera vision system for driving scene parsing.", "method_label"], ["Experimental results show that our network outperforms the baseline method on the parsing accuracy with comparable computations.", "result_label"]]]
[0, [["Scene parsing is challenging for unrestricted open vocabulary and diverse scenes.", "background_label"], ["In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet).", "background_label"], ["Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks.", "method_label"], ["The proposed approach achieves state-of-the-art performance on various datasets.", "method_label"], ["It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark.", "method_label"], ["A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.", "result_label"]]]
[0, [["In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efficient semantic video segmentation.", "background_label"], ["DVSNet consists of two convolutional neural networks: a segmentation network and a flow network.", "background_label"], ["The former generates highly accurate semantic segmentations, but is deeper and slower.", "background_label"], ["The latter is much faster than the former, but its output requires further processing to generate less accurate semantic segmentations.", "method_label"], ["We explore the use of a decision network to adaptively assign different frame regions to different networks based on a metric called expected confidence score.", "method_label"], ["Frame regions with a higher expected confidence score traverse the flow network.", "background_label"], ["Frame regions with a lower expected confidence score have to pass through the segmentation network.", "background_label"], ["We have extensively performed experiments on various configurations of DVSNet, and investigated a number of variants for the proposed decision network.", "method_label"], ["The experimental results show that our DVSNet is able to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset.", "result_label"], ["A high speed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on the same dataset.", "result_label"], ["DVSNet is also able to reduce up to 95% of the computational workloads.", "result_label"]]]
[0, [["We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design.", "background_label"], ["MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances.", "background_label"], ["This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art.", "objective_label"], ["Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases.", "method_label"], ["These models are then adapted and applied to the tasks of object detection and semantic segmentation.", "method_label"], ["For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP).", "objective_label"], ["We achieve new state of the art results for mobile classification, detection and segmentation.", "method_label"], ["MobileNetV3-Large is 3.2\\% more accurate on ImageNet classification while reducing latency by 15\\% compared to MobileNetV2.", "method_label"], ["MobileNetV3-Small is 4.6\\% more accurate while reducing latency by 5\\% compared to MobileNetV2.", "result_label"], ["MobileNetV3-Large detection is 25\\% faster at roughly the same accuracy as MobileNetV2 on COCO detection.", "result_label"], ["MobileNetV3-Large LR-ASPP is 30\\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.", "result_label"]]]
[0, [["In this work, we address the challenging video scene parsing problem by developing effective representation learning methods given limited parsing annotations.", "background_label"], ["In particular, we contribute two novel methods that constitute a unified parsing framework.", "method_label"], ["(1) \\textbf{Predictive feature learning}} from nearly unlimited unlabeled video data.", "method_label"], ["Different from existing methods learning features from single frame parsing, we learn spatiotemporal discriminative features by enforcing a parsing network to predict future frames and their parsing maps (if available) given only historical frames.", "method_label"], ["In this way, the network can effectively learn to capture video dynamics and temporal context, which are critical clues for video scene parsing, without requiring extra manual annotations.", "method_label"], ["(2) \\textbf{Prediction steering parsing}} architecture that effectively adapts the learned spatiotemporal features to scene parsing tasks and provides strong guidance for any off-the-shelf parsing model to achieve better video scene parsing performance.", "method_label"], ["Extensive experiments over two challenging datasets, Cityscapes and Camvid, have demonstrated the effectiveness of our methods by showing significant improvement over well-established baselines.", "result_label"]]]
[0, [["As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users.", "background_label"], ["Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments.", "background_label"], ["In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results.", "method_label"], ["In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game.", "method_label"], ["Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods.", "result_label"], ["The source code and datasets have been published.", "result_label"]]]
[0, [["We present SetExpander, a corpus-based system for expanding a seed set of terms into amore complete set of terms that belong to the same semantic class.", "background_label"], ["SetExpander implements an iterative end-to-end workflow.", "background_label"], ["It enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes.SetExpander has been used successfully in real-life use cases including integration into an automated recruitment system and an issues and defects resolution system.", "method_label"], ["A video demo of SetExpander is available at https://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv (some images were blurred for privacy reasons)", "other_label"]]]
[0, [["In this paper, we propose a Multiple Human Tracking method using multi-cues including Primitive Action Features (MHT-PAF).", "objective_label"], ["MHT-PAF can perform the accurate human tracking in dynamic aerial videos captured by a drone.", "method_label"], ["PAF employs a global context, rich information by multi-label actions, and a middle level feature.", "method_label"], ["The accurate human tracking result using PAF helps multi-frame-based action recognition.", "method_label"], ["In the experiments, we verified the effectiveness of the proposed method using the Okutama-Action dataset.", "result_label"], ["Our code is available online.", "result_label"]]]
[0, [["We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset.", "background_label"], ["Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks.", "result_label"], ["In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets.", "result_label"], ["Finally, they are conceptually very simple and easy to train and use.", "result_label"]]]
[0, [["Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances.", "background_label"], ["This problem requires not only recognizing action categories but also detecting start time and end time of each action instance.", "background_label"], ["Many state-of-the-art methods adopt the \"detection by classification\"framework: first do proposal, and then classify proposals.", "background_label"], ["The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step.", "background_label"], ["To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video.", "method_label"], ["On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted.", "method_label"], ["Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy.", "method_label"], ["We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2.", "method_label"], ["When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.", "result_label"]]]
[0, [["Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms.", "background_label"], ["In this paper, we integrate appearance information to improve the performance of SORT.", "background_label"], ["Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches.", "method_label"], ["In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset.", "method_label"], ["During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space.", "method_label"], ["Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.", "result_label"]]]
[0, [["A main issue preventing the use of Convolutional Neural Networks (CNN) in end user applications is the low level of transparency in the decision process.", "background_label"], ["Previous work on CNN interpretability has mostly focused either on localizing the regions of the image that contribute to the result or on building an external model that generates plausible explanations.", "background_label"], ["However, the former does not provide any semantic information and the latter does not guarantee the faithfulness of the explanation.", "background_label"], ["We propose an intermediate representation composed of multiple Semantically Interpretable Activation Maps (SIAM) indicating the presence of predefined attributes at different locations of the image.", "method_label"], ["These attribute maps are then linearly combined to produce the final output.", "method_label"], ["This gives the user insight into what the model has seen, where, and a final output directly linked to this information in a comprehensive and interpretable way.", "method_label"], ["We test the method on the task of landscape scenicness (aesthetic value) estimation, using an intermediate representation of 33 attributes from the SUN Attributes database.", "method_label"], ["The results confirm that SIAM makes it possible to understand what attributes in the image are contributing to the final score and where they are located.", "result_label"], ["Since it is based on learning from multiple tasks and datasets, SIAM improve the explanability of the prediction without additional annotation efforts or computational overhead at inference time, while keeping good performances on both the final and intermediate tasks.", "result_label"]]]
[0, [["Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via application-specific loss functions.", "background_label"], ["In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional.", "objective_label"], ["It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization.", "method_label"], ["Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN.", "method_label"], ["As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.", "result_label"]]]
[0, [["In this paper, we propose a novel single image action recognition algorithm which is based on the idea of semantic body part actions.", "objective_label"], ["Unlike existing bottom up methods, we argue that the human action is a combination of meaningful body part actions.", "background_label"], ["In detail, we divide human body into five parts: head, torso, arms, hands and legs.", "method_label"], ["And for each of the body parts, we define several semantic body part actions, e.g., hand holding, hand waving.", "method_label"], ["These semantic body part actions are strongly related to the body actions, e.g., writing, and jogging.", "method_label"], ["Based on the idea, we propose a deep neural network based system: first, body parts are localized by a Semi-FCN network.", "method_label"], ["Second, for each body parts, a Part Action Res-Net is used to predict semantic body part actions.", "method_label"], ["And finally, we use SVM to fuse the body part actions and predict the entire body action.", "method_label"], ["Experiments on two dataset: PASCAL VOC 2012 and Stanford-40 report mAP improvement from the state-of-the-art by 3.8% and 2.6% respectively.", "result_label"]]]
[0, [["This paper introduces a novel method for the representation of images that is semantic by nature, addressing the question of computation intelligibility in computer vision tasks.", "background_label"], ["More specifically, our proposition is to introduce what we call a semantic bottleneck in the processing pipeline, which is a crossing point in which the representation of the image is entirely expressed with natural language , while retaining the efficiency of numerical representations.", "objective_label"], ["We show that our approach is able to generate semantic representations that give state-of-the-art results on semantic content-based image retrieval and also perform very well on image classification tasks.", "result_label"], ["Intelligibility is evaluated through user centered experiments for failure detection.", "result_label"]]]
[0, [["This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions.", "background_label"], ["It is based on a tutorial given at ICASSP 2017.", "background_label"], ["It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data.", "method_label"], ["It also discusses a number of practical applications.", "method_label"]]]
[0, [["With too few samples or too many model parameters, overfitting can inhibit the ability to generalise predictions to new data.", "background_label"], ["Within medical imaging, this can occur when features are incorrectly assigned importance such as distinct hospital specific artifacts, leading to poor performance on a new dataset from a different institution without those features, which is undesirable.", "background_label"], ["Most regularization methods do not explicitly penalize the incorrect association of these features to the target class and hence fail to address this issue.", "background_label"], ["We propose a regularization method, GradMask, which penalizes saliency maps inferred from the classifier gradients when they are not consistent with the lesion segmentation.", "method_label"], ["This prevents non-tumor related features to contribute to the classification of unhealthy samples.", "method_label"], ["We demonstrate that this method can improve test accuracy between 1-3% compared to the baseline without GradMask, showing that it has an impact on reducing overfitting.", "result_label"]]]
[0, [["In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels.", "background_label"], ["While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks.", "background_label"], ["Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach.", "result_label"], ["We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them", "result_label"]]]
[0, [["This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs.", "objective_label"], ["In an interpretable CNN, each filter in a high conv-layer represents a certain object part.", "method_label"], ["We do not need any annotations of object parts or textures to supervise the learning process.", "method_label"], ["Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process.", "method_label"], ["Our method can be applied to different types of CNNs with different structures.", "method_label"], ["The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision.", "method_label"], ["Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.", "result_label"]]]
[0, [["Semantic object parts can be useful for several visual recognition tasks.", "background_label"], ["Lately, these tasks have been addressed using Convolutional Neural Networks (CNN), achieving outstanding results.", "background_label"], ["In this work we study whether CNNs learn semantic parts in their internal representation.", "objective_label"], ["We investigate the responses of convolutional filters and try to associate their stimuli with semantic parts.", "method_label"], ["We perform two extensive quantitative analyses.", "method_label"], ["First, we use ground-truth part bounding-boxes from the PASCAL-Part dataset to determine how many of those semantic parts emerge in the CNN.", "result_label"], ["We explore this emergence for different layers, network depths, and supervision levels.", "background_label"], ["Second, we collect human judgements in order to study what fraction of all filters systematically fire on any semantic part, even if not annotated in PASCAL-Part.", "background_label"], ["Moreover, we explore several connections between discriminative power and semantics.", "method_label"], ["We find out which are the most discriminative filters for object recognition, and analyze whether they respond to semantic parts or to other image patches.", "method_label"], ["We also investigate the other direction: we determine which semantic parts are the most discriminative and whether they correspond to those parts emerging in the network.", "method_label"], ["This enables to gain an even deeper understanding of the role of semantic parts in the network.", "result_label"]]]
[0, [["A segmentation-based architecture is proposed to decompose objects into multiple primitive shapes from monocular depth input for robotic manipulation.", "background_label"], ["The backbone deep network is trained on synthetic data with 6 classes of primitive shapes generated by a simulation engine.", "method_label"], ["Each primitive shape is designed with parametrized grasp families, permitting the pipeline to identify multiple grasp candidates per shape primitive region.", "method_label"], ["The grasps are priority ordered via proposed ranking algorithm, with the first feasible one chosen for execution.", "method_label"], ["On task-free grasping of individual objects, the method achieves a 94% success rate.", "method_label"], ["On task-oriented grasping, it achieves a 76% success rate.", "method_label"], ["Overall, the method supports the hypothesis that shape primitives can support task-free and task-relevant grasp prediction.", "result_label"]]]
[0, [["Grasping skill is a major ability that a wide number of real-life applications require for robotisation.", "background_label"], ["State-of-the-art robotic grasping methods perform prediction of object grasp locations based on deep neural networks.", "background_label"], ["However, such networks require huge amount of labeled data for training making this approach often impracticable in robotics.", "background_label"], ["In this paper, we propose a method to generate a large scale synthetic dataset with ground truth, which we refer to as the Jacquard grasping dataset.", "method_label"], ["Jacquard is built on a subset of ShapeNet, a large CAD models dataset, and contains both RGB-D images and annotations of successful grasping positions based on grasp attempts performed in a simulated environment.", "method_label"], ["We carried out experiments using an off-the-shelf CNN, with three different evaluation metrics, including real grasping robot trials.", "method_label"], ["The results show that Jacquard enables much better generalization skills than a human labeled dataset thanks to its diversity of objects and grasping positions.", "result_label"], ["For the purpose of reproducible research in robotics, we are releasing along with the Jacquard dataset a web interface for researchers to evaluate the successfulness of their grasping position detections using our dataset.", "result_label"]]]
[0, [["We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects.", "background_label"], ["In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features.", "background_label"], ["This presents two main challenges.", "objective_label"], ["First, we need to evaluate a huge number of candidate grasps.", "method_label"], ["In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second.", "method_label"], ["The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps.", "method_label"], ["The second, with more features, is slower but has to run only on the top few detections.", "method_label"], ["Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization.", "method_label"], ["We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.", "result_label"]]]
[0, [["Robotic grasp detection task is still challenging, particularly for novel objects.", "background_label"], ["With the recent advance of deep learning, there have been several works on detecting robotic grasp using neural networks.", "background_label"], ["Typically, regression based grasp detection methods have outperformed classification based detection methods in computation complexity with excellent accuracy.", "background_label"], ["However, classification based robotic grasp detection still seems to have merits such as intermediate step observability and straightforward back propagation routine for end-to-end training.", "background_label"], ["In this work, we propose a novel classification based robotic grasp detection method with multiple-stage spatial transformer networks (STN).", "method_label"], ["Our proposed method was able to achieve state-of-the-art performance in accuracy with real- time computation.", "method_label"], ["Additionally, unlike other regression based grasp detection methods, our proposed method allows partial observation for intermediate results such as grasp location and orientation for a number of grasp configuration candidates.", "result_label"]]]
[0, [["Human beings are particularly good at reasoning and inference from just a few examples.", "background_label"], ["When facing new tasks, humans will leverage knowledge and skills learned before, and quickly integrate them with the new task.", "background_label"], ["In addition to learning by experimentation, human also learn socio-culturally through instructions and learning by example.", "background_label"], ["In this way humans can learn much faster compared with most current artificial intelligence algorithms in many tasks.", "background_label"], ["In this paper, we test the idea of speeding up machine learning through social learning.", "objective_label"], ["We argue that in solving real-world problems, especially when the task is designed by humans, and/or for humans, there are typically instructions from user manuals and/or human experts which give guidelines on how to better accomplish the tasks.", "method_label"], ["We argue that these instructions have tremendous value in designing a reinforcement learning system which can learn in human fashion, and we test the idea by playing the Atari games Tennis and Pong.", "method_label"], ["We experimentally demonstrate that the instructions provide key information about the task, which can be used to decompose the learning task into sub-systems and construct options for the temporally extended planning, and dramatically accelerate the learning process.", "result_label"]]]
[0, [["Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning.", "background_label"], ["One popular approach for addressing this challenge is the options framework (Sutton et al., 1999).", "background_label"], ["However, only recently in (Bacon et al., 2017) was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion.", "background_label"], ["In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time.", "objective_label"], ["We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time.", "method_label"], ["We extend results from (Bacon et al., 2017) and derive policy gradient theorems for a deep hierarchy of options.", "method_label"], ["Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals.", "method_label"], ["Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.", "result_label"]]]
[0, [["Deep reinforcement learning has achieved many impressive results in recent years.", "background_label"], ["However, tasks with sparse rewards or long horizons continue to pose significant challenges.", "background_label"], ["To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks.", "objective_label"], ["Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks.", "method_label"], ["Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks.", "method_label"], ["To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer.", "method_label"], ["Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.", "result_label"]]]
[0, [["We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning.", "background_label"], ["Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time.", "method_label"], ["Our framework employs a Manager module and a Worker module.", "method_label"], ["The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker.", "method_label"], ["The Worker generates primitive actions at every tick of the environment.", "method_label"], ["The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager.", "method_label"], ["These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.", "method_label"], ["We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.", "result_label"]]]
[0, [["We study how to effectively leverage expert feedback to learn sequential decision-making policies.", "background_label"], ["We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning.", "background_label"], ["We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction.", "method_label"], ["Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration.", "method_label"], ["Using long-horizon benchmarks, including Montezuma's Revenge, we demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL.", "result_label"], ["We also theoretically analyze labeling cost for certain instantiations of our framework.", "result_label"]]]
[0, [["Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward.", "background_label"], ["However, environments contain a much wider variety of possible training signals.", "background_label"], ["In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning.", "background_label"], ["All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards.", "method_label"], ["We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task.", "method_label"], ["Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth.", "result_label"]]]
[0, [["Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL).", "background_label"], ["Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs.", "background_label"], ["In this paper we address the option discovery problem by showing how PVFs implicitly define options.", "objective_label"], ["We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations.", "method_label"], ["The options discovered from eigenpurposes traverse the principal directions of the state space.", "method_label"], ["They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration.", "method_label"], ["Moreover, different options act at different time scales, making them helpful for exploration.", "method_label"], ["We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.", "result_label"]]]
[0, [["We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards.", "background_label"], ["Recent approaches have shown great successes in many Atari 2600 domains.", "background_label"], ["However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods.", "background_label"], ["Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems.", "background_label"], ["We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction.", "method_label"], ["We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains.", "method_label"], ["Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al.", "result_label"], ["2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.", "result_label"]]]
[0, [["This paper describes a purely data-driven solution to a class of sequential decision-making problems with a large number of concurrent online decisions, with applications to computing systems and operations research.", "background_label"], ["We assume that while the micro-level behaviour of the system can be broadly captured by analytical expressions or simulation, the macro-level or emergent behaviour is complicated by non-linearity, constraints, and stochasticity.", "background_label"], ["If we represent the set of concurrent decisions to be computed as a vector, each element of the vector is assumed to be a continuous variable, and the number of such elements is arbitrarily large and variable from one problem instance to another.", "method_label"], ["We first formulate the decision-making problem as a canonical reinforcement learning (RL) problem, which can be solved using purely data-driven techniques.", "method_label"], ["We modify a standard approach known as advantage actor critic (A2C) to ensure its suitability to the problem at hand, and compare its performance to that of baseline approaches on the specific instance of a multi-product inventory management task.", "method_label"], ["The key modifications include a parallelised formulation of the decision-making task, and a training procedure that explicitly recognises the quantitative relationship between different decisions.", "method_label"], ["We also present experimental results probing the learned policies, and their robustness to variations in the data.", "result_label"]]]
[0, [["Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori.", "background_label"], ["We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions.", "method_label"], ["These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken.", "method_label"], ["We provide an algorithm to both learn and use action representations and provide conditions for its convergence.", "method_label"], ["The efficacy of the proposed method is demonstrated on large-scale real-world problems.", "result_label"]]]
[0, [["The popular Q-learning algorithm is known to overestimate action values under certain conditions.", "background_label"], ["It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented.", "background_label"], ["In this paper, we answer all these questions affirmatively.", "objective_label"], ["In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain.", "method_label"], ["We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation.", "method_label"], ["We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.", "result_label"]]]
[0, [["Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL).", "background_label"], ["Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance.", "background_label"], ["However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality.", "background_label"], ["This puts many real-world tasks out of practical reach of RL methods.", "background_label"], ["In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm.", "method_label"], ["Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks.", "method_label"], ["It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations.", "method_label"], ["Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.", "method_label"]]]
[0, [["We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\"objective function using stochastic gradient ascent.", "background_label"], ["Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates.", "method_label"], ["The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically).", "method_label"], ["Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.", "result_label"]]]
[0, [["In this work we describe a novel deep reinforcement learning architecture that allows multiple actions to be selected at every time-step in an efficient manner.", "background_label"], ["Multi-action policies allow complex behaviours to be learnt that would otherwise be hard to achieve when using single action selection techniques.", "background_label"], ["We use both imitation learning and temporal difference (TD) reinforcement learning (RL) to provide a 4x improvement in training time and 2.5x improvement in performance over single action selection TD RL.", "method_label"], ["We demonstrate the capabilities of this network using a complex in-house 3D game.", "result_label"], ["Mimicking the behavior of the expert teacher significantly improves world state exploration and allows the agents vision system to be trained more rapidly than TD RL alone.", "result_label"], ["This initial training technique kick-starts TD learning and the agent quickly learns to surpass the capabilities of the expert.", "result_label"]]]
[0, [["Discrete-action algorithms have been central to numerous recent successes of deep reinforcement learning.", "background_label"], ["However, applying these algorithms to high-dimensional action tasks requires tackling the combinatorial increase of the number of possible actions with the number of action dimensions.", "background_label"], ["This problem is further exacerbated for continuous-action tasks that require fine control of actions via discretization.", "background_label"], ["In this paper, we propose a novel neural architecture featuring a shared decision module followed by several network branches, one for each action dimension.", "objective_label"], ["This approach achieves a linear increase of the number of network outputs with the number of degrees of freedom by allowing a level of independence for each individual action dimension.", "method_label"], ["To illustrate the approach, we present a novel agent, called Branching Dueling Q-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network (Dueling DDQN).", "method_label"], ["We evaluate the performance of our agent on a set of challenging continuous control tasks.", "method_label"], ["The empirical results show that the proposed agent scales gracefully to environments with increasing action dimensionality and indicate the significance of the shared decision module in coordination of the distributed action branches.", "result_label"], ["Furthermore, we show that the proposed agent performs competitively against a state-of-the-art continuous control algorithm, Deep Deterministic Policy Gradient (DDPG).", "result_label"]]]
[0, [["Translucency is prevalent in everyday scenes.", "background_label"], ["As such, perception of transparent objects is essential for robots to perform manipulation.", "background_label"], ["Compared with texture-rich or texture-less Lambertian objects, transparency induces significant uncertainty on object appearances.", "background_label"], ["Ambiguity can be due to changes in lighting, viewpoint, and backgrounds, each of which brings challenges to existing object pose estimation algorithms.", "background_label"], ["In this work, we propose LIT, a two-stage method for transparent object pose estimation using light-field sensing and photorealistic rendering.", "method_label"], ["LIT employs multiple filters specific to light-field imagery in deep networks to capture transparent material properties, with robust depth and pose estimators based on generative sampling.", "method_label"], ["Along with the LIT algorithm, we introduce the light-field transparent object dataset ProLIT for the tasks of recognition, localization and pose estimation.", "method_label"], ["With respect to this ProLIT dataset, we demonstrate that LIT can outperform both state-of-the-art end-to-end pose estimation methods and a generative pose estimator on transparent objects.", "result_label"]]]
[0, [["Object pose estimation is frequently achieved by first segmenting an RGB image and then, given depth data, registering the corresponding point cloud segment against the object's 3D model.", "background_label"], ["Despite the progress due to CNNs, semantic segmentation output can be noisy, especially when the CNN is only trained on synthetic data.", "background_label"], ["This causes registration methods to fail in estimating a good object pose.", "background_label"], ["This work proposes a novel stochastic optimization process that treats the segmentation output of CNNs as a confidence probability.", "objective_label"], ["The algorithm, called Stochastic Congruent Sets (StoCS), samples pointsets on the point cloud according to the soft segmentation distribution and so as to agree with the object's known geometry.", "method_label"], ["The pointsets are then matched to congruent sets on the 3D object model to generate pose estimates.", "method_label"], ["StoCS is shown to be robust on an APC dataset, despite the fact the CNN is trained only on synthetic data.", "result_label"], ["In the YCB dataset, StoCS outperforms a recent network for 6D pose estimation and alternative pointset matching techniques.", "result_label"]]]
[0, [["In order to perform autonomous sequential manipulation tasks, perception in cluttered scenes remains a critical challenge for robots.", "background_label"], ["In this paper, we propose a probabilistic approach for robust sequential scene estimation and manipulation - Sequential Scene Understanding and Manipulation(SUM).", "objective_label"], ["SUM considers uncertainty due to discriminative object detection and recognition in the generative estimation of the most likely object poses maintained over time to achieve a robust estimation of the scene under heavy occlusions and unstructured environment.", "method_label"], ["Our method utilizes candidates from discriminative object detector and recognizer to guide the generative process of sampling scene hypothesis, and each scene hypotheses is evaluated against the observations.", "method_label"], ["Also SUM maintains beliefs of scene hypothesis over robot physical actions for better estimation and against noisy detections.", "method_label"], ["We conduct extensive experiments to show that our approach is able to perform robust estimation and manipulation.", "result_label"]]]
[0, [["Transparent objects are prevalent across many environments of interest for dexterous robotic manipulation.", "background_label"], ["Such transparent material leads to considerable uncertainty for robot perception and manipulation, and remains an open challenge for robotics.", "background_label"], ["This problem is exacerbated when multiple transparent objects cluster into piles of clutter.", "background_label"], ["In household environments, for example, it is common to encounter piles of glassware in kitchens, dining rooms, and reception areas, which are essentially invisible to modern robots.", "background_label"], ["We present the GlassLoc algorithm for grasp pose detection of transparent objects in transparent clutter using plenoptic sensing.", "method_label"], ["GlassLoc classifies graspable locations in space informed by a Depth Likelihood Volume (DLV) descriptor.", "method_label"], ["We extend the DLV to infer the occupancy of transparent objects over a given space from multiple plenoptic viewpoints.", "method_label"], ["We demonstrate and evaluate the GlassLoc algorithm on a Michigan Progress Fetch mounted with a first-generation Lytro.", "result_label"], ["The effectiveness of our algorithm is evaluated through experiments for grasp detection and execution with a variety of transparent glassware in minor clutter.", "result_label"]]]
[0, [["A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources.", "background_label"], ["Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications.", "background_label"], ["In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images.", "objective_label"], ["DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated.", "method_label"], ["Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference.", "method_label"], ["Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD.", "result_label"], ["We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.", "result_label"]]]
[0, [["We propose a real-time RGB-based pipeline for object detection and 6D pose estimation.", "objective_label"], ["Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization.", "method_label"], ["This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries.", "method_label"], ["Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space.", "method_label"], ["Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain.", "result_label"], ["We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches.", "result_label"], ["We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results.", "result_label"]]]
[0, [["Estimating the 6D pose of objects from images is an important problem in various applications such as robot manipulation and virtual reality.", "background_label"], ["While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the observed image can produce accurate results.", "background_label"], ["In this work, we propose a novel deep neural network for 6D pose matching named DeepIM.", "objective_label"], ["Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image.", "method_label"], ["The network is trained to predict a relative pose transformation using an untangled representation of 3D location and 3D orientation and an iterative training process.", "method_label"], ["Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods.", "result_label"], ["We furthermore show that DeepIM is able to match previously unseen objects.", "result_label"]]]
[0, [["Data loading can dominate deep neural network training time on large-scale systems.", "background_label"], ["We present a comprehensive study on accelerating data loading performance in large-scale distributed training.", "objective_label"], ["We first identify performance and scalability issues in current data loading implementations.", "method_label"], ["We then propose optimizations that utilize CPU resources to the data loader design.", "method_label"], ["We use an analytical model to characterize the impact of data loading on the overall training time and establish the performance trend as we scale up distributed training.", "method_label"], ["Our model suggests that I/O rate limits the scalability of distributed training, which inspires us to design a locality-aware data loading method.", "method_label"], ["By utilizing software caches, our method can drastically reduce the data loading communication volume in comparison with the original data loading implementation.", "method_label"], ["Finally, we evaluate the proposed optimizations with various experiments.", "result_label"], ["We achieved more than 30x speedup in data loading using 256 nodes with 1,024 learners.", "result_label"]]]
[0, [["Deep learning is extremely computationally intensive, and hardware vendors have responded by building faster accelerators in large clusters.", "background_label"], ["Training deep learning models at petaFLOPS scale requires overcoming both algorithmic and systems software challenges.", "background_label"], ["In this paper, we discuss three systems-related optimizations: (1) distributed batch normalization to control per-replica batch sizes, (2) input pipeline optimizations to sustain model throughput, and (3) 2-D torus all-reduce to speed up gradient summation.", "objective_label"], ["We combine these optimizations to train ResNet-50 on ImageNet to 76.3% accuracy in 2.2 minutes on a 1024-chip TPU v3 Pod with a training throughput of over 1.05 million images/second and no accuracy drop.", "result_label"]]]
[0, [["We extract pixel-level masks of extreme weather patterns using variants of Tiramisu and DeepLabv3+ neural networks.", "background_label"], ["We describe improvements to the software frameworks, input pipeline, and the network training algorithms necessary to efficiently scale deep learning on the Piz Daint and Summit systems.", "method_label"], ["The Tiramisu network scales to 5300 P100 GPUs with a sustained throughput of 21.0 PF/s and parallel efficiency of 79.0%.", "method_label"], ["DeepLabv3+ scales up to 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel efficiency of 90.7% in single precision.", "method_label"], ["By taking advantage of the FP16 Tensor Cores, a half-precision version of the DeepLabv3+ network achieves a peak and sustained throughput of 1.13 EF/s and 999.0 PF/s respectively.", "result_label"]]]
[0, [["Recently, researches have explored the graph neural network (GNN) techniques on text classification, since GNN does well in handling complex structures and preserving global information.", "background_label"], ["However, previous methods based on GNN are mainly faced with the practical problems of fixed corpus level graph structure which do not support online testing and high memory consumption.", "background_label"], ["To tackle the problems, we propose a new GNN based model that builds graphs for each input text with global parameters sharing instead of a single graph for the whole corpus.", "method_label"], ["This method removes the burden of dependence between an individual text and entire corpus which support online testing, but still preserve global information.", "method_label"], ["Besides, we build graphs by much smaller windows in the text, which not only extract more local features but also significantly reduce the edge numbers as well as memory consumption.", "method_label"], ["Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory.", "result_label"]]]
[0, [["The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error.", "background_label"], ["Can we automate this challenging, tedious process, and learn the algorithms instead?", "background_label"], ["In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data.", "background_label"], ["This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems.", "background_label"], ["In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge.", "objective_label"], ["The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution.", "method_label"], ["We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.", "result_label"]]]
[0, [["Presence of bias (in datasets or tasks) is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in recent years.", "background_label"], ["Such challenges range from spurious associations between variables in medical studies to the bias of race in gender or face recognition systems.", "background_label"], ["Controlling for all types of biases in the dataset curation stage is cumbersome and sometimes impossible.", "background_label"], ["The alternative is to use the available data and build models incorporating fair representation learning.", "objective_label"], ["In this paper, we propose such a model based on adversarial training with two competing objectives to learn features that have (1) maximum discriminative power with respect to the task and (2) minimal statistical mean dependence with the protected (bias) variable(s).", "objective_label"], ["Our approach does so by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and the learned features.", "method_label"], ["We apply our method to synthetic data, medical images (containing task bias), and a dataset for gender classification (containing dataset bias).", "method_label"], ["Our results show that the learned features by our method not only result in superior prediction performance but also are unbiased.", "result_label"], ["The code is available at https://github.com/QingyuZhao/BR-Net/.", "result_label"]]]
[0, [["Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time.", "background_label"], ["Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect.", "background_label"], ["We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest.", "method_label"], ["We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not.", "method_label"], ["We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior.", "method_label"], ["In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably.", "method_label"], ["Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.", "result_label"]]]
[0, [["Recent methods for video action recognition have reached outstanding performances on existing benchmarks.", "background_label"], ["However, they tend to leverage context such as scenes or objects instead of focusing on understanding the human action itself.", "background_label"], ["For instance, a tennis field leads to the prediction playing tennis irrespectively of the actions performed in the video.", "background_label"], ["In contrast, humans have a more complete understanding of actions and can recognize them without context.", "background_label"], ["The best example of out-of-context actions are mimes, that people can typically recognize despite missing relevant objects and scenes.", "background_label"], ["In this paper, we propose to benchmark action recognition methods in such absence of context and introduce a novel dataset, Mimetics, consisting of mimed actions for a subset of 50 classes from the Kinetics benchmark.", "method_label"], ["Our experiments show that (a) state-of-the-art 3D convolutional neural networks obtain disappointing results on such videos, highlighting the lack of true understanding of the human actions and (b) models leveraging body language via human pose are less prone to context biases.", "result_label"], ["In particular, we show that applying a shallow neural network with a single temporal convolution over body pose features transferred to the action recognition problem performs surprisingly well compared to 3D action recognition methods.", "result_label"]]]
[0, [["Machine learning based systems are reaching society at large and in many aspects of everyday life.", "background_label"], ["This phenomenon has been accompanied by concerns about the ethical issues that may arise from the adoption of these technologies.", "background_label"], ["ML fairness is a recently established area of machine learning that studies how to ensure that biases in the data and model inaccuracies do not lead to models that treat individuals unfavorably on the basis of characteristics such as e.g.", "background_label"], ["race, gender, disabilities, and sexual or political orientation.", "background_label"], ["In this manuscript, we discuss some of the limitations present in the current reasoning about fairness and in methods that deal with it, and describe some work done by the authors to address them.", "method_label"], ["More specifically, we show how causal Bayesian networks can play an important role to reason about and deal with fairness, especially in complex unfairness scenarios.", "method_label"], ["We describe how optimal transport theory can be used to develop methods that impose constraints on the full shapes of distributions corresponding to different sensitive attributes, overcoming the limitation of most approaches that approximate fairness desiderata by imposing constraints on the lower order moments or other functions of those distributions.", "method_label"], ["We present a unified framework that encompasses methods that can deal with different settings and fairness criteria, and that enjoys strong theoretical guarantees.", "method_label"], ["We introduce an approach to learn fair representations that can generalize to unseen tasks.", "method_label"], ["Finally, we describe a technique that accounts for legal restrictions about the use of sensitive attributes.", "method_label"]]]
[0, [["Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation.", "background_label"], ["Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging.", "background_label"], ["Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization.", "method_label"], ["We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized.", "method_label"], ["We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.", "result_label"]]]
[0, [["Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains.", "background_label"], ["They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance.", "background_label"], ["Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts.", "background_label"], ["Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss.", "method_label"], ["We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches.", "method_label"], ["We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA).", "method_label"], ["We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.", "result_label"]]]
[0, [["Machine learning is a tool for building models that accurately represent input training data.", "background_label"], ["When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases.", "background_label"], ["We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary.", "objective_label"], ["The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code.", "method_label"], ["The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z.", "objective_label"], ["Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z.", "method_label"], ["When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016).", "result_label"], ["The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.", "result_label"]]]
[0, [["Due to the recent cases of algorithmic bias in data-driven decision-making, machine learning methods are being put under the microscope in order to understand the root cause of these biases and how to correct them.", "background_label"], ["Here, we consider a basic algorithmic task that is central in machine learning: subsampling from a large data set.", "objective_label"], ["Subsamples are used both as an end-goal in data summarization (where fairness could either be a legal, political or moral requirement) and to train algorithms (where biases in the samples are often a source of bias in the resulting model).", "background_label"], ["Consequently, there is a growing effort to modify either the subsampling methods or the algorithms themselves in order to ensure fairness.", "background_label"], ["However, in doing so, a question that seems to be overlooked is whether it is possible to produce fair subsamples that are also adequately representative of the feature space of the data set - an important and classic requirement in machine learning.", "background_label"], ["Can diversity and fairness be simultaneously ensured?", "objective_label"], ["We start by noting that, in some applications, guaranteeing one does not necessarily guarantee the other, and a new approach is required.", "method_label"], ["Subsequently, we present an algorithmic framework which allows us to produce both fair and diverse samples.", "method_label"], ["Our experimental results on an image summarization task show marked improvements in fairness without compromising feature diversity by much, giving us the best of both the worlds.", "result_label"]]]
[0, [["We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes.", "background_label"], ["Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \\emph{flexibly fair}, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions.", "method_label"], ["We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---enables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.", "result_label"]]]
[0, [["Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning.", "background_label"], ["In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data.", "objective_label"], ["The representation learning process is formulated as an adversarial minimax game.", "objective_label"], ["We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions.", "method_label"], ["On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.", "result_label"]]]
[0, [["In practice, there are often explicit constraints on what representations or decisions are acceptable in an application of machine learning.", "background_label"], ["For example it may be a legal requirement that a decision must not favour a particular group.", "background_label"], ["Alternatively it can be that that representation of data must not have identifying information.", "background_label"], ["We address these two related issues by learning flexible representations that minimize the capability of an adversarial critic.", "method_label"], ["This adversary is trying to predict the relevant sensitive variable from the representation, and so minimizing the performance of the adversary ensures there is little or no information in the representation about the sensitive variable.", "method_label"], ["We demonstrate this adversarial approach on two problems: making decisions free from discrimination and removing private information from images.", "method_label"], ["We formulate the adversarial model as a minimax problem, and optimize that minimax objective using a stochastic gradient alternate min-max optimizer.", "method_label"], ["We demonstrate the ability to provide discriminant free representations for standard test problems, and compare with previous state of the art methods for fairness, showing statistically significant improvement across most cases.", "result_label"], ["The flexibility of this method is shown via a novel problem: removing annotations from images, from unaligned training examples of annotated and unannotated images, and with no a priori knowledge of the form of annotation provided to the model.", "result_label"]]]
[0, [["Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation.", "background_label"], ["We show that demographic information of authors is encoded in -- and can be recovered from -- the intermediate representations learned by text-based neural classifiers.", "background_label"], ["The implication is that decisions of classifiers trained on textual data are not agnostic to -- and likely condition on -- demographic attributes.", "background_label"], ["When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data.", "method_label"], ["This behavior is consistent across several tasks, demographic properties and datasets.", "result_label"], ["We explore several techniques to improve the effectiveness of the adversarial component.", "method_label"], ["Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.", "result_label"]]]
[0, [["Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity---minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss.", "background_label"], ["Worse, as model accuracy affects user retention, a minority group can shrink over time.", "background_label"], ["In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even make initially fair models unfair.", "method_label"], ["To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution.", "method_label"], ["We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups.", "method_label"], ["We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.", "result_label"]]]
[0, [["Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes.", "background_label"], ["Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees.", "background_label"], ["In this paper, we first study the \"linear\"form of this problem i.e., the setting where all the players are linear functions.", "method_label"], ["We show that the resulting optimization problem is both non-convex and non-differentiable.", "method_label"], ["We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance.", "method_label"], ["We then extend this solution and analysis to non-linear functions through kernel representation.", "method_label"], ["Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for \"imparting\"provable invariance to any biased pre-trained data representation, and (b) empirically, the trade-off between utility and invariance provided by our solution is comparable to iterative minimax optimization of existing deep neural network based approaches.", "result_label"], ["Code is available at https://github.com/human-analysis/Kernel-ARL", "other_label"]]]
[0, [["Learning data representations that are transferable and are fair with respect to certain protected attributes is crucial to reducing unfair decisions while preserving the utility of the data.", "background_label"], ["We propose an information-theoretically motivated objective for learning maximally expressive representations subject to fairness constraints.", "objective_label"], ["We demonstrate that a range of existing approaches optimize approximations to the Lagrangian dual of our objective.", "method_label"], ["In contrast to these existing approaches, our objective allows the user to control the fairness of the representations by specifying limits on unfairness.", "method_label"], ["Exploiting duality, we introduce a method that optimizes the model parameters as well as the expressiveness-fairness trade-off.", "method_label"], ["Empirical evidence suggests that our proposed method can balance the trade-off between multiple notions of fairness and achieves higher expressiveness at a lower computational cost.", "result_label"]]]
[0, [["Accurate localization of proteins from fluorescence microscopy images is challenging due to the inter-class similarities and intra-class disparities introducing grave concerns in addressing multi-class classification problems.", "background_label"], ["Conventional machine learning-based image prediction pipelines rely heavily on pre-processing such as normalization and segmentation followed by hand-crafted feature extraction to identify useful, informative, and application-specific features.", "background_label"], ["Here, we demonstrate that deep learning-based pipelines can effectively classify protein images from different datasets.", "objective_label"], ["We propose an end-to-end Protein Localization Convolutional Neural Network (PLCNN) that classifies protein images more accurately and reliably.", "method_label"], ["PLCNN processes raw imagery without involving any pre-processing steps and produces outputs without any customization or parameter adjustment for a particular dataset.", "method_label"], ["Experimental analysis is performed on five benchmark datasets.", "method_label"], ["PLCNN consistently outperformed the existing state-of-the-art approaches from traditional machine learning and deep architectures.", "method_label"], ["This study highlights the importance of deep learning for the analysis of fluorescence microscopy protein imagery.", "result_label"], ["The proposed deep pipeline can better guide drug designing procedures in the pharmaceutical industry and open new avenues for researchers in computational biology and bioinformatics.", "result_label"]]]
[0, [["A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part.", "background_label"], ["We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters.", "background_label"], ["Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules.", "method_label"], ["When multiple predictions agree, a higher level capsule becomes active.", "method_label"], ["We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits.", "result_label"], ["To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.", "result_label"]]]
[0, [["Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years.", "background_label"], ["Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage.", "background_label"], ["Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes.", "background_label"], ["This also applies to residual networks and is very closely related to their performance.", "background_label"], ["In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible.", "method_label"], ["This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability.", "method_label"], ["Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture.", "method_label"], ["Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks.", "result_label"], ["Code is available at https://github.com/jhkim89/PyramidNet}", "other_label"]]]
[0, [["We present a system to help designers create icons that are widely used in banners, signboards, billboards, homepages, and mobile apps.", "background_label"], ["Designers are tasked with drawing contours, whereas our system colorizes contours in different styles.", "background_label"], ["This goal is achieved by training a dual conditional generative adversarial network (GAN) on our collected icon dataset.", "objective_label"], ["One condition requires the generated image and the drawn contour to possess a similar contour, while the other anticipates the image and the referenced icon to be similar in color style.", "method_label"], ["Accordingly, the generator takes a contour image and a man-made icon image to colorize the contour, and then the discriminators determine whether the result fulfills the two conditions.", "method_label"], ["The trained network is able to colorize icons demanded by designers and greatly reduces their workload.", "method_label"], ["For the evaluation, we compared our dual conditional GAN to several state-of-the-art techniques.", "method_label"], ["Experiment results demonstrate that our network is over the previous networks.", "result_label"], ["Finally, we will provide the source code, icon dataset, and trained network for public use.", "result_label"]]]
[0, [["Colorization of grayscale images has been a hot topic in computer vision.", "background_label"], ["Previous research mainly focuses on producing a colored image to match the original one.", "background_label"], ["However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality.", "background_label"], ["In this paper, we design a novel solution for unsupervised diverse colorization.", "objective_label"], ["Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information.", "method_label"], ["With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset.", "method_label"], ["The Turing test of 80 humans further indicates our generated color schemes are highly convincible.", "result_label"]]]
[0, [["Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models.", "background_label"], ["In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator.", "method_label"], ["We show that this model can generate MNIST digits conditioned on class labels.", "method_label"], ["We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.", "result_label"]]]
[0, [["This paper investigates a novel problem of generating images from visual attributes.", "objective_label"], ["We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder.", "method_label"], ["We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations.", "method_label"], ["We use a general energy minimization algorithm for posterior inference of latent variables given novel images.", "method_label"], ["Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion.", "result_label"]]]
[0, [["Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution.", "background_label"], ["However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure.", "background_label"], ["In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S^2-GAN).", "objective_label"], ["Our S^2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image.", "method_label"], ["Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images.", "method_label"], ["The two GANs are first trained independently, and then merged together via joint learning.", "method_label"], ["We show our S^2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations.", "result_label"]]]
[0, [["This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space.", "background_label"], ["As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values.", "method_label"], ["By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image.", "method_label"], ["This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects.", "method_label"], ["Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes.", "result_label"], ["We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.", "result_label"]]]
[0, [["We present an autoencoder that leverages learned representations to better measure similarities in data space.", "background_label"], ["By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective.", "method_label"], ["Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g.", "method_label"], ["translation.", "method_label"], ["We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity.", "method_label"], ["Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g.", "result_label"], ["wearing glasses) can be modified using simple arithmetic.", "result_label"]]]
[0, [["One of the challenges in the study of generative adversarial networks is the instability of its training.", "background_label"], ["In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator.", "objective_label"], ["Our new normalization technique is computationally light and easy to incorporate into existing implementations.", "objective_label"], ["We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.", "result_label"]]]
[0, [["We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks.", "method_label"], ["This method balances the generator and discriminator during training.", "method_label"], ["Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality.", "method_label"], ["We also derive a way of controlling the trade-off between image diversity and visual quality.", "method_label"], ["We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions.", "method_label"], ["This is achieved while using a relatively simple model architecture and a standard training procedure.", "method_label"]]]
[0, [["Visual signals in a video can be divided into content and motion.", "background_label"], ["While content specifies which objects are in the video, motion describes their dynamics.", "background_label"], ["Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation.", "objective_label"], ["The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames.", "method_label"], ["Each random vector consists of a content part and a motion part.", "method_label"], ["While the content part is kept fixed, the motion part is realized as a stochastic process.", "method_label"], ["To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators.", "method_label"], ["Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework.", "result_label"], ["In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion.", "result_label"]]]
[0, [["We present the first dataset targeted at end-to-end NLG in Czech in the restaurant domain, along with several strong baseline models using the sequence-to-sequence approach.", "background_label"], ["While non-English NLG is under-explored in general, Czech, as a morphologically rich language, makes the task even harder: Since Czech requires inflecting named entities, delexicalization or copy mechanisms do not work out-of-the-box and lexicalizing the generated outputs is non-trivial.", "background_label"], ["In our experiments, we present two different approaches to this this problem: (1) using a neural language model to select the correct inflected form while lexicalizing, (2) a two-step generation setup: our sequence-to-sequence model generates an interleaved sequence of lemmas and morphological tags, which are then inflected by a morphological generator.", "method_label"]]]
[0, [["We consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots.", "background_label"], ["To this end, we propose a topic aware sequence-to-sequence (TA-Seq2Seq) model.", "objective_label"], ["The model utilizes topics to simulate prior knowledge of human that guides them to form informative and interesting responses in conversation, and leverages the topic information in generation by a joint attention mechanism and a biased generation probability.", "method_label"], ["The joint attention mechanism summarizes the hidden vectors of an input message as context vectors by message attention, synthesizes topic vectors by topic attention from the topic words of the message obtained from a pre-trained LDA model, and let these vectors jointly affect the generation of words in decoding.", "method_label"], ["To increase the possibility of topic words appearing in responses, the model modifies the generation probability of topic words by adding an extra probability item to bias the overall distribution.", "method_label"], ["Empirical study on both automatic evaluation metrics and human annotations shows that TA-Seq2Seq can generate more informative and interesting responses, and significantly outperform the-state-of-the-art response generation models.", "result_label"]]]
[0, [["In geometry processing, symmetry is a universal type of high-level structural information of 3D models and benefits many geometry processing tasks including shape segmentation, alignment, matching, and completion.", "background_label"], ["Thus it is an important problem to analyze various symmetry forms of 3D shapes.", "background_label"], ["Planar reflective symmetry is the most fundamental one.", "background_label"], ["Traditional methods based on spatial sampling can be time-consuming and may not be able to identify all the symmetry planes.", "background_label"], ["In this paper, we present a novel learning framework to automatically discover global planar reflective symmetry of a 3D shape.", "method_label"], ["Our framework trains an unsupervised 3D convolutional neural network to extract global model features and then outputs possible global symmetry parameters, where input shapes are represented using voxels.", "method_label"], ["We introduce a dedicated symmetry distance loss along with a regularization loss to avoid generating duplicated symmetry planes.", "background_label"], ["Our network can also identify generalized cylinders by predicting their rotation axes.", "background_label"], ["We further provide a method to remove invalid and duplicated planes and axes.", "method_label"], ["We demonstrate that our method is able to produce reliable and accurate results.", "method_label"], ["Our neural network based method is hundreds of times faster than the state-of-the-art methods, which are based on sampling.", "method_label"], ["Our method is also robust even with noisy or incomplete input surfaces.", "result_label"]]]
[0, [["We present a learning framework for abstracting complex shapes by learning to assemble objects using 3D volumetric primitives.", "background_label"], ["In addition to generating simple and geometrically interpretable explanations of 3D objects, our framework also allows us to automatically discover and exploit consistent structure in the data.", "method_label"], ["We demonstrate that using our method allows predicting shape representations which can be leveraged for obtaining a consistent parsing across the instances of a shape collection and constructing an interpretable shape similarity measure.", "method_label"], ["We also examine applications for image-based prediction as well as shape manipulation.", "result_label"]]]
[0, [["What is a good vector representation of an object?", "background_label"], ["We believe that it should be generative in 3D, in the sense that it can produce new 3D objects; as well as be predictable from 2D, in the sense that it can be perceived from 2D images.", "background_label"], ["We propose a novel architecture, called the TL-embedding network, to learn an embedding space with these properties.", "objective_label"], ["The network consists of two components: (a) an autoencoder that ensures the representation is generative; and (b) a convolutional network that ensures the representation is predictable.", "method_label"], ["This enables tackling a number of tasks including voxel prediction from 2D images and 3D model retrieval.", "method_label"], ["Extensive experimental analysis demonstrates the usefulness and versatility of this embedding.", "result_label"]]]
[0, [["We present an algorithm for extraction of a probabilistic deterministic finite automaton (PDFA) from a given black-box language model, such as a recurrent neural network (RNN).", "objective_label"], ["The algorithm is a variant of the exact-learning algorithm L*, adapted to a probabilistic setting with noise.", "method_label"], ["The key insight is the use of conditional probabilities for observations, and the introduction of a local tolerance when comparing them.", "method_label"], ["When applied to RNNs, our algorithm often achieves better word error rate (WER) and normalised distributed cumulative gain (NDCG) than that achieved by spectral extraction of weighted finite automata (WFA) from the same networks.", "result_label"], ["PDFAs are substantially more expressive than n-grams, and are guaranteed to be stochastic and deterministic - unlike spectrally extracted WFAs.", "result_label"]]]
[0, [["We present a solution to scale spectral algorithms for learning sequence functions.", "background_label"], ["We are interested in the case where these functions are sparse (that is, for most sequences they return 0).", "background_label"], ["Spectral algorithms reduce the learning problem to the task of computing an SVD decomposition over a special type of matrix called the Hankel matrix.", "method_label"], ["This matrix is designed to capture the relevant statistics of the training sequences.", "method_label"], ["What is crucial is that to capture long range dependencies we must consider very large Hankel matrices.", "method_label"], ["Thus the computation of the SVD becomes a critical bottleneck.", "method_label"], ["Our solution finds a subset of rows and columns of the Hankel that realizes a compact and informative Hankel submatrix.", "method_label"], ["The novelty lies in the way that this subset is selected: we exploit a maximal bipartite matching combinatorial algorithm to look for a sub-block with full structural rank, and show how computation of this sub-block can be further improved by exploiting the specific structure of Hankel matrices.", "result_label"]]]
[0, [["In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and clustering.", "background_label"], ["We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential.", "background_label"], ["While all of these techniques have been studied separately, they have rarely been studied in combination.", "background_label"], ["We find some significant interactions, especially with smoothing and clustering techniques.", "method_label"], ["We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs.", "method_label"], ["We achieve perplexity reductions between 38% and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%.", "method_label"], ["Our perplexity reductions are perhaps the highest reported compared to a fair baseline.", "result_label"], ["This is the extended version of the paper; it contains additional details and proofs, and is designed to be a good introduction to the state of the art in language modeling.", "result_label"]]]
[0, [["Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series.", "background_label"], ["In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues.", "background_label"], ["We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs.", "method_label"], ["The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM.", "method_label"], ["This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language.", "method_label"], ["The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.", "method_label"]]]
[0, [["Understanding how a learned black box works is of crucial interest for the future of Machine Learning.", "background_label"], ["In this paper, we pioneer the question of the global interpretability of learned black box models that assign numerical values to symbolic sequential data.", "background_label"], ["To tackle that task, we propose a spectral algorithm for the extraction of weighted automata (WA) from such black boxes.", "objective_label"], ["This algorithm does not require the access to a dataset or to the inner representation of the black box: the inferred model can be obtained solely by querying the black box, feeding it with inputs and analyzing its outputs.", "method_label"], ["Experiments using Recurrent Neural Networks (RNN) trained on a wide collection of 48 synthetic datasets and 2 real datasets show that the obtained approximation is of great quality.", "result_label"]]]
[0, [["We share a French-English parallel corpus of Foursquare restaurant reviews (https://europe.naverlabs.com/research/natural-language-processing/machine-translation-of-restaurant-reviews), and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where better-quality MT would be greatly beneficial.", "objective_label"], ["We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness.", "method_label"], ["We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems.", "method_label"], ["Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words.", "result_label"]]]
[0, [["Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems.", "background_label"], ["However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets.", "background_label"], ["In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations.", "objective_label"], ["We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair.", "method_label"], ["We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data.", "method_label"], ["This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.", "result_label"], ["The data is publicly available at www.cs.cmu.edu/~pmichel1/mtnt/.", "other_label"]]]
[0, [["We share the findings of the first shared task on improving robustness of Machine Translation (MT).", "background_label"], ["The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models; robustness to noisy input and domain mismatch.", "objective_label"], ["We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations.", "method_label"], ["As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc.", "method_label"], ["All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU.", "method_label"], ["We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson's r = 0.94 and 0.95).", "result_label"], ["Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task.", "result_label"], ["Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g.", "result_label"], ["systems better at producing colloquial expressions received higher score from human judgment.", "result_label"]]]
[0, [["Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems.", "background_label"], ["Unfortunately, they are also very brittle and easily falter when presented with noisy data.", "background_label"], ["In this paper, we confront NMT models with synthetic and natural sources of noise.", "objective_label"], ["We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending.", "method_label"], ["We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts.", "method_label"], ["We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.", "result_label"]]]
[0, [["Modern Machine Translation (MT) systems perform consistently well on clean, in-domain text.", "background_label"], ["However most human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of output translation.", "background_label"], ["In this paper we leverage the Machine Translation of Noisy Text (MTNT) dataset to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data.", "method_label"], ["Synthesizing noise in this manner we are ultimately able to make a vanilla MT system resilient to naturally occurring noise and partially mitigate loss in accuracy resulting therefrom.", "result_label"]]]
[0, [["Neural Machine Translation (NMT) is a new approach for automatic translation of text from one human language into another.", "background_label"], ["The basic concept in NMT is to train a large Neural Network that maximizes the translation performance on a given parallel corpus.", "background_label"], ["NMT is gaining popularity in the research community because it outperformed traditional SMT approaches in several translation tasks at WMT and other evaluation tasks/benchmarks at least for some language pairs.", "background_label"], ["However, many of the enhancements in SMT over the years have not been incorporated into the NMT framework.", "background_label"], ["In this paper, we focus on one such enhancement namely domain adaptation.", "objective_label"], ["We propose an approach for adapting a NMT system to a new domain.", "objective_label"], ["The main idea behind domain adaptation is that the availability of large out-of-domain training data and a small in-domain training data.", "method_label"], ["We report significant gains with our proposed method in both automatic metrics and a human subjective evaluation metric on two language pairs.", "result_label"], ["With our adaptation method, we show large improvement on the new domain while the performance of our general domain only degrades slightly.", "result_label"], ["In addition, our approach is fast enough to adapt an already trained system to a new domain within few hours without the need to retrain the NMT model on the combined data which usually takes several days/weeks depending on the volume of the data.", "result_label"]]]
[0, [["Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training.", "background_label"], ["Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT.", "background_label"], ["In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture.", "method_label"], ["By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results.", "method_label"], ["We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.", "result_label"]]]
[0, [["Machine translation systems are very sensitive to the domains they were trained on.", "background_label"], ["Several domain adaptation techniques have been deeply studied.", "background_label"], ["We propose a new technique for neural machine translation (NMT) that we call domain control which is performed at runtime using a unique neural network covering multiple domains.", "method_label"], ["The presented approach shows quality improvements when compared to dedicated domains translating on any of the covered domains and even on out-of-domain data.", "method_label"], ["In addition, model parameters do not need to be re-estimated for each domain, making this effective to real use cases.", "method_label"], ["Evaluation is carried out on English-to-French translation for two different testing scenarios.", "method_label"], ["We first consider the case where an end-user performs translations on a known domain.", "method_label"], ["Secondly, we consider the scenario where the domain is not known and predicted at the sentence level before translating.", "method_label"], ["Results show consistent accuracy improvements for both conditions.", "result_label"]]]
[0, [["The primary goal of ad-hoc retrieval (document retrieval in the context of question answering) is to find relevant documents satisfied the information need posted in a natural language query.", "objective_label"], ["It requires a good understanding of the query and all the documents in a corpus, which is difficult because the meaning of natural language texts depends on the context, syntax,and semantics.", "background_label"], ["Recently deep neural networks have been used to rank search results in response to a query.", "background_label"], ["In this paper, we devise a multi-resolution neural network(MRNN) to leverage the whole hierarchy of representations for document retrieval.", "objective_label"], ["The proposed MRNN model is capable of deriving a representation that integrates representations of different levels of abstraction from all the layers of the learned hierarchical representation.Moreover, a duplex attention component is designed to refinethe multi-resolution representation so that an optimal contextfor matching the query and document can be determined.", "method_label"], ["More specifically, the first attention mechanism determines optimal context from the learned multi-resolution representation for the query and document.", "method_label"], ["The latter attention mechanism aims to fine-tune the representation so that the query and the relevant document are closer in proximity.", "method_label"], ["The empirical study shows that MRNN with the duplex attention is significantly superior to existing models used for ad-hoc retrieval on benchmark datasets including SQuAD, WikiQA, QUASAR, and TrecQA.", "result_label"]]]
[0, [["Answer sentence selection is the task of identifying sentences that contain the answer to a given question.", "background_label"], ["This is an important problem in its own right as well as in the larger context of open domain question answering.", "background_label"], ["We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding.", "objective_label"], ["This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources.", "method_label"], ["Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages.", "method_label"], ["Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.", "result_label"]]]
[0, [["In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training.", "objective_label"], ["In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations.", "method_label"], ["Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g.", "method_label"], ["trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling.", "method_label"], ["Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies.", "method_label"], ["The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.", "result_label"]]]
[0, [["Many machine learning algorithms require the input to be represented as a fixed-length feature vector.", "background_label"], ["When it comes to texts, one of the most common fixed-length features is bag-of-words.", "background_label"], ["Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words.", "background_label"], ["For example, \"powerful,\"\"strong\"and \"Paris\"are equally distant.", "background_label"], ["In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents.", "objective_label"], ["Our algorithm represents each document by a dense vector which is trained to predict words in the document.", "method_label"], ["Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models.", "method_label"], ["Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations.", "result_label"], ["Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "result_label"]]]
[0, [["We present two new large-scale datasets aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text.", "background_label"], ["The Quasar-S dataset consists of 37000 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow.", "background_label"], ["The posts and comments on the website serve as the background corpus for answering the cloze questions.", "method_label"], ["The Quasar-T dataset consists of 43000 open-domain trivia questions and their answers obtained from various internet sources.", "method_label"], ["ClueWeb09 serves as the background corpus for extracting these answers.", "method_label"], ["We pose these datasets as a challenge for two related subtasks of factoid Question Answering: (1) searching for relevant pieces of text that include the correct answer to a query, and (2) reading the retrieved text to answer the query.", "method_label"], ["We also describe a retrieval system for extracting relevant sentences and documents from the corpus given a query, and include these in the release for researchers wishing to only focus on (2).", "method_label"], ["We evaluate several baselines on both datasets, ranging from simple heuristics to powerful neural models, and show that these lag behind human performance by 16.4% and 32.1% for Quasar-S and -T respectively.", "result_label"], ["The datasets are available at https://github.com/bdhingra/quasar .", "result_label"]]]
[0, [["In this paper, we present an accurate and extensible approach for the coreference resolution task.", "background_label"], ["We formulate the problem as a span prediction task, like in machine reading comprehension (MRC): A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query.", "method_label"], ["This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the MRC framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing MRC datasets can be used for data augmentation to improve the model's generalization capability.", "method_label"], ["Experiments demonstrate significant performance boost over previous models, with 87.5 (+2.5) F1 score on the GAP benchmark and 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark.", "result_label"]]]
[0, [["Deep learning has improved performance on many natural language processing (NLP) tasks individually.", "background_label"], ["However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task.", "background_label"], ["We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution.", "method_label"], ["We cast all tasks as question answering over a context.", "method_label"], ["Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting.", "method_label"], ["MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification.", "method_label"], ["We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy.", "result_label"], ["Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting.", "result_label"], ["We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.", "result_label"]]]
[0, [["The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.", "background_label"], ["Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels.", "background_label"], ["In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks.", "objective_label"], ["Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task.", "objective_label"], ["For example, extracting entities with the \\textsc{per} label is formalized as extracting answer spans to the question \"{\\it which person is mentioned in the text?}\".", "method_label"], ["This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities for different categories requires answering two independent questions.", "method_label"], ["Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER.", "method_label"], ["We conduct experiments on both {\\em nested} and {\\em flat} NER datasets.", "method_label"], ["Experimental results demonstrate the effectiveness of the proposed formulation.", "result_label"], ["We are able to achieve vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e.,+0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0.", "result_label"]]]
[0, [["There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters.", "background_label"], ["Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features.", "background_label"], ["We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions.", "objective_label"], ["We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.", "result_label"]]]
[0, [["Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers.", "background_label"], ["Could ordering the sublayers in a different pattern lead to better performance?", "background_label"], ["We generate randomly ordered transformers and train them with the language modeling objective.", "method_label"], ["We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top.", "method_label"], ["We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time.", "method_label"], ["However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models.", "result_label"], ["Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.", "result_label"]]]
[0, [["Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models.", "background_label"], ["Our goal is to apply NAS to search for a better alternative to the Transformer.", "objective_label"], ["We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer.", "method_label"], ["To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models.", "method_label"], ["The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B.", "result_label"], ["At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original \"big\"Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.", "result_label"]]]
[0, [["Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available.", "background_label"], ["In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.", "background_label"], ["Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient.", "method_label"], ["We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.", "method_label"], ["To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.", "method_label"], ["In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.", "method_label"], ["Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.", "result_label"], ["Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.", "other_label"]]]
[0, [["In this work, we present a family of vector quantization schemes \\emph{vqSGD} (Vector-Quantized Stochastic Gradient Descent) that provide an asymptotic reduction in the communication cost with convergence guarantees in first-order distributed optimization.", "background_label"], ["In the process we derive the following fundamental information theoretic fact: $\\Theta(\\frac{d}{R^2})$ bits are necessary and sufficient to describe an unbiased estimator ${\\hat{g}}({g})$ for any ${g}$ in the $d$-dimensional unit sphere, under the constraint that $\\|{\\hat{g}}({g})\\|_2\\le R$ almost surely.", "method_label"], ["In particular, we consider a randomized scheme based on the convex hull of a point set, that returns an unbiased estimator of a $d$-dimensional gradient vector with almost surely bounded norm.", "method_label"], ["We provide multiple efficient instances of our scheme, that are near optimal, and require only $o(d)$ bits of communication at the expense of tolerable increase in error.", "method_label"], ["The instances of our quantization scheme are obtained using the properties of binary error-correcting codes and provide a smooth tradeoff between the communication and the estimation error of quantization.", "method_label"], ["Furthermore, we show that \\emph{vqSGD} also offers strong privacy guarantees.", "result_label"]]]
[0, [["In distributed statistical learning, $N$ samples are split across $m$ machines and a learner wishes to use minimal communication to learn as well as if the examples were on a single machine.", "background_label"], ["This model has received substantial interest in machine learning due to its scalability and potential for parallel speedup.", "background_label"], ["However, in high-dimensional settings, where the number examples is smaller than the number of features (\"dimension\"), the speedup afforded by distributed learning may be overshadowed by the cost of communicating a single example.", "background_label"], ["This paper investigates the following question: When is it possible to learn a $d$-dimensional model in the distributed setting with total communication sublinear in $d$?", "objective_label"], ["Starting with a negative result, we show that for learning $\\ell_1$-bounded or sparse linear models, no algorithm can obtain optimal error until communication is linear in dimension.", "method_label"], ["Our main result is that that by slightly relaxing the standard boundedness assumptions for linear models, we can obtain distributed algorithms that enjoy optimal error with communication logarithmic in dimension.", "result_label"], ["This result is based on a family of algorithms that combine mirror descent with randomized sparsification/quantization of iterates, and extends to the general stochastic convex optimization model.", "result_label"]]]
[0, [["Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation.", "background_label"], ["Unlike previous works, we make no probabilistic assumptions on the data.", "background_label"], ["We first show that for $d$ dimensional data with $n$ clients, a naive stochastic binary rounding approach yields a mean squared error (MSE) of $\\Theta(d/n)$ and uses a constant number of bits per dimension per client.", "method_label"], ["We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to $\\mathcal{O}((\\log d)/n)$ and a better coding strategy further reduces the error to $\\mathcal{O}(1/n)$ and uses a constant number of bits per dimension per client.", "method_label"], ["We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost.", "result_label"], ["We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd's algorithm for k-means and power iteration for PCA.", "result_label"]]]
[0, [["Sign-based algorithms (e.g.", "background_label"], ["signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers.", "background_label"], ["We show simple convex counter-examples where signSGD does not converge to the optimum.", "method_label"], ["Further, even when it does converge, signSGD may generalize poorly when compared with SGD.", "method_label"], ["These issues arise because of the biased nature of the sign compression operator.", "result_label"], ["We then show that using error-feedback, i.e.", "background_label"], ["incorporating the error made by the compression operator into the next step, overcomes these issues.", "background_label"], ["We prove that our algorithm EF-SGD with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions.", "method_label"], ["Thus EF-SGD achieves gradient compression for free.", "method_label"], ["Our experiments thoroughly substantiate the theory and show that error-feedback improves both convergence and generalization.", "result_label"], ["Code can be found at \\url{https://github.com/epfml/error-feedback-SGD}.", "other_label"]]]
[0, [["Distributed training of massive machine learning models, in particular deep neural networks, via Stochastic Gradient Descent (SGD) is becoming commonplace.", "background_label"], ["Several families of communication-reduction methods, such as quantization, large-batch methods, and gradient sparsification, have been proposed.", "background_label"], ["To date, gradient sparsification methods - where each node sorts gradients by magnitude, and only communicates a subset of the components, accumulating the rest locally - are known to yield some of the largest practical gains.", "method_label"], ["Such methods can reduce the amount of communication per step by up to three orders of magnitude, while preserving model accuracy.", "method_label"], ["Yet, this family of methods currently has no theoretical justification.", "method_label"], ["This is the question we address in this paper.", "method_label"], ["We prove that, under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel SGD.", "method_label"], ["The main insight is that sparsification methods implicitly maintain bounds on the maximum impact of stale updates, thanks to selection by magnitude.", "result_label"], ["Our analysis and empirical validation also reveal that these methods do require analytical conditions to converge well, justifying existing heuristics.", "result_label"]]]
[0, [["Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e.", "background_label"], ["algorithms that leverage the compute power of many devices for training.", "background_label"], ["The communication overhead is a key bottleneck that hinders perfect scalability.", "background_label"], ["Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification).", "background_label"], ["Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far.", "background_label"], ["In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory).", "method_label"], ["That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate.", "method_label"], ["We present numerical experiments to illustrate the theoretical findings and the better scalability for distributed applications.", "result_label"]]]
[0, [["Making decisions in complex driving environments is a challenging task for autonomous agents.", "background_label"], ["Imitation learning methods have great potentials for achieving such a goal.", "background_label"], ["Adversarial Inverse Reinforcement Learning (AIRL) is one of the state-of-art imitation learning methods that can learn both a behavioral policy and a reward function simultaneously, yet it is only demonstrated in simple and static environments where no interactions are introduced.", "background_label"], ["In this paper, we improve and stabilize AIRL's performance by augmenting it with semantic rewards in the learning framework.", "method_label"], ["Additionally, we adapt the augmented AIRL to a more practical and challenging decision-making task in a highly interactive environment in autonomous driving.", "method_label"], ["The proposed method is compared with four baselines and evaluated by four performance metrics.", "method_label"], ["Simulation results show that the augmented AIRL outperforms all the baseline methods, and its performance is comparable with that of the experts on all of the four metrics.", "result_label"]]]
[0, [["The ability to accurately predict and simulate human driving behavior is critical for the development of intelligent transportation systems.", "background_label"], ["Traditional modeling methods have employed simple parametric models and behavioral cloning.", "background_label"], ["This paper adopts a method for overcoming the problem of cascading errors inherent in prior approaches, resulting in realistic behavior that is robust to trajectory perturbations.", "method_label"], ["We extend Generative Adversarial Imitation Learning to the training of recurrent policies, and we demonstrate that our model outperforms rule-based controllers and maximum likelihood models in realistic highway simulations.", "method_label"], ["Our model both reproduces emergent behavior of human drivers, such as lane change rate, while maintaining realistic control over long time horizons.", "result_label"]]]
[0, [["Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering.", "background_label"], ["Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function.", "background_label"], ["Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics.", "background_label"], ["In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation.", "objective_label"], ["We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training.", "method_label"], ["Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.", "result_label"]]]
[0, [["In network link prediction, it is possible to hide a target link from being predicted with a small perturbation on network structure.", "background_label"], ["This observation may be exploited in many real world scenarios, for example, to preserve privacy, or to exploit financial security.", "background_label"], ["There have been many recent studies to generate adversarial examples to mislead deep learning models on graph data.", "background_label"], ["However, none of the previous work has considered the dynamic nature of real-world systems.", "background_label"], ["In this work, we present the first study of adversarial attack on dynamic network link prediction (DNLP).", "objective_label"], ["The proposed attack method, namely time-aware gradient attack (TGA), utilizes the gradient information generated by deep dynamic network embedding (DDNE) across different snapshots to rewire a few links, so as to make DDNE fail to predict target links.", "method_label"], ["We implement TGA in two ways: one is based on traversal search, namely TGA-Tra; and the other is simplified with greedy search for efficiency, namely TGA-Gre.", "method_label"], ["We conduct comprehensive experiments which show the outstanding performance of TGA in attacking DNLP algorithms.", "result_label"]]]
[0, [["Predicting the potential relations between nodes in networks, known as link prediction, has long been a challenge in network science.", "background_label"], ["However, most studies just focused on link prediction of static network, while real-world networks always evolve over time with the occurrence and vanishing of nodes and links.", "background_label"], ["Dynamic network link prediction thus has been attracting more and more attention since it can better capture the evolution nature of networks, but still most algorithms fail to achieve satisfied prediction accuracy.", "background_label"], ["Motivated by the excellent performance of Long Short-Term Memory (LSTM) in processing time series, in this paper, we propose a novel Encoder-LSTM-Decoder (E-LSTM-D) deep learning model to predict dynamic links end to end.", "objective_label"], ["It could handle long term prediction problems, and suits the networks of different scales with fine-tuned structure.", "method_label"], ["To the best of our knowledge, it is the first time that LSTM, together with an encoder-decoder architecture, is applied to link prediction in dynamic networks.", "method_label"], ["This new model is able to automatically learn structural and temporal features in a unified framework, which can predict the links that never appear in the network before.", "method_label"], ["The extensive experiments show that our E-LSTM-D model significantly outperforms newly proposed dynamic network link prediction methods and obtain the state-of-the-art results.", "result_label"]]]
[0, [["In this paper, we study the robustness of graph convolutional networks (GCNs).", "background_label"], ["Previous work have shown that GCNs are vulnerable to adversarial perturbation on adjacency or feature matrices of existing nodes; however, such attacks are usually unrealistic in real applications.", "background_label"], ["For instance, in social network applications, the attacker will need to hack into either the client or server to change existing links or features.", "background_label"], ["In this paper, we propose a new type of \"fake node attacks\"to attack GCNs by adding malicious fake nodes.", "objective_label"], ["This is much more realistic than previous attacks; in social network applications, the attacker only needs to register a set of fake accounts and link to existing ones.", "background_label"], ["To conduct fake node attacks, a greedy algorithm is proposed to generate edges of malicious nodes and their corresponding features aiming to minimize the classification accuracy on the target nodes.", "method_label"], ["In addition, we introduce a discriminator to classify malicious nodes from real nodes, and propose a Greedy-GAN attack to simultaneously update the discriminator and the attacker, to make malicious nodes indistinguishable from the real ones.", "method_label"], ["Our non-targeted attack decreases the accuracy of GCN down to 0.03, and our targeted attack reaches a success rate of 78% on a group of 100 nodes, and 90% on average for attacking a single target node.", "result_label"]]]
[0, [["The Internet and social media have fueled enormous interest in social network analysis.", "background_label"], ["New tools continue to be developed and used to analyse our personal connections, with particular emphasis on detecting communities or identifying key individuals in a social network.", "background_label"], ["This raises privacy concerns that are likely to exacerbate in the future.", "background_label"], ["With this in mind, we ask the question: Can individuals or groups actively manage their connections to evade social network analysis tools?", "objective_label"], ["By addressing this question, the general public may better protect their privacy, oppressed activist groups may better conceal their existence, and security agencies may better understand how terrorists escape detection.", "result_label"], ["We first study how an individual can evade \"network centrality\"analysis without compromising his or her influence within the network.", "background_label"], ["We prove that an optimal solution to this problem is hard to compute.", "background_label"], ["Despite this hardness, we demonstrate that even a simple heuristic, whereby attention is restricted to the individual's immediate neighbourhood, can be surprisingly effective in practice.", "background_label"], ["For instance, it could disguise Mohamed Atta's leading position within the WTC terrorist network, and that is by rewiring a strikingly-small number of connections.", "method_label"], ["Next, we study how a community can increase the likelihood of being overlooked by community-detection algorithms.", "method_label"], ["We propose a measure of concealment, expressing how well a community is hidden, and use it to demonstrate the effectiveness of a simple heuristic, whereby members of the community either \"unfriend\"certain other members, or \"befriend\"some non-members, in a coordinated effort to camouflage their community.", "result_label"]]]
[0, [["Network embedding maps a network into a low-dimensional Euclidean space, and thus facilitate many network analysis tasks, such as node classification, link prediction and community detection etc, by utilizing machine learning methods.", "background_label"], ["In social networks, we may pay special attention to user privacy, and would like to prevent some target nodes from being identified by such network analysis methods in certain cases.", "objective_label"], ["Inspired by successful adversarial attack on deep learning models, we propose a framework to generate adversarial networks based on the gradient information in Graph Convolutional Network (GCN).", "method_label"], ["In particular, we extract the gradient of pairwise nodes based on the adversarial network, and select the pair of nodes with maximum absolute gradient to realize the Fast Gradient Attack (FGA) and update the adversarial network.", "method_label"], ["This process is implemented iteratively and terminated until certain condition is satisfied, i.e., the number of modified links reaches certain predefined value.", "method_label"], ["Comprehensive attacks, including unlimited attack, direct attack and indirect attack, are performed on six well-known network embedding methods.", "method_label"], ["The experiments on real-world networks suggest that our proposed FGA behaves better than some baseline methods, i.e., the network embedding can be easily disturbed using FGA by only rewiring few links, achieving state-of-the-art attack performance.", "result_label"]]]
[0, [["Deep learning models for graphs have achieved strong performance for the task of node classification.", "background_label"], ["Despite their proliferation, currently there is no study of their robustness to adversarial attacks.", "background_label"], ["Yet, in domains where they are likely to be used, e.g.", "background_label"], ["the web, adversaries are common.", "background_label"], ["Can deep learning models for graphs be easily fooled?", "background_label"], ["In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions.", "objective_label"], ["In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model.", "background_label"], ["We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account.", "method_label"], ["Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics.", "method_label"], ["To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations.", "method_label"], ["Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations.", "result_label"], ["Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.", "result_label"]]]
[0, [["Unsupervised node embedding methods (e.g., DeepWalk, LINE, and node2vec) have attracted growing interests given their simplicity and effectiveness.", "background_label"], ["However, although these methods have been proved effective in a variety of applications, none of the existing work has analyzed the robustness of them.", "background_label"], ["This could be very risky if these methods are attacked by an adversarial party.", "background_label"], ["In this paper, we take the task of link prediction as an example, which is one of the most fundamental problems for graph analysis, and introduce a data positioning attack to node embedding methods.", "method_label"], ["We give a complete characterization of attacker's utilities and present efficient solutions to adversarial attacks for two popular node embedding methods: DeepWalk and LINE.", "method_label"], ["We evaluate our proposed attack model on multiple real-world graphs.", "method_label"], ["Experimental results show that our proposed model can significantly affect the results of link prediction by slightly changing the graph structures (e.g., adding or removing a few edges).", "result_label"], ["We also show that our proposed model is very general and can be transferable across different embedding methods.", "result_label"], ["Finally, we conduct a case study on a coauthor network to better understand our attack method.", "result_label"]]]
[0, [["Recent developments in computer vision and machine learning have made it possible to create realistic manipulated videos of human faces, raising the issue of ensuring adequate protection against the malevolent effects unlocked by such capabilities.", "background_label"], ["In this paper we propose local image features that are shared across manipulated regions are the key element for the automatic detection of manipulated face images.", "objective_label"], ["We also design a lightweight architecture with the correct structural biases for extracting such features and derive a multitask training scheme that consistently outperforms image class supervision alone.", "method_label"], ["The trained networks achieve state-of-the-art results in the FaceForensics++ dataset using significantly reduced number of parameters and are shown to work well in detecting fully generated face images.", "result_label"]]]
[0, [["This paper presents a method to automatically and efficiently detect face tampering in videos, and particularly focuses on two recent techniques used to generate hyper-realistic forged videos: Deepfake and Face2Face.", "background_label"], ["Traditional image forensics techniques are usually not well suited to videos due to the compression that strongly degrades the data.", "background_label"], ["Thus, this paper follows a deep learning approach and presents two networks, both with a low number of layers to focus on the mesoscopic properties of images.", "method_label"], ["We evaluate those fast networks on both an existing dataset and a dataset we have constituted from online videos.", "result_label"], ["The tests demonstrate a very successful detection rate with more than 98% for Deepfake and 95% for Face2Face.", "result_label"]]]
[0, [["In this work, we describe a new deep learning based method that can effectively distinguish AI-generated fake videos (referred to as {\\em DeepFake} videos hereafter) from real videos.", "objective_label"], ["Our method is based on the observations that current DeepFake algorithm can only generate images of limited resolutions, which need to be further warped to match the original faces in the source video.", "method_label"], ["Such transforms leave distinctive artifacts in the resulting DeepFake videos, and we show that they can be effectively captured by convolutional neural networks (CNNs).", "method_label"], ["Compared to previous methods which use a large amount of real and DeepFake generated images to train CNN classifier, our method does not need DeepFake generated images as negative training examples since we target the artifacts in affine face warping as the distinctive feature to distinguish real and fake images.", "method_label"], ["The advantages of our method are two-fold: (1) Such artifacts can be simulated directly using simple image processing operations on a image to make it as negative example.", "method_label"], ["Since training a DeepFake model to generate negative examples is time-consuming and resource-demanding, our method saves a plenty of time and resources in training data collection; (2) Since such artifacts are general existed in DeepFake videos from different sources, our method is more robust compared to others.", "method_label"], ["Our method is evaluated on two sets of DeepFake video datasets for its effectiveness in practice.", "result_label"]]]
[0, [["In this paper, we propose a new method to expose AI-generated fake face images or videos (commonly known as the Deep Fakes).", "objective_label"], ["Our method is based on the observations that Deep Fakes are created by splicing synthesized face region into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images.", "method_label"], ["We perform experiments to demonstrate this phenomenon and further develop a classification method based on this cue.", "method_label"], ["Using features based on this cue, an SVM classifier is evaluated using a set of real face images and Deep Fakes.", "result_label"]]]
[0, [["Deep metric learning has yielded impressive results in tasks such as clustering and image retrieval by leveraging neural networks to obtain highly discriminative feature embeddings, which can be used to group samples into different classes.", "background_label"], ["Much research has been devoted to the design of smart loss functions or data mining strategies for training such networks.", "background_label"], ["Most methods consider only pairs or triplets of samples within a mini-batch to compute the loss function, which is commonly based on the distance between embeddings.", "method_label"], ["We propose Group Loss, a loss function based on a differentiable label-propagation method that enforces embedding similarity across all samples of a group while promoting, at the same time, low-density regions amongst data points belonging to different groups.", "method_label"], ["Guided by the smoothness assumption that \"similar objects should belong to the same group\", the proposed loss trains the neural network for a classification task, enforcing a consistent labelling amongst samples within a class.", "method_label"], ["We show state-of-the-art results on clustering and image retrieval on several datasets, and show the potential of our method when combined with other techniques such as ensembles", "result_label"]]]
[0, [["Learning the embedding space, where semantically similar objects are located close together and dissimilar objects far apart, is a cornerstone of many computer vision applications.", "background_label"], ["Existing approaches usually learn a single metric in the embedding space for all available data points, which may have a very complex non-uniform distribution with different notions of similarity between objects, e.g.", "background_label"], ["appearance, shape, color or semantic meaning.", "background_label"], ["Approaches for learning a single distance metric often struggle to encode all different types of relationships and do not generalize well.", "background_label"], ["In this work, we propose a novel easy-to-implement divide and conquer approach for deep metric learning, which significantly improves the state-of-the-art performance of metric learning.", "objective_label"], ["Our approach utilizes the embedding space more efficiently by jointly splitting the embedding space and data into $K$ smaller sub-problems.", "method_label"], ["It divides both, the data and the embedding space into $K$ subsets and learns $K$ separate distance metrics in the non-overlapping subspaces of the embedding space, defined by groups of neurons in the embedding layer of the neural network.", "method_label"], ["The proposed approach increases the convergence speed and improves generalization since the complexity of each sub-problem is reduced compared to the original one.", "method_label"], ["We show that our approach outperforms the state-of-the-art by a large margin in retrieval, clustering and re-identification tasks on CUB200-2011, CARS196, Stanford Online Products, In-shop Clothes and PKU VehicleID datasets.", "result_label"]]]
[0, [["We present a novel hierarchical triplet loss (HTL) capable of automatically collecting informative training samples (triplets) via a defined hierarchical tree that encodes global context information.", "background_label"], ["This allows us to cope with the main limitation of random sampling in training a conventional triplet loss, which is a central issue for deep metric learning.", "objective_label"], ["Our main contributions are two-fold.", "objective_label"], ["(i) we construct a hierarchical class-level tree where neighboring classes are merged recursively.", "method_label"], ["The hierarchical structure naturally captures the intrinsic data distribution over the whole database.", "method_label"], ["(ii) we formulate the problem of triplet collection by introducing a new violate margin, which is computed dynamically based on the designed hierarchical tree.", "method_label"], ["This allows it to automatically select meaningful hard samples with the guide of global context.", "method_label"], ["It encourages the model to learn more discriminative features from visual similar classes, leading to faster convergence and better performance.", "method_label"], ["Our method is evaluated on the tasks of image retrieval and face recognition, where it outperforms the standard triplet loss substantially by 1%-18%.", "result_label"], ["It achieves new state-of-the-art performance on a number of benchmarks, with much fewer learning iterations.", "result_label"]]]
[0, [["We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity.", "background_label"], ["Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point $x$ is similar to a set of positive points $Y$, and dissimilar to a set of negative points $Z$, and a loss defined over these distances is minimized.", "background_label"], ["While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods.", "method_label"], ["These methods are challenging to optimize.", "method_label"], ["A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc.", "method_label"], ["Even with these tricks, the convergence rate of such methods is slow.", "background_label"], ["In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well.", "method_label"], ["These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss.", "method_label"], ["This proxy-based loss is empirically better behaved.", "method_label"], ["As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.", "result_label"]]]
[0, [["Deep embeddings answer one simple question: How similar are two images?", "background_label"], ["Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search.", "background_label"], ["The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss.", "background_label"], ["While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role.", "objective_label"], ["We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches.", "method_label"], ["In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions.", "method_label"], ["We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification.", "result_label"], ["Our method achieves state-of-the-art performance on all of them.", "result_label"]]]
[0, [["Learning embedding functions, which map semantically related inputs to nearby locations in a feature space supports a variety of classification and information retrieval tasks.", "background_label"], ["In this work, we propose a novel, generalizable and fast method to define a family of embedding functions that can be used as an ensemble to give improved results.", "method_label"], ["Each embedding function is learned by randomly bagging the training labels into small subsets.", "method_label"], ["We show experimentally that these embedding ensembles create effective embedding functions.", "method_label"], ["The ensemble output defines a metric space that improves state of the art performance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes Retrieval and VehicleID.", "result_label"]]]
[0, [["Learning the representation and the similarity metric in an end-to-end fashion with deep networks have demonstrated outstanding results for clustering and retrieval.", "background_label"], ["However, these recent approaches still suffer from the performance degradation stemming from the local metric training procedure which is unaware of the global structure of the embedding space.", "background_label"], ["We propose a global metric learning scheme for optimizing the deep metric embedding with the learnable clustering function and the clustering metric (NMI) in a novel structured prediction framework.", "method_label"], ["Our experiments on CUB200-2011, Cars196, and Stanford online products datasets show state of the art performance both on the clustering and retrieval tasks measured in the NMI and Recall@K evaluation metrics.", "result_label"]]]
[0, [["The modern image search system requires semantic understanding of image, and a key yet under-addressed problem is to learn a good metric for measuring the similarity between images.", "background_label"], ["While deep metric learning has yielded impressive performance gains by extracting high level abstractions from image data, a proper objective loss function becomes the central issue to boost the performance.", "background_label"], ["In this paper, we propose a novel angular loss, which takes angle relationship into account, for learning better similarity metric.", "objective_label"], ["Whereas previous metric learning methods focus on optimizing the similarity (contrastive loss) or relative similarity (triplet loss) of image pairs, our proposed method aims at constraining the angle at the negative point of triplet triangles.", "method_label"], ["Several favorable properties are observed when compared with conventional methods.", "method_label"], ["First, scale invariance is introduced, improving the robustness of objective against feature variance.", "method_label"], ["Second, a third-order geometric constraint is inherently imposed, capturing additional local structure of triplet triangles than contrastive loss or triplet loss.", "method_label"], ["Third, better convergence has been demonstrated by experiments on three publicly available datasets.", "result_label"]]]
[0, [["Recognition of human actions and associated interactions with objects and the environment is an important problem in computer vision due to its potential applications in a variety of domains.", "background_label"], ["The most versatile methods can generalize to various environments and deal with cluttered backgrounds, occlusions, and viewpoint variations.", "background_label"], ["Among them, methods based on graph convolutional networks that extract features from the skeleton have demonstrated promising performance.", "background_label"], ["In this paper, we propose a novel Spatio-Temporal Pyramid Graph Convolutional Network (ST-PGN) for online action recognition for ergonomic risk assessment that enables the use of features from all levels of the skeleton feature hierarchy.", "method_label"], ["The proposed algorithm outperforms state-of-art action recognition algorithms tested on two public benchmark datasets typically used for postural assessment (TUM and UW-IOM).", "method_label"], ["We also introduce a pipeline to enhance postural assessment methods with online action recognition techniques.", "method_label"], ["Finally, the proposed algorithm is integrated with a traditional ergonomic risk index (REBA) to demonstrate the potential value for assessment of musculoskeletal disorders in occupational safety.", "result_label"]]]
[0, [["In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition.", "background_label"], ["Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition.", "background_label"], ["In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning.", "method_label"], ["Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly advantages in accuracy.", "method_label"], ["Our empirical study leads to the design of a new spatiotemporal convolutional block \"R(2+1)D\"which gives rise to CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101 and HMDB51.", "result_label"]]]
[0, [["The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond.", "background_label"], ["Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns.", "background_label"], ["We introduce a new class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection.", "method_label"], ["Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions.", "method_label"], ["We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks.", "method_label"], ["We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.", "result_label"]]]
[0, [["Light field (LF) cameras record both intensity and directions of light rays, and capture scenes from a number of viewpoints.", "background_label"], ["Both information within each perspective (i.e., spatial information) and among different perspectives (i.e., angular information) is beneficial to image super-resolution (SR).", "background_label"], ["In this paper, we propose a spatial-angular interactive network (namely, LF-InterNet) for LF image SR.", "objective_label"], ["Specifically, spatial and angular features are first separately extracted from input LFs, and then repetitively interacted to progressively incorporate spatial and angular information.", "method_label"], ["Finally, the interacted features are fused to superresolve each sub-aperture image.", "method_label"], ["Experimental results demonstrate the superiority of LF-InterNet over the state-of-the-art methods, i.e., our method can achieve high PSNR and SSIM scores with low computational cost, and recover faithful details in the reconstructed images.", "result_label"]]]
[0, [["Deep convolutional networks based super-resolution is a fast-growing field with numerous practical applications.", "background_label"], ["In this exposition, we extensively compare 30+ state-of-the-art super-resolution Convolutional Neural Networks (CNNs) over three classical and three recently introduced challenging datasets to benchmark single image super-resolution.", "background_label"], ["We introduce a taxonomy for deep-learning based super-resolution networks that groups existing methods into nine categories including linear, residual, multi-branch, recursive, progressive, attention-based and adversarial designs.", "method_label"], ["We also provide comparisons between the models in terms of network complexity, memory footprint, model input and output, learning details, the type of network losses and important architectural differences (e.g., depth, skip-connections, filters).", "method_label"], ["The extensive evaluation performed, shows the consistent and rapid growth in the accuracy in the past few years along with a corresponding boost in model complexity and the availability of large-scale datasets.", "result_label"], ["It is also observed that the pioneering methods identified as the benchmark have been significantly outperformed by the current contenders.", "result_label"], ["Despite the progress in recent years, we identify several shortcomings of existing techniques and provide future research directions towards the solution of these open problems.", "result_label"]]]
[0, [["Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision.", "background_label"], ["Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques.", "background_label"], ["This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches.", "objective_label"], ["In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR.", "method_label"], ["In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics.", "method_label"], ["Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.", "result_label"]]]
[0, [["Single image super-resolution (SISR) is a notoriously challenging ill-posed problem, which aims to obtain a high-resolution (HR) output from one of its low-resolution (LR) versions.", "objective_label"], ["To solve the SISR problem, recently powerful deep learning algorithms have been employed and achieved the state-of-the-art performance.", "method_label"], ["In this survey, we review representative deep learning-based SISR methods, and group them into two categories according to their major contributions to two essential aspects of SISR: the exploration of efficient neural network architectures for SISR, and the development of effective optimization objectives for deep SISR learning.", "method_label"], ["For each category, a baseline is firstly established and several critical limitations of the baseline are summarized.", "method_label"], ["Then representative works on overcoming these limitations are presented based on their original contents as well as our critical understandings and analyses, and relevant comparisons are conducted from a variety of perspectives.", "method_label"], ["Finally we conclude this review with some vital current challenges and future trends in SISR leveraging deep learning algorithms.", "result_label"]]]
[0, [["In this paper, we explore mobile app use as a behavioral biometric identifier.", "background_label"], ["While several efforts have also taken on this challenge, many have alluded to the inconsistency in human behavior, resulting in updating the biometric template frequently and periodically.", "background_label"], ["Here, we represent app usage as simple images wherein each pixel value provides some information about the user's app usage.", "method_label"], ["Then, we feed use these images to train a deep learning network (convolutional neural net) to classify the user's identity.", "method_label"], ["Our contribution lies in the random order in which the images are fed to the classifier, thereby presenting novel evidence that there are some aspects of app usage that are indeed persistent.", "method_label"], ["Our results yield a 96.8% $F$-score without any updates to the template data.", "result_label"]]]
[0, [["An empirical investigation of active/continuous authentication for smartphones is presented in this paper by exploiting users' unique application usage data, i.e., distinct patterns of use, modeled by a Markovian process.", "background_label"], ["Variations of Hidden Markov Models (HMMs) are evaluated for continuous user verification, and challenges due to the sparsity of session-wise data, an explosion of states, and handling unforeseen events in the test data are tackled.", "background_label"], ["Unlike traditional approaches, the proposed formulation does not depend on the top N-apps, rather uses the complete app-usage information to achieve low latency.", "method_label"], ["Through experimentation, empirical assessment of the impact of unforeseen events, i.e., unknown applications and unforeseen observations, on user verification is done via a modified edit-distance algorithm for simple sequence matching.", "method_label"], ["It is found that for enhanced verification performance, unforeseen events should be incorporated in the models by adopting smoothing techniques with HMMs.", "method_label"], ["For validation, extensive experiments on two distinct datasets are performed.", "method_label"], ["The marginal smoothing technique is the most effective for user verification in terms of equal error rate (EER) and with a sampling rate of 1/30s^{-1} and 30 minutes of historical data, and the method is capable of detecting an intrusion within ~2.5 minutes of application use.", "result_label"]]]
[0, [["Deep Neural Networks (DNNs) have recently been achieving state-of-the-art performance on a variety of computer vision related tasks.", "background_label"], ["However, their computational cost limits their ability to be implemented in embedded systems with restricted resources or strict latency constraints.", "background_label"], ["Model compression has therefore been an active field of research to overcome this issue.", "background_label"], ["Additionally, DNNs typically require massive amounts of labeled data to be trained.", "background_label"], ["This represents a second limitation to their deployment.", "background_label"], ["Domain Adaptation (DA) addresses this issue by allowing knowledge learned on one labeled source distribution to be transferred to a target distribution, possibly unlabeled.", "background_label"], ["In this paper, we investigate on possible improvements of compression methods in DA setting.", "objective_label"], ["We focus on a compression method that was previously developed in the context of a single data distribution and show that, with a careful choice of data to use during compression and additional regularization terms directly related to DA objectives, it is possible to improve compression results.", "method_label"], ["We also show that our method outperforms an existing compression method studied in the DA setting by a large margin for high compression rates.", "result_label"], ["Although our work is based on one specific compression method, we also outline some general guidelines for improving compression in DA setting.", "result_label"]]]
[0, [["Training of large-scale deep neural networks is often constrained by the available computational resources.", "background_label"], ["We study the effect of limited precision data representation and computation on neural network training.", "background_label"], ["Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training.", "method_label"], ["Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy.", "result_label"], ["We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.", "result_label"]]]
[0, [["We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference.", "objective_label"], ["We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network.", "background_label"], ["We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters.", "method_label"], ["We focus on transfer learning, where large pretrained networks are adapted to specialized tasks.", "method_label"], ["The proposed criterion demonstrates superior performance compared to other criteria, e.g.", "method_label"], ["the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information.", "method_label"], ["We also show that pruning can lead to more than 10x theoretical (5x practical) reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier.", "result_label"], ["Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.", "result_label"]]]
[0, [["Deep learning has become a ubiquitous technology to improve machine intelligence.", "background_label"], ["However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power.", "background_label"], ["In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning.", "objective_label"], ["Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance.", "method_label"], ["The effectiveness of our method is proved with experiments.", "method_label"], ["Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins.", "result_label"], ["Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.", "other_label"]]]
[0, [["This paper presents an automatic network adaptation method that finds a ConvNet structure well-suited to a given target task, e.g., image classification, for efficiency as well as accuracy in transfer learning.", "background_label"], ["We call the concept target-aware transfer learning.", "background_label"], ["Given only small-scale labeled data, and starting from an ImageNet pre-trained network, we exploit a scheme of removing its potential redundancy for the target task through iterative operations of filter-wise pruning and network optimization.", "method_label"], ["The basic motivation is that compact networks are on one hand more efficient and should also be more tolerant, being less complex, against the risk of overfitting which would hinder the generalization of learned representations in the context of transfer learning.", "method_label"], ["Further, unlike existing methods involving network simplification, we also let the scheme identify redundant portions across the entire network, which automatically results in a network structure adapted to the task at hand.", "method_label"], ["We achieve this with a few novel ideas: (i) cumulative sum of activation statistics for each layer, and (ii) a priority evaluation of pruning across multiple layers.", "method_label"], ["Experimental results by the method on five datasets (Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable accuracies over the related state-of-the-art techniques while enhancing the computational and storage efficiency of the transferred model.", "result_label"]]]
[0, [["Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually.", "background_label"], ["While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms.", "background_label"], ["These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures.", "background_label"], ["In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning.", "objective_label"], ["To this end, we propose to make use of structured sparsity during learning.", "objective_label"], ["More precisely, we use a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron.", "method_label"], ["Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\\% while retaining or even improving the network accuracy.", "result_label"]]]
[0, [["With time, machine learning models have increased in their scope, functionality and size.", "background_label"], ["Consequently, the increased functionality and size of such models requires high-end hardware to both train and provide inference after the fact.", "background_label"], ["This paper aims to explore the possibilities within the domain of model compression and discuss the efficiency of each of the possible approaches while comparing model size and performance with respect to pre- and post-compression.", "objective_label"]]]
[0, [["Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years.", "background_label"], ["However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large.", "background_label"], ["This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices.", "background_label"], ["In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs.", "method_label"], ["In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods.", "method_label"], ["Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy.", "method_label"], ["For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.", "result_label"]]]
[0, [["Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks.", "background_label"], ["However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions.", "background_label"], ["In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models.", "objective_label"], ["Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response.", "method_label"], ["Extensive experiments on the ILSVRC-12 benchmark demonstrate 4~6x speed-up and 15~20x compression with merely one percentage loss of classification accuracy.", "result_label"], ["With our quantized CNN model, even mobile devices can accurately classify images within one second.", "result_label"]]]
[0, [["While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear.", "background_label"], ["The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks.", "background_label"], ["In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student.", "method_label"], ["Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer.", "method_label"], ["This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity.", "method_label"], ["For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.", "result_label"]]]
[0, [["Spoken dialogue systems typically use a list of top-N ASR hypotheses for inferring the semantic meaning and tracking the state of the dialogue.", "background_label"], ["However ASR graphs, such as confusion networks (confnets), provide a compact representation of a richer hypothesis space than a top-N ASR list.", "background_label"], ["In this paper, we study the benefits of using confusion networks with a state-of-the-art neural dialogue state tracker (DST).", "objective_label"], ["We encode the 2-dimensional confnet into a 1-dimensional sequence of embeddings using an attentional confusion network encoder which can be used with any DST system.", "method_label"], ["Our confnet encoder is plugged into the state-of-the-art 'Global-locally Self-Attentive Dialogue State Tacker' (GLAD) model for DST and obtains significant improvements in both accuracy and inference time compared to using top-N ASR hypotheses.", "result_label"]]]
[0, [["Dialogue state tracking is the core part of a spoken dialogue system.", "background_label"], ["It estimates the beliefs of possible user's goals at every dialogue turn.", "background_label"], ["However, for most current approaches, it's difficult to scale to large dialogue domains.", "background_label"], ["They have one or more of following limitations: (a) Some models don't work in the situation where slot values in ontology changes dynamically; (b) The number of model parameters is proportional to the number of slots; (c) Some models extract features based on hand-crafted lexicons.", "background_label"], ["To tackle these challenges, we propose StateNet, a universal dialogue state tracker.", "method_label"], ["It is independent of the number of values, shares parameters across all slots, and uses pre-trained word vectors instead of explicit semantic dictionaries.", "method_label"], ["Our experiments on two datasets show that our approach not only overcomes the limitations, but also significantly outperforms the performance of state-of-the-art approaches.", "result_label"]]]
